{"posts":[{"title":"WSL2 上安装 K3S","content":"在wsl2 ubuntu-22.04，使用k3s官网的一键安装命令，是安装不成功的。 curl -sfL https://get.k3s.io | sh - # Check for Ready node, takes ~30 seconds sudo k3s kubectl get node 原因，k3s的运行还依赖systemd。在执行上面命令之前， 先要在wsl2 上开启systemd 在wsl2 上新建文件：/etc/wsl.conf，添加以下内容： [boot] systemd=true 重启wsl2 在windows11 上执行wsl --shutdown，然后开启新的wsl，wsl。 在ubuntu上执行命令，验证systemd 的开启效果。 systemctl list-unit-files --type=service 重新执行k3s 的一键安装脚本 则会看到安装成功的输出了： ➜ ~ curl -sfL https://get.k3s.io | sh - # Check for Ready node, takes ~30 seconds sudo k3s kubectl get node .... [INFO] systemd: Starting k3s NAME STATUS ROLES AGE VERSION nb-hz20314960 Ready control-plane,master 14m v1.28.4+k3s2 免sudo 执行kubectl 更改的文件权限： sudo chmod 644 /etc/rancher/k3s/k3s.yaml 重启k3s sudo systemctl restart k3s 然后执行 kubectl get node 则不再需要root权限。 得到k3s的kubeconfig 接下来可以去这个路径复制 kubeconfig，添加到你操作电脑的kubeconfig里，即可跟WSL2 下的k3s进行交互了。 /etc/rancher/k3s/k3s.yaml Systemd support is now available in WSL! ","link":"https://phantomma.top/post/wsl2-shang-an-zhuang-k3s/"},{"title":"Windows11 开启WSL2 和局域网访问","content":"安装WSL2 参考这篇文章： https://zhuanlan.zhihu.com/p/475462241 https://www.sysgeek.cn/windows-11-install-wsl2/ WSL2开启ssh vim /etc/ssh/sshd_config 修改如下： # Port 22 AddressFamily any ListenAddress 0.0.0.0 ListenAddress :: PasswordAuthentication yes 重启sshd：sudo service ssh restart。 看到重启成功即可，使用ssh登录WSL2。 开机启动 windows11 + WSL2，整个启动过程如下： Win11开机 &gt;&gt; Win11开机脚本 &gt;&gt; WSL子系统脚本 &gt;&gt; 启动Linux程序 在ubuntu下添加开机启动 为WSL2 开启systemd，那么可以使用systemctl service ssh start，这种服务启动的方式了。具体可参考下篇中的开启systemd部分。 常驻后台运行 正常情况，当关闭WSL的console时，实例会自动关闭。这对于我们想长时间把WSL ubuntu当作一台远程vm主机来说，显然是不合适的，需要长时间打开一个终端窗口。可以采用以下方式，在开机时打开一个实例。0表示，它会一直等待输入，而不会休眠。 set ws=wscript.CreateObject(&quot;wscript.shell&quot;) ws.run &quot;wsl -d Ubuntu-22.04&quot;, 0 移动到Windows开机启动项 Windows 下 Win+R 输入 shell:startup，打开文件夹，把linux-wsl.vbs，拖入到文件夹中。 故障排除办法 如果出现运行不成功，多半是权限问题，可以在cmd窗口运行如下启动命令 wsl -d Ubuntu-22.04 局域网访问Windows11 下的WSL2 WSL 2 发布了最新版本 2.0.0，这个版本开始，自带支持新的镜像网络，WSL2可以使用宿主机的IP，开启的端口直接打通到宿主机。这样可以通过宿主机IP + Port访问WSL2 虚拟机里的服务。 从而不用折腾桥接网络下，在windows上端口转发，开启防火墙等复杂操作。而且，ip经常跳的问题，导致没法用。绑定localhost，也要解决几个奇怪的报错。 现在有了镜像网络，前几项的折腾就可以省去了。 更新 WSL：wsl --update --pre-release [experimental] networkingMode=mirrored # 镜像网络模式 dnsTunneling=true firewall=true autoProxy=true 重启wsl2，就可以使用宿主机windows11 的ip，通过ssh，来访问内部虚拟机WSL2 ubuntu了。 WSL2 今天史诗级更新 WSL2 网络的最终解决方案 ","link":"https://phantomma.top/post/windows11-kai-qi-wsl2/"},{"title":"vagrant 一键拉起 github action runner","content":"一个通用的 github runner 定义。可按需定制为自己想要的 # hostname $host_name = &quot;XXX&quot; $host_ip = &quot;XXX&quot; $runner_token = &quot;XXX&quot; Vagrant.configure(&quot;2&quot;) do |config| config.vm.box = &quot;ubuntu/jammy64&quot; config.vm.box_url = &quot;https://mirrors.tuna.tsinghua.edu.cn/ubuntu-cloud-images/jammy/current/jammy-server-cloudimg-amd64-vagrant.box&quot; config.vm.hostname = $host_name config.vm.network &quot;private_network&quot;, ip: $host_ip # spec config config.vm.provider :virtualbox do |vbox| vbox.name = $host_name vbox.cpus = 4 vbox.memory = 8000 end # init shell, run as root config.vm.provision &quot;shell&quot;, path: &quot;run_as_root.sh&quot; # init shell, run as user config.vm.provision &quot;shell&quot;, privileged: false, path: &quot;run_as_user.sh&quot;, args: [$runner_token] end 以root身份执行的动作： # 启用ssh密码认证 echo &quot;[Step 1] Enable ssh password authentication&quot; sed -i 's/^PasswordAuthentication .*/PasswordAuthentication yes/' /etc/ssh/sshd_config echo 'PermitRootLogin yes' &gt;&gt; /etc/ssh/sshd_config systemctl reload sshd # ssh以root用户登陆，需要重置root密码 echo &quot;[Step 2] change root password&quot; echo &quot;root:123456&quot; | sudo chpasswd # apt install apps echo &quot;[Step 3] apt install apps&quot; sed -i '1i deb http://cn.archive.ubuntu.com/ubuntu/ jammy-backports main restricted universe multiverse' /etc/apt/sources.list sed -i '1i deb http://cn.archive.ubuntu.com/ubuntu/ jammy-updates multiverse' /etc/apt/sources.list sed -i '1i deb http://cn.archive.ubuntu.com/ubuntu/ jammy multiverse' /etc/apt/sources.list sed -i '1i deb http://cn.archive.ubuntu.com/ubuntu/ jammy-updates universe' /etc/apt/sources.list sed -i '1i deb http://cn.archive.ubuntu.com/ubuntu/ jammy universe' /etc/apt/sources.list sed -i '1i deb http://cn.archive.ubuntu.com/ubuntu/ jammy-updates main restricted' /etc/apt/sources.list sed -i '1i deb http://cn.archive.ubuntu.com/ubuntu/ jammy main restricted' /etc/apt/sources.list apt-get update apt-get install -y \\ ca-certificates \\ curl \\ gnupg \\ lsb-release \\ mysql-client mkdir -m 0755 -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg echo \\ &quot;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable&quot; | tee /etc/apt/sources.list.d/docker.list &gt; /dev/null apt-get update apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin # add docker registry mirror # echo &quot;[Step 4] add docker registry mirror&quot; tee /etc/docker/daemon.json &lt;&lt;-EOF { &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;,&quot;http://hub-mirror.c.163.com&quot;] } EOF # docker cmd need no root echo &quot;[Step 5] docker cmd need no root&quot; usermod -aG docker vagrant 以普通用户身份执行的动作： # install and config github action runner echo &quot;[Step 6] install and config github action runner&quot; mkdir actions-runner &amp;&amp; cd actions-runner curl -o ./actions-runner-linux-x64-2.302.1.tar.gz -L https://github.com/actions/runner/releases/download/v2.302.1/actions-runner-linux-x64-2.302.1.tar.gz tar xzf ./actions-runner-linux-x64-2.302.1.tar.gz ./config.sh --url https://github.com/AutoMQ --token $1 --unattended nohup ./run.sh &gt; runner.log 2&gt;&amp;1 &amp; ","link":"https://phantomma.top/post/vagrant-yi-jian-la-qi-github-action-runner/"},{"title":"Debezium outbox pattern","content":"自部署的mysql上开启binlog 查看mysql cnf文件所在位置 mysql --help Default options are read from the following files in the given order: /etc/my.cnf /etc/mysql/my.cnf /opt/homebrew/etc/my.cnf ~/.my.cnf 修改cnf文件，增加如下内容： server-id = 1 log_bin = /tmp/mysql-bin binlog_format = ROW binlog_row_image = FULL expire_logs_days = 1 重启mysql服务 在aws rds for mysql上开启binlog rds dashboard上创建新的参数组，更改binlog_format为ROW 确认数据库是否已开启binlog，再次运行命令： show variables like 'log_bin'; show variables like 'binlog_format'; 修改rds示例配置。修改参数组为上面新建的；修改备份保留期为1天 设置binlog保存时长为24h call mysql.rds_set_configuration('binlog retention hours', 24); call mysql.rds_show_configuration; 重启实例 ","link":"https://phantomma.top/post/debezium-outbox-pattern/"},{"title":"使用OrbStack 容器里安装常用软件","content":"安装 OrbStack OrbStack is a fast, light, and simple way to run Docker containers and Linux machines on macOS. You can think of it as a supercharged WSL and Docker Desktop replacement, all in one easy-to-use app. 安装MySQL 拉取镜像 docker pull mysql //拉取镜像 docker images //查看本地镜像 创建数据卷 将其配置和数据等等挂载到数据卷以持久化到宿主机。 创建三个数据卷，分别用于挂载并持久化MySQL的 数据文件、配置文件 和 日志文件 ： docker volume create mysql-data docker volume create mysql-config docker volume create mysql-log 创建并运行容器 docker run -id --name=mysql -v mysql-config:/etc/mysql/conf.d \\ -v mysql-log:/logs \\ -v mysql-data:/var/lib/mysql \\ -p 3307:3306 \\ -e MYSQL_ROOT_PASSWORD=xxx \\ -e LANG=C.UTF-8 mysql -id 将MySQL容器挂在后台运行 --name=mysql 将容器起名为 mysql -v mysql-config:/etc/mysql/conf.d 把MySQL容器中的配置文件目录挂载至上述创建的名为mysql-config的数据卷上面，其他两个 -v 挂载数据卷的参数同理 -p 3306:3306 将主机的 3306 端口映射到容器的 3306 -e MYSQL_ROOT_PASSWORD=xxx 设置 root 用户的密码 -e LANG=C.UTF-8 设置容器的语言环境变量 LANG 值为 C.UTF-8 通过容器访问MySQL docker exec -it mysql bash mysql -uroot -p 在容器外访问： mysql -u root -pxxx -hmysql.orb.local 安装redis 拉取镜像 docker pull redis 创建数据卷 docker volume create redis-config docker volume create redis-data 修改配置文件 cd ~/OrbStack/docker/volumes/redis-config vim redis.conf # 启动redis持久化功能 appendonly yes # 设置密码 requirepass xxx # 指定数据存储位置 dir /data 创建容器 docker run -id --name=redis \\ -v redis-config:/usr/local/etc/redis \\ -v redis-data:/data -p 6379:6379 \\ -e LANG=C.UTF-8 redis \\ redis-server /usr/local/etc/redis/redis.conf # su -l root -c &quot;redis-server /usr/local/etc/redis/redis.conf&quot; --name redis 指定容器名字 -v 指定数据卷，可见将容器配置文件夹/usr/local/etc/redis挂载至了数据卷redis-config，将容器 内/data挂载至数据卷redis-data，可见这里挂载数据卷的容器内路径和我们上述预先写的配置文件中对应的路径是要一致的 -p 6379:6379 端口映射 -e 用于指定容器内环境变量，设置容器的语言环境变量LANG值为C.UTF-8，这个最好是要设置，否则容器内默认是英文环境，使得Redis可能无法存放中文内容 su -l root -c &quot;redis-server /usr/local/etc/redis/redis.conf&quot; 在容器内以root身份运行redis-server并指定了配置文件位置 通过容器执行redis命令 运行redis docker start redis 查看redis运行状态 docker ps | grep redis 进入redis容器内部 docker exec -it redis bash 进入Redis控制台 redis-cli MongoDB 安装 docker pull mongo docker run --name mongo -v mongo-data:/data/db -v mongo-config:/data/configdb --privileged -p 27017:27017 -e MONGO_INITDB_ROOT_USERNAME=admin -e MONGO_INITDB_ROOT_PASSWORD=123456 -d mongo --auth -name 指定容器名称 -v 指定数据存储位置 --privileged root权限 -p 端口映射 -d 后台运行 -auth 需要认证，默认mongo是不需要认证的 -e MONGO_INITDB_ROOT_USERNAME=admin 指定用户名 -e MONGO_INITDB_ROOT_PASSWORD=123456 指定密码 添加用户 进入容器 docker exec -it mongo mongosh admin 添加用户 db.createUser({user:'admin',pwd:'123456',roles[{role:'userAdminAnyDatabase', db: 'admin'},&quot;readWriteAnyDatabase&quot;]}) kafka 安装kafka 最新Kafka 使用内置的 Raft 组件来管理主题和分区，因此我们不用额外安装ZooKeeper，可以直接安装 Kafka 拉取镜像 docker pull bitnami/kafka:latest 创建网络 docker network create kafka-net 启动Kafka服务实例 docker run -d --name kafka \\ --network kafka-net \\ -p 9092:9092 \\ -e ALLOW_PLAINTEXT_LISTENER=yes \\ bitnami/kafka:latest 测试Kafka生产者和消费者 docker exec -it kafka /bin/b cd opt/bitnami/kafka/bin/ 开启两个终端 运行生产者发送消息 ./kafka-console-producer.sh --broker-list localhost:9092 --topic test-topic 运行消费者接受消息 ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic --from-beginning ","link":"https://phantomma.top/post/shi-yong-orbstack-an-zhuang-chang-yong-yi-lai/"},{"title":"在 Kubernetes 上运行高可用的 Kafka 集群","content":"Apache Kafka 是目前最流行的分布式消息发布订阅系统，虽然 Kafka 非常强大，但它同样复杂，需要一个高可用的强大平台来运行。在微服务盛行，大多数公司都采用分布式计算的今天，将 Kafka 作为核心的消息系统使用还是非常有优势的。 如果你在 Kubernetes 集群中运行你的微服务，那么在 Kubernetes 中运行 Kafka 集群也是很有意义的，这样可以利用其内置的弹性和高可用，我们可以使用内置的 Kubernetes 服务发现轻松地与集群内的 Kafka Pods 进行交互。 下面我们将来介绍下如何在 Kubernetes 上构建分布式的 Kafka 集群，这里我们将使用 Helm Chart 和 StatefulSet 来进行部署，当然如果想要动态生成持久化数据卷，还需要提前配置一个 StorageClass 资源，比如基于 Ceph RBD 的，如果你集群中没有配置动态卷，则需要提前创建 3 个未绑定的 PV 用于数据持久化。 当前基于 Helm 官方仓库的 chartincubator/kafka 在 Kubernetes 上部署的 Kafka，使用的镜像是 confluentinc/cp-kafka:5.0.1，即部署的是 Confluent 公司提供的 Kafka 版本，Confluent Platform Kafka(简称 CP Kafka)提供了一些 Apache Kafka 没有的高级特性，例如跨数据中心备份、Schema 注册中心以及集群监控工具等。 安装 使用 Helm Chart 安装当然前提要安装 Helm，直接使用最新版本的 Helm v3 版本即可： &gt; wget https://get.helm.sh/helm-v3.4.0-linux-amd64.tar.gz &gt; tar -zxvf helm-v3.4.0-linux-amd64.tar.gz &gt; sudo cp -a linux-amd64/helm /usr/local/bin/helm &gt; chmod +x /usr/local/bin/helm 然后添加 Kafka 的 Chart 仓库： &gt; helm repo add incubator http://mirror.azure.cn/kubernetes/charts-incubator/ &gt; helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the &quot;incubator&quot; chart repository ...Successfully got an update from the &quot;stable&quot; chart repository Update Complete. ⎈Happy Helming!⎈ 接着我们就可以配置需要安装的 Values 文件了，可以直接使用默认的 values.yaml 文件，然后可以用它来进行定制，比如指定我们自己的 StorageClass： &gt; curl https://raw.githubusercontent.com/helm/charts/master/incubator/kafka/values.yaml &gt; kfk-values.yaml 这里我直接使用默认的进行安装： &gt; helm install kafka incubator/kafka -f kfk-values.yaml NAME: kafka LAST DEPLOYED: Sun Nov 1 09:36:44 2020 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: ### Connecting to Kafka from inside Kubernetes You can connect to Kafka by running a simple pod in the K8s cluster like this with a configuration like this: apiVersion: v1 kind: Pod metadata: name: testclient namespace: default spec: containers: - name: kafka image: confluentinc/cp-kafka:5.0.1 command: - sh - -c - &quot;exec tail -f /dev/null&quot; Once you have the testclient pod above running, you can list all kafka topics with: kubectl -n default exec testclient -- ./bin/kafka-topics.sh --zookeeper kafka-zookeeper:2181 --list To create a new topic: kubectl -n default exec testclient -- ./bin/kafka-topics.sh --zookeeper kafka-zookeeper:2181 --topic test1 --create --partitions 1 --replication-factor 1 To listen for messages on a topic: kubectl -n default exec -ti testclient -- ./bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic test1 --from-beginning To stop the listener session above press: Ctrl+C To start an interactive message producer session: kubectl -n default exec -ti testclient -- ./bin/kafka-console-producer.sh --broker-list kafka-headless:9092 --topic test1 To create a message in the above session, simply type the message and press &quot;enter&quot; To end the producer session try: Ctrl+C If you specify &quot;zookeeper.connect&quot; in configurationOverrides, please replace &quot;kafka-zookeeper:2181&quot; with the value of &quot;zookeeper.connect&quot;, or you will get error. 如果你没配置 StorageClass 或者可用的 PV，安装的时候 kafka 的 Pod 会处于 Pending 状态，所以一定要提前配置好数据卷。 正常情况隔一会儿 Kafka 就可以安装成功了： &gt; kubectl get pods NAME READY STATUS RESTARTS AGE kafka-0 1/1 Running 0 25m kafka-1 1/1 Running 0 11m kafka-2 1/1 Running 0 2m kafka-zookeeper-0 1/1 Running 0 25m kafka-zookeeper-1 1/1 Running 0 22m kafka-zookeeper-2 1/1 Running 0 18m 默认会安装 3 个 ZK Pods 和 3 个 Kafka Pods，这样可以保证应用的高可用，也可以看下我配置的持久卷信息： &gt; kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE datadir-kafka-0 Bound kfk0 1Gi RWO 28m datadir-kafka-1 Bound kfk1 1Gi RWO 13m datadir-kafka-2 Bound kfk2 1Gi RWO 4m9s &gt; kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE kfk0 1Gi RWO Retain Bound default/datadir-kafka-0 23m kfk1 1Gi RWO Retain Bound default/datadir-kafka-1 22m kfk2 1Gi RWO Retain Bound default/datadir-kafka-2 10m 如果我们配置一个 default 的 StorageClass，则会动态去申请持久化卷，如果你的集群没有启用动态卷，可以修改 values.yaml 来使用静态卷。 然后查看下对应的 Service 对象： &gt; kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kafka ClusterIP 10.100.205.187 &lt;none&gt; 9092/TCP 31m kafka-headless ClusterIP None &lt;none&gt; 9092/TCP 31m kafka-zookeeper ClusterIP 10.100.230.255 &lt;none&gt; 2181/TCP 31m kafka-zookeeper-headless ClusterIP None &lt;none&gt; 2181/TCP,3888/TCP,2888/TCP 31m kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 14d 可以看到又一个叫 kafka-zookeeper 的 zookeeper 服务和一个叫 kafka 的 Kafka 服务，对于 Kafka 集群的管理，我们将与 kafka-zookeeper 服务进行交互，对于集群消息的收发，我们将使用 kafka 服务。 客户端测试 现在 Kafka 集群已经搭建好了，接下来我们来安装一个 Kafka 客户端，用它来帮助我们产生和获取 topics 消息。 直接使用下面的命令创建客户端： &gt; cat &lt;&lt;EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: testclient namespace: default spec: containers: - name: kafka image: confluentinc/cp-kafka:5.0.1 command: - sh - -c - &quot;exec tail -f /dev/null&quot; EOF &gt; kubectl get pod testclient NAME READY STATUS RESTARTS AGE testclient 1/1 Running 0 23s 客户端 Pod 创建成功后我们就可以开始进行一些简单的测试了。首先让我们创建一个名为 test1 的有一个分区和复制因子'1'的 topic： &gt; kubectl exec -it testclient -- /usr/bin/kafka-topics --zookeeper kafka-zookeeper:2181 --topic test1 --create --partitions 1 --replication-factor 1 Created topic &quot;test1&quot;. 然后创建一个生产者，将消息发布到这个 topic 主题上： &gt; kubectl exec -ti testclient -- /usr/bin/kafka-console-producer --broker-list kafka:9092 --topic test1 然后重新打一个终端页面，让我们打开一个消费者会话，这样我们就可以看到我们发送的消息了。 &gt; kubectl exec -ti testclient -- /usr/bin/kafka-console-consumer --bootstrap-server kafka:9092 --topic test1 现在我们在生产者的窗口发送消息，在上面的消费者会话窗口中就可以看到对应的消息了： 到这里证明 Kafka 集群就正常工作了。比如需要注意 zk 集群我们并没有做持久化，如果是生产环境一定记得做下数据持久化，在 values.yaml 文件中根据需求进行定制即可，当然对于生产环境还是推荐使用 Operator 来搭建 Kafka 集群，比如 strimzi-kafka-operator。 ","link":"https://phantomma.top/post/zai-kubernetes-shang-yun-xing-gao-ke-yong-de-kafka-ji-qun/"},{"title":"Helm Chart 模板技巧","content":"Helm Chart 在我们使用的时候非常方便的，但是对于开发人员来说 Helm Chart 模板就并不一定显得那么友好了，本文主要介绍了 Helm Chart 模板开发人员在构建生产级的 Chart 包时的一些技巧和窍门。 了解你的模板功能 Helm 使用Go Template来模板化资源文件。在 Go 提供的内置函数基础上，还添加了许多其他功能。 首先，添加了Sprig 库中的几乎所有函数，出于安全原因，删除了两个函数：env和expandenv（这会让 Chart 模板开发者访问到 Tiller 的环境）。 另外还添加了两个特殊的模板函数：include和required，include函数允许你引入另一个模板，然后将结果传递给其他模板函数。 例如，下面的模板片段中引用了一个名为mytpl的模板，然后将结果转成小写，并用双引号包装起来： value: {{ include &quot;mytpl&quot; . | lower | quote }} required函数允许你根据模板的需要声明特定的值，如果值为空，则默认渲染的时候会报错。下面的这个示例被声明为 .Values.who 是必须的，为空的时候会打印出一段错误提示信息： value: {{required &quot;A valid .Values.who entry required!&quot; .Values.who }} 引用字符串，不要引用整数 当你使用字符串数据的时候，为了安全考虑应该总是使用字符串而不是直接暴露出来：露： name: {{ .Values.MyName | quote }} 当使用整数时，不要直接引用这些值，在很多情况下，可能会导致 Kubernetes 内部的解析错误。 port: {{ .Values.Port }} 使用 include 功能 Go 提供了一种使用内置template指令将一个模板包含在另外一个模板中的方法。但是，内置函数不能用于 Go 模板管道。为了能够包含模板，然后对该模板的输出执行操作，Helm 提供了特殊的include功能： {{ include &quot;toYaml&quot; $value | indent 2}} 上面包含一个名为toYaml的模板，然后将值$value传递给模板，最后将该模板的输出传递给indent函数。 由于 YAML 对于缩进级别和空格的重要性，所以这是包含代码片段的一种很好的方法，但是需要在相关的上下文中处理缩进。 使用 tpl 函数 tpl函数运行允许开发人员将字符串计算为模板内的模板，这对于将模板字符串作为值传递给 Chart 或者呈现外部配置文件很有用：{{ tpl TEMPLATE_STRING VALUES }} 例子： # values template: &quot;{{ .Values.name }}&quot; name: &quot;Tom&quot; # template {{ tpl .Values.template . }} # output Tom 渲染外部配置文件： # external configuration file conf/app.conf firstName={{ .Values.firstName }} lastName={{ .Values.lastName }} # values firstName: Peter lastName: Parker # template {{ tpl (.Files.Get &quot;conf/app.conf&quot;) . }} # output firstName=Peter lastName=Parker 创建 imagePullSecret imagePullSecret基本上是registry、用户名和密码的组合，我们在使用私有仓库的时候需要使用到，需要用base64对这些数据进行编码，我们可以编写一个模板来生成这个配置文件： 首先，假设我们在values.yaml中定义如下： imageCredentials: registry: quay.io username: someone password: sillyness 然后我们可以这样来定义模板： {{- define &quot;imagePullSecret&quot; }} {{- printf &quot;{\\&quot;auths\\&quot;: {\\&quot;%s\\&quot;: {\\&quot;auth\\&quot;: \\&quot;%s\\&quot;}}}&quot; .Values.imageCredentials.registry (printf &quot;%s:%s&quot; .Values.imageCredentials.username .Values.imageCredentials.password | b64enc) | b64enc }} {{- end }} 最后，我们在 Secret 模板中使用上面定义的模板来创建对象： apiVersion: v1 kind: Secret metadata: name: myregistrykey type: kubernetes.io/dockerconfigjson data: .dockerconfigjson: { { template &quot;imagePullSecret&quot; . } } ConfigMap 或者 Secret 更改时自动更新 ConfigMap 或者 Secret 通常作为配置文件注入到容器中，如果后面使用helm upgrade来升级更新这些应用程序，则可能需要重新启动，但如果部署的资源清单数据没有改变则应用程序还会继续使用旧的配置，从而导致部署不一致。 sha256sum函数可用于确保在另一个文件更改时更新部署的 annotations 部分： kind: Deployment spec: template: metadata: annotations: checksum/config: {{ include (print $.Template.BasePath &quot;/configmap.yaml&quot;) . | sha256sum }} [...] 更多的信息我们可以查看 helm upgrade --recreate-pods 命令来了解这个问题的其他信息。 告诉 Tiller 不要删除资源 有的时候在运行helm delete命令后有些资源不应该被删除。Chart 开发者可以在资源对象中添加一个 annotation 来保护资源不被删除： kind: Secret metadata: annotations: &quot;helm.sh/resource-policy&quot;: keep [...] 注意引号是必须的 &quot;helm.sh/resource-policy&quot;: keep 这个 annotation 用来指示 Tiller 在删除一个 realease 的时候跳过当前这个资源。但是需要注意的是，这样这个资源就变成了孤儿，Helm 将不会再管理它了，如果在已经删除但是仍然还保留了部分资源的 realese 上面使用 helm install --replace 命令可能就会出现问题了。 使用Partials 有时候可能你想要在 Chart 中创建一些可重复使用的片段，无论是一块还是模板的一部分，通常将它们保存在自己的文件中会更清晰。 在templates/目录下面，任何以下划线(_)开头的文件都不会被输出到 Kubernetes 资源清单文件中去，按照惯例，帮助模板一般放在_helpers.tpl文件中。 有需要依赖的复杂 Chart 官方的 Chart 仓库中有许多 Chart 都是用于创建更加高级的应用程序的“构建块”。但是 Chart 也可以用于创建大型应用程序。在这种情况下，单个 Chart 可能需要包含多个子 Chart，每个子 Chart 作为整体的一部分。 对于复杂的应用程序当前最佳的实践方式是创建一个顶级的 Chart，然后使用charts子目录嵌入每个组件。 下面是两个复杂的项目使用案例： **SAP 的 OpenStack Chart：**这个 Chart 包用于在 Kubernetes 上安装一套完整的OpenStack IaaS系统，所有的 Charts 包都在这个 Github 仓库中：openstack-helm **Deis 的 Workflow：**这个 Chart 包使用一个 Chart 安装整个 Deis PaaS 系统，但是它与SAP Chart的不同之处在于，每个子 Chart 都是独立的，都在不同的 Git 仓库中进行托管的，查看requirements.yaml文件，可以了解该 Chart 是如何通过他的 CI/CD pipeline 构建的。仓库地址：(Workflow)[https://github.com/deis/workflow/tree/master/charts/workflow] 这两个 Chart 都说明了使用 Helm 构建复杂环境是很成熟的技术。 YAML 是 JSON 的超集 根据 YAML 的规范，YAML 是 JSON 的超集，这意味着任何有效的 JSON 结构在 YAML 中都是有效的。 所以有时候可能我们去使用 JSON 的语法来表达数据结构更容易，而不是去处理 YAML 的空白。 当然作为最佳实践，模板应该遵循 YAML 的语法，除非 JSON 语法大大降低了格式化问题的风险。 小心生成随机值 Helm 中有一些功能运行你生成随机数据，加密密钥等，但需要注意的是，在升级过程中，会重新执行模板渲染，当模板运行生成的数据与上次不一致时，会触发该资源的更新。 系统的升级版本 在安装和升级版本时使用相同的命令： helm upgrade --install &lt;release name&gt; --values &lt;values file&gt; &lt;chart directory&gt; 相关链接 https://github.com/technosophos/k8s-helm/blob/master/docs/charts_tips_and_tricks.md https://github.com/sapcc/helm-charts https://github.com/deis/workflow/tree/master/charts/workflow https://godoc.org/text/template https://godoc.org/github.com/Masterminds/sprig ","link":"https://phantomma.top/post/helm-chart-mo-ban-kai-fa-ji-qiao/"},{"title":"使用 kotlin + jdbcTemplate 写 dao 层","content":"类orm的方案 编写dal层代码，java领域比较火的两个阵营：JPA(Hibernate)和MyBatis。JPA是java制定的一种ORM规范；MyBatis较轻量，还需要开发人员理解sql。 国内的情况，mybatis会更流行一些。能够胜出的点，大概是因为：可定制性更强，更易于进行性能优化。 但是，mybatis用久了，需要不断修改xml，又引起了很多人的不爽。随后基于mybatis之上的工具，如mybatis-plus，tk.mybatis便被提出来。使用lambda的函数，替代动态xml模板。 同时，采用类似做法的也有很多其他语言的框架，比如kotlin的Exposed、ktorm，golang的gorm。 简单介绍下这几个框架的用法： mybatis-plus和tk.mybatis比较类似，使用lambda函数描述sql中的一些语法含义，替代了mybatis的动态xml模板层 Exposed和ktorm，kotlin写的，和上面两个比较像。一个比较明显区别，没有基于mybatis spring jdbc 除了上面类orm的框架，还有spring提供的jdbcTemplate，对JDBC进行简单的封装，提供了数据层操作最基本的一些能力，诸如：动态数据源、返回结果到java对象的映射等。 spring jdbc的用法，没有动态xml，需要在java代码里手写sql。sql一长，换行拼接的方式，又非常不具有可读性。所以，被大规模正式地使用的情况，还是比较少。 所有，在github也有类似spring-data-mybatis-mini的小工具，基于spring jdbc + mybatis的xml动态模板，实现动态sql的能力。 那么，在上面两种方式之外，有没有一种中间路径，比orm更轻，比spring jdbc更易用的第三个选择？ 第三种方案：kotlin + spring jdbc 先上代码： @Repository abstract class BaseDAO { @Resource private lateinit var sequenceService: SequenceService fun createEntity(tableName: String, entityDO: EntityDO, jdbcInsert: SimpleJdbcInsert): Long { if (entityDO.id == null) { entityDO.id = sequenceService.nextValue(tableName) } entityDO.gmtCreate = Date() entityDO.gmtModified = Date() val affectRow: Number = jdbcInsert.execute(BeanPropertySqlParameterSource(entityDO)) if (affectRow != 1) { throw RuntimeException(&quot;create jdbcInsert.execute fail&quot;) } return entityDO.id } } @Repository class TenantStaffDAO : BaseDAO() { private val tableName: String = &quot;tenant_staff&quot; @Resource private lateinit var jdbcTemplate: JdbcTemplate private lateinit var jdbcInsert: SimpleJdbcInsert private lateinit var jdbcUpdate: SimpleJdbcUpdate @PostConstruct fun init() { jdbcInsert = SimpleJdbcInsert(jdbcTemplate).withTableName(tableName) jdbcUpdate = SimpleJdbcUpdate(jdbcTemplate).withTableName(tableName).ukColumns(&quot;tenant_id&quot;, &quot;id&quot;) } fun create(staffDO: TenantStaffDO): Long { return super.createEntity(tableName, staffDO, jdbcInsert) } fun get(tenantId: Long, staffId: Long): TenantStaffDO? { val results = jdbcTemplate.query(&quot;select * from tenant_staff where tenant_id = ? and id = ?&quot;, arrayOf(tenantId, staffId), BeanPropertyRowMapper(TenantStaffDO::class.java)) return DataAccessUtils.singleResult(results) } fun getByUid(tenantId: Long, uid: Long): TenantStaffDO? { val results = jdbcTemplate.query(&quot;select * from tenant_staff where tenant_id = ? and uid = ?&quot;, arrayOf(tenantId, uid), BeanPropertyRowMapper(TenantStaffDO::class.java)) return DataAccessUtils.singleResult(results) } fun batchGet(tenantId: Long, staffIdList: List&lt;Long&gt;): List&lt;TenantStaffDO&gt;? { val sql = &quot;&quot;&quot; select * from tenant_staff where tenant_id = ? and id in ${staffIdList.joinToString(prefix = &quot;(&quot;, separator = &quot;, &quot;, postfix = &quot;)&quot;) { &quot;?&quot; }} &quot;&quot;&quot; val args = arrayListOf&lt;Any&gt;() args.add(tenantId) args.addAll(staffIdList) return jdbcTemplate.query(sql, args.toArray(), BeanPropertyRowMapper(TenantStaffDO::class.java)) } fun listStaffs(tenantId: Long, size: Int, offset: Long): List&lt;TenantStaffDO&gt;? { val sql = &quot;select * from tenant_staff where tenant_id = ? order by id desc limit ? offset ?&quot; return jdbcTemplate.query(sql, arrayOf(tenantId, size, offset), BeanPropertyRowMapper(TenantStaffDO::class.java)) } fun listTenants(uid: Long): List&lt;TenantStaffDO&gt;? { val sql = &quot;select * from tenant_staff where uid = ?&quot; return jdbcTemplate.query(sql, arrayOf(uid), BeanPropertyRowMapper(TenantStaffDO::class.java)) } fun singleUpdate(staffDO: TenantStaffDO): Boolean { staffDO.gmtModified = Date() return jdbcUpdate.singleUpdate(staffDO) } fun delete(tenantId: Long, id: Long): Int { return jdbcTemplate.update(&quot;delete from tenant_staff where tenant_id = ? and id = ?&quot;, tenantId, id) } fun count(tenantId: Long): Int { return jdbcTemplate.queryForObject(&quot;select count(*) from tenant_staff where tenant_id = ?&quot;, arrayOf(tenantId), Int::class.java) } } 解决了dal层最基本需求 数据库结果到java对象映射 支持动态sql能力，比如in查询的列表，动态拼接 使用SimpleJdbcInsert进行DO对象的写入，可以做到不用关心表的字段 另外，spring-jdbc没有提供SimpleJdbcUpdate的实现，简单实现了一个，可以达到一样的效果：传入DO对象进行update 几个好处 和java混合编程，基本可以无缝衔接 直接编写程序员最熟悉的sql，而不是去理解一个xml或者lambda的转换层 不需要了解xml动态模板语法 特殊字符无需转义 底层基于spring jdbc，简单纯粹 借助kotlin的字符串模板功能，基本可以做到mybatis的xml模板同样的效果。支持if、for等 存在的问题 java编写的DO类，如果使用了lombok，kotlin dao层代码在引用DO对象的属性时，会报编译错误。目前lombok和kotlin还无法在一起完美使用。解决办法是，在DO类里将kotlin代码使用的对象，手动生成setter、getter 另外，附上文中提到的几个框架的介绍链接 Mybatis-plus学习与实践——从繁琐的CRUD中解放出来 对mybatis和mybatis plus进行扩展，更好的支持dao编码和测试 spring-data-mybatis-mini：spring jdbc + mybatis 实现动态sql能力 你还在用 MyBatis 吗，Ktorm 了解一下？——专注于 Kotlin 的 ORM 框架 ","link":"https://phantomma.top/post/shi-yong-kotlin-jdbctemplate-xie-dao-ceng/"},{"title":"SpringCloud On K8s Demo","content":"主要参考spring-cloud-kubernetes，它是springcloud官方推出的开源项目，用于将Spring Cloud和Spring Boot应用运行在kubernetes环境。并结合fabric8 maven插件，做到CI，自动部署到k8s环境。 部署结构 Demo工程结构 ~/projects/SpringCloudOnK8sDemo &gt; tree -L 1 . ├── backend-hello ├── backend-login ├── eureka ├── gateway-eureka └── gateway-k8s eureka服务注册中心 一个gateway，两个后端服务 服务发现机制 基于k8s可以有两种服务机制： SpringCloud eureka k8s api server 他们分别提供了生态内的相关能力，都有类似如：灰度发布、容量分配、熔断、降级等基础能力。但考虑专有云是天基底座，可能没有k8s，避免依赖k8s，可以脱离其运行，内置SpringCloud是一种更架构独立、减小绑定的选择。 服务路由 在gateway中，通过uri中route前缀判断是需要做服务路由的请求 例如这几个示例： 访问gateway：http://gateway-eureka.c066372bb24a34c35a2050758bc67bb9e.cn-hangzhou.alicontainer.com/hostName 通过服务路由访问hello服务：http://gateway-eureka.c066372bb24a34c35a2050758bc67bb9e.cn-hangzhou.alicontainer.com/route/hello/hostName 通过服务路由访问login服务：http://gateway-eureka.c066372bb24a34c35a2050758bc67bb9e.cn-hangzhou.alicontainer.com/route/login/hostName 服务间调用hello调login：http://gateway-eureka.c066372bb24a34c35a2050758bc67bb9e.cn-hangzhou.alicontainer.com/route/hello/list 负载均衡 Ribbon简单来说 是一个基于 HTTP 和 TCP 客户端的负载均衡器，它可以在客户端配置 ribbonServerList（服务端列表），然后轮询请求以实现均衡负载。 Feign是一个使用起来更加方便的 HTTP 客户端，使用起来就像是调用自身工程的方法，而感觉不到是调用远程方法，达到类rpc调用的效果。 Ribbon和Feign可以封装组合使用（Feign封装了Ribbon）。 使用Ribbon客户端调用http接口，具有客户端负载均衡的能力 使用Feign做到类rpc调用 基于http做到类rpc调用的异常机制 隔离 hystrix熔断器，负载服务故障的隔离。 RestTemplate --&gt; Ribbon --&gt; hystrix traceId的透传 结合EagleEye.getTraceId()获得traceId 对于EagleEye，traceId是放ThreadLocal。基于SpringCloud http的方式，可以http header。在http header中透传 全链路跟踪（轻量鹰眼） 使用鹰眼生成全局唯一traceId，以及traceId机制。每台日志对请求打点，然后结合ELK，做到日志全链路查询 灰度 灰度发布 依赖k8s的内置支持 流量灰度 AB测试 对用户打标，内测发布 beta发布 开发方式 使用fabric8 maven插件完成 打包、上传、部署 mvn clean package fabric8:build -Pdev // 指定环境变量，打包docker镜像 docker tag xxx registry.cn-hangzhou.aliyuncs.com/cmp-poc/xxx // 重命名docker镜像 docker push registry.cn-hangzhou.aliyuncs.com/cmp-poc/xxx // push到私有仓库 mvn fabric8:deploy // 使用kubectl create depoly &amp; service ","link":"https://phantomma.top/post/springcloud-on-k8s-demo/"},{"title":"kotlin 快速浏览","content":"Kotlin是啥 Kotlin是一门JVM上的，与Java完美互操作，简洁、易读、安全、通用为设计目标，同时支持面向对象和函数式两种编程范式，高质量的现代静态类型语言。或者换一句话，是集成了Java（面向对象，强大生态），C#（美妙的扩展方法），Ruby（魔法般的Code Block），Scala（函数式编程，克制的运算符重载，易用版Trait）等众多语言优点的高性能的美妙语言。 Kotlin「简历」 来自于著名的IntelliJ IDEA的软件开发公司 JetBrains (位于东欧捷克) 起源来自 JetBrains 的圣彼得堡团队，名称取自圣彼得堡附近的一个小岛 (Kotlin Island) 一种基于 JVM 的静态类型编程语言 增长情况 有啥亮点 安身立命之本--互操作性和Java生态 无缝引入到现有java项目 java -&gt; kotlin ✔️ kotlin -&gt; java ✔️ 安卓阵营的拯救者 谷歌和Oracle的Java侵权案 安卓Dalvik虚拟机智能使用jdk7以下api kotlin被谷歌钦定官方推荐语言 编写更安全、易读的代码 var a: String = “abc”; // 定义个一个非null的字符串变量a a = null; // 编译直接失败 var b: String? = “abc”; // 定义一个可为null的字符串变量b b = null; // 编译通过 val l = b.length; // 编译失败，因为b可能为null l = b?.length ?: -1 // 如b为null，就返回-1 l = b?.length; // 如b为null，就返回null l = b!!.length; // 如b为null，就会直接抛NPE错误 b?.let { println(b) } // 如b为null，就不执行let后面的代码块 val nullableList: List&lt;Int?&gt; = listOf(1, 2, null, 4) val intList: List&lt;Int&gt; = nullableList.filterNotNull() // 过滤出列表中所有不为null的数据，组成新的队列intList // 可以通过lateinit var(不可为val)，定义一个无需在申明时初始化的non-nullable的参数，这个参数不允许被赋值为空，并且在调用时如果没有初始化会抛异常 lateinit var lateInitValue : String // 通过by lazy { ... } 表达式，让所定义的参数在第一次访问(get)的时候执行{...}这段代码块，并赋值 val lazyValue: String by lazy { doAnything() &quot;build lazy value&quot; } // 可以直接在赋值中使用表达式，甚至内嵌执行语句 val max = if (a &gt; b) a else b // 三元表达式 val max = if (a &gt; b) { print(&quot;Choose a&quot;) a } else { print(&quot;Choose b&quot;) b } // 支持when的表达式 println(when (language) { &quot;EN&quot; -&gt; &quot;Hello!&quot; &quot;FR&quot; -&gt; &quot;Salut!&quot; else -&gt; &quot;Sorry, I can't greet you in $language yet&quot; }) // 支持in，表达在一定的范围内作为条件 when (x) { in 1..10 -&gt; print(&quot;x is in the range&quot;) in validNumberArray -&gt; print(&quot;x is valid&quot;) else -&gt; print(&quot;none of the above&quot;) } 数据类 一行代码搞定 pojo, 自动生成 get/set/toString, 类似 lombok 插件 data class User(val name: String, val age: Int) 更好用的函数式编程 原生支持的不可变对象 更好用的lambda表达式，且性能更高（提升30%） 比jdk8 stream api更好用的集合操作 val numbers = arrayListOf(-42, 17, 13, -9, 12) //创建一个List，并给定值 val nonNegative = numbers.filter { it &gt;= 0 } //从numbers中过滤出&gt;=0的队列 listOf(1, 2, 3, 4) // 列出 1, 2, 3, 4 .map { it * 10 } // 所有值乘以10 10, 20, 30, 40 .filter { it &gt; 20 } // 过滤出&gt;20的值 30, 40 .forEach { print(it) } // 打印出每个值 30, 40 非受检异常 unchecked exception，可以自己决定在调用栈的某一层 catch 和处理，不必到处 try-catch 字符串模板 val s = &quot;abc&quot; val str = &quot;$s.length is ${s.length}&quot; // 结果为 &quot;abc.length is 3&quot; val x = 4 val y = 7 print(&quot;sum of $x and $y is ${x + y}&quot;) // sum of 4 and 7 is 11 类似一个更智能、更易读的Java版本的String.format() 类扩展 fun MutableList&lt;Int&gt;.swap(index1: Int, index2: Int) { val tmp = this[index1] // 'this' 对应该列表 this[index1] = this[index2] this[index2] = tmp } } //使用 val l = mutableListOf(1, 2, 3) l.swap(0, 2) // 'swap()' 内部的 'this' 得到 'l' 的值 免费的午餐--性能无损 Kotlin增加特性的同时，没有降低性能，部分benchmark中相比java还有微弱的性能提升，马儿跑得更快还吃的更少。 Kotlin运行时性能 DSL——去敌人的地盘吃鸡 Kotlin因为其灵活的语法（比如可以重定义运算符），适合用来写DSL，比如：用Kotlin写HTML模板，anko 安卓绑定控件类库。 kotlin协程 kotlin 1.3发布了稳定版的协程 让原本需要异步+回调的才能获得高性能的场景，可以通过使用看似同步的方式来编写代码 java平台上，在loom项目推出之前，最容易体验到协程遍历的途径 Coroutine的四大金刚操作：launch, runBlocking, async和await。 launch。launch最简单，只负责启动，启动完就不管了，既不等待结束，也不关心结果。 runBlocking。runBlocking在运行代码块之后会阻塞当前coroutine，直到代码块运行完成，然后获取结果。 async和await。launch和runBlocking都相对简单，如果我要启动coroutine，然后立马返回，但是又想关心结果怎么办呢，用async。async会返回一个Deferred，T是结果类型，然后你可以做别的，需要的时候，用Deferred上的await()函数来等待结果。注意await()本身也是suspend函数，你需要把它放在coroutine里面（launch，runBlocking，async调用的代码块）里面，或者将代码所在的函数标为suspend。 四大金刚操作的比较： 回调 vs 响应式编程 vs 协程 node.js reactive coroutine 如何在项目中使用kotlin idea（或eclipse）较新的版本+kotlin插件 添加kotlin依赖 &lt;dependency&gt; &lt;groupId&gt;org.jetbrains.kotlin&lt;/groupId&gt; &lt;artifactId&gt;kotlin-stdlib-jdk8&lt;/artifactId&gt; &lt;version&gt;${kotlin.version}&lt;/version&gt; &lt;/dependency&gt; 混合编程 现阶段的不足 ide代码提示还会有些卡顿 与lombok不兼容（看不到lombok增强后的方法） 相关资料 基本语法 习惯用法 与java互操作 kotlin与java语法对比 kotlin增长报告 代码review 正反向关系表同步，逆向表查询索引，根据id回主表查询。（新加坡vpc有精卫环境） lombok标记Data类，另外可选kotlin的data对象 使用pandora boot。定制了一个消费多个topic的MultiMetaqListener jdk8新特性，stream kotlin混合编程 改良版geo hash简单介绍 lwp、hsf接口测试方案 ","link":"https://phantomma.top/post/kotlin-kuai-su-liu-lan/"},{"title":"个人效率提升工具（Mac）","content":"Mac个人软件 Raycast 对于没有使用过Alfred的同学，可以考虑直接使用Raycast rust编写，效率更高 类Alfred的搜索功能，除了搜索（文件、app、日历、代办登），还集成了很多其它小功能，如：剪贴板历史、计算器。。 Xnip 截图并贴图 免费版基本够用，轻量不占资源 The archiver 轻量干净的解压缩软件 UPDF 现代强大的pdf阅读软件 iTerm 全彩色终端 掌握快速唤起、隐藏终端 split新的窗口，以及在之间切换 终端增强：OhMyZsh + PowerLevel10K 安装zsh插件plugins=( git zsh-completions zsh-autosuggestions zsh-syntax-highlighting ) Maccy 非常轻量的剪贴板历史工具。只保存纯文本 有了Raycast，就没有必要了 RunCat mac菜单栏工具，一只奔跑的cat，显示一些系统运行状态 Stats 免费软件，功能和界面基本做到了付费软件 iStatMenu一致的水平 比上面的runcat功能强大，不止cpu，内存、网络、磁盘都可以监控显示 Bob翻译 划词翻译 可设置第三方翻译服务 OCR识别 motrix 下载工具 支持多线程，BT下载 XMind 本地思维导图，免费版基本够用 AltTab cmd + Tab 窗口切换之外的一个补充工具 可以在同一app下的多个窗口之间切换 Gridea 静态博客app。自带一个客户端，可以直接在app内编辑blog，提供了一个markdown编辑器，还算好用 支持上传图片到本地git仓库，渲染时自动替换为相对路径 自动github同步功能，写完一键同步至github page 不过最近停止更新了。软件包还是intel的打包 Xnip 试用下来，满足我当前需求的最佳截图软件。比较，iShot、Paste等，更原生，且轻量 具备钉图功能，图标标注也不错 重要的是，appstore上免费 ClashX 科学上网 结合fastlink梯子。https://v02.fl-aff.com/auth/register?code=5X1E 体验下来，最好的版本 ShadowSocketsX-NG 同上 比叫轻量，界面更像mac软件 HiddenBar macbook 刘海屏，拯救状态栏被刘海隐藏的图标 Bartender 同上，功能更强大一些。可以把隐藏图标单独显示成一栏 CheatSheet 快捷键作弊表 长按cmd键，显示当前应用的快捷键列表 UPDF 国产pdf软件，功能是真的强大 编辑功能，ocr识别等 Notion 支持markdown语法，同时编辑时可见即所得 支持图片的大小调整，比较适合码农写技术文章 方便在文档里编辑表格。这对原生markdown在表格方便的弱鸡功能，做了很大的增强 OmniGraffle 很适合画软件架构图 提供丰富的模版和内置图形 ShortCutDetective 快捷键冲突检测工具 Silicon 扫描电脑上的app，显示是否针对apple m芯片单独编译的 How to Tell Which Mac Apps Are Optimized for Apple Silicon brew 安装的工具 首先是安装 homebrew 通过brew 安装的工具 brew install wget brew install jq brew install git brew install tree brew install jenv brew install maven 开发工具 JDK 推荐Oracle JDK jenv 管理多个jdk版本的命令行工具 IDEA java IDE VS Code 轻量IDE，可装各种插件，支持各种语言 OrbStack 可以创建vm 可以创建docker 自带一个k8s集群 虚拟化方案针对mac平台专门优化，不想DockerDesktop底层以来一个linux的vm Lens 功能强大的k8s ide 能够显示所有k8s内置资源类型，以及自定义CRDs 不过也有个小bug。使用长时间后，肯跟会堵塞网络，导致浏览器也无法上网。解决办法，重启应用 就好了 SaaS服务 Poe quora提供的聚合各种AI机器人的网站，也支持ChatGPT机器人，速度比较块，也不想OpenAI对使用者IP region限制比较严格 AI论文阅读 Humata：对输入的论文理解表现是AI中比较出色的，缺点是用多了就得付费 ChatPDF：免费的PDF学习AI，理解力比Humata差点，不过完全免费 推荐一个mac软件网站：https://haxmac.cc/bartender-crack-mac/ 。。。TODO ","link":"https://phantomma.top/post/ge-ren-xiao-lu-ti-sheng-gong-ju-mac/"},{"title":"MySQL Binlog 源码学习","content":"MySQL主从节点之间同步数据，备份点还原，会用到binary log，其负责记录数据更改的操作。因为Binlog在运用到数据页之前需要经过复杂的过程，没有redolog直接，所以性能比不上直接使用redo复制的方式（物理复制的优势），但是它也有不可或缺的作用。本文重点介绍MySQL Binlog的作用、记录的内容、组织结构、写入方式、主备复制等内容，基于MySQL 8.0的代码。因为网上对Binlog各个知识点的介绍都非常详细，但是知识点非常杂，所以给本人初学Binlog的时候带来很多困难，因此本文的目的是总结这些知识点。 本文内容基于 MySQL Community 8.0.13 Version 为什么要有Binlog MySQL上下分为SQL层和引擎层，不同存储引擎中的日志格式是不同的，由于要对多引擎支持，必须在SQL层设计逻辑日志以透明化不同存储引擎，而这个逻辑日志就是Binlog。当有数据修改请求时，primary会产生包含该修改操作的Binlog，并发送给replica，replica通过回放该Binlog以执行和primary同样的修改。此外还可用于备份点还原。 在PolarDB中，虽然通过物理复制可以完成上面的功能，但是MySQL生态中用户需要Binlog导出数据库做审计、数据校验、数据清理等操作；以及用户混合使用多种数据库搭建业务平台也需要Binlog完成不同数据库之间数据传输；还有某些数据备份工具例如阿里云DTS、第三方的OGG（Oracle）、开源的canal/open-replicator、以及MySQL自带的mysqlbinlog等仍然依赖Binlog。所以PolarDB也支持Binlog，但是其通过Logic Redo的方式将Binlog写入redo中来提升性能。 Binlog的记录格式 显然，server记录Binlog要尽量少，因为对数据库的修改只有在其Binlog落盘后才算成功，同时还要保证在主从上执行的同一语句的结果相同。所以Mysql提供了三种记录Binlog的格式：基于语句的（statement-based logging），基于行的（row-based logging）和混合的（mixed logging）。可以通过binlog-format指定。 在介绍Binlog类型前先说下什么是非确定性的语句（ non-deterministic），即同一条语句在集群的不同server上执行的结果不同，举个例子：UUID()，如果在某个修改操作的SQL中使用了这个语句，那么在不同server上的效果是不同的。 基于语句的方式会直接记录SQL语句，这种方式产生的Binlog少，占用磁盘空间和I/O也最少，此外主从复制的数据也少，审计数据库更改也更加方便，缺点是无法用于非确定性的语句（ non-deterministic）；基于行的方式记录了对表中某个行的修改，这种方式因为是直接复制整个行，所以可以避免上面的问题，此外需要拿的行锁也更少，缺点就是日志本身占用空间更大，采用二进制记录格式不易审计；混合的方式会根据操作类型（切换原则）切换使用这两种方式。此外DDL操作即使在row格式下也是记录SQL语句的。 所以如果存储空间和I/O不是主要问题，最好使用基于行的记录格式，因为这样更加安全。 最后放一个测试的demo，让大家更好的从具体SQL去理解这三种记录格式。本文接下来只介绍row-based logging。 Binlog文件里面是什么 Binlog的版本为4，以未加密的Binlog为例，布局如图1所示，Binlog的开头分别由MAGIC HEADER、FORMAT_DESCRIPTION_EVENT和PREVIOUS_GTIDS_LOG_EVENT构成。后面就是一个个其他Binlog Event了。注意是否开启GTID都是这样的布局，区别是若未开启GTID（gtid_mode=OFF），则previous_gtids_log_event和gtid_log_event记录的gtid为空。 图1 开启GTID且未加密的Binlog文件 Binlog Event结构 前面说了Binlog的格式，下面说下Binlog文件的内容，首先介绍各种Binlog Event，每个Event分为Event Header、Post-Header、Payload、Event Footer四部分，如图2所示。 图2 单个Binlog Event结构 Event Header 内容类型一样，占用19bytes，对应类，内容如图3，图4所示： 图3 Event Header 内存结构 图4 Event Header 各个字段含义 Event Footer记录计算event checksum的算法信息，这个信息也记录在FDE：FORMAT_DESCRIPTION_EVENT中，同一Binlog文件中的该信息一样，对应Log_event_footer类。Post-Header、Payload分别是每个Event 类型的Header和内容实体，不同Event种类不同。下面介绍几个典型event_type。 Binlog Event类型 FORMAT_DESCRIPTION_EVENT 该event写在Binlog文件开始4字节的位置，紧挨着magic。用于描述binlog的layout和解码Binlog Event。该类型中Post-Header、Payload指的是同样的内容。记录了Binlog Version、Mysql Server Version和Create Timestamp信息。 PREVIOUS_GTIDS_LOG_EVENT GTID在后面介绍，这里只用知道这个Event中涵盖了该Binlog之前所有Binlog文件中（包括已经被删除的）事务的GTID，也就是说记录了所有被执行的事务的GTID。为什么说是被执行了的？因为在Binlog rotate出新的文件前，旧文件的事务会被提交或者回滚，保留下来的一定被提交了。 GTID_LOG_EVENT GTID唯一的对应一个事务。因为Binlog是逻辑层日志，本身不幂等，所以为了防止一个事务被多次执行，每个事务都需要有一个全局的事务标识——Global Transaction IDentifier（GTID）。GTID由全局事务标识的UUID和递增的Group number组成。该Event中还记录了事务在不同server上的提交时间，更详细的可以查看MySQL GTID EVENT。 QUERY_EVENT 基于statement格式的对数据修改操作都是以这种Binlog类型记录，此外，DDL也是以这种类型记录的。这里不详细展开了，详见官方文档。 WRITE_ROWS_EVENT、UPDATE_ROWS_EVENT、DELETE_ROWS_EVENT Row记录格式下，对表的修改会产生这些类型的Binlog。 XID_EVENT 在XA事务commit时记录，标识事务的结尾，其中XID是事务号，由一个8位无符号整型表示。在recover时会根据它判断Binlog所记录的XA事务是否完整，注意XID和GTID所描述的不同，XID是对上层应用而言的事务号，关系到事务能否原子的执行；GTID为保证集群内的一致性，关系到事务能否在集群中的所有server上有且只有一次执行。 小结 介绍完Event类型后，很容易理解在基于Row格式记录的Binlog中，Event往往以图5这两种形式组合排布，图5（左）中的QUERY_EVENT记录的内容是执行的具体SQL，图5右中的QUERY_EVENT记录的是BEGIN，标识事务的开启。在下一节中，我会结合Binlog文件内容介绍Binlog是如何高效，安全的写入磁盘的，以及如何与物理层日志之间保持一致性。 图5 DDL（左）/DML（右） Binlog事务 Binlog是如何写入的 由于MySQL的SQL和引擎层的双日志体系，Binlog写入需要解决多个引擎之间事务执行的一致性问题。此外，由于从日志产生到落盘是数据库写入的关键路径，所以写入的效率也是需要关注的。下面我就从这两个方面来介绍Binlog的写入过程。 分布式事务模型——XA XA源于Distributed Transaction Processing: The XA Specification，这篇文章定义了分布式事务处理模型，其中定义了事务管理器（充当协调者），负责为事务分配标识符，监视它们在不同参与者上执行的进度，并负责事务完成和故障恢复。 还定义了资源管理器，充当参与者，受协调者管理。此外还有应用程序，充当事务的发起者。 MySQL中的XA类型以及协调者选择 在MySQL中，如果事务的参与者是各个实例节点，那么是外部XA，由上层程序担当协调者，上层程序可以通过XA start，XA prepre，XA end，和XA commit的命令管理事务的执行。如果事务的参与者只在单实例节点内部，那么称为内部XA，例如参与者是Binlog和innodb。对于内部XA的协调者，如果开启Binlog，则Binlog为协调者，显然选择Binlog作为协调者是最合适的，因为Binlog位于引擎层之上且还负责主备之间数据的同步。如果不开Binlog，且只有innodb一个成员，那就不需要XA了。但是如果没有Binlog且在引擎层有多个参与者，那么MySQL会使用TC_LOG_MMAP作为协调者。XA采用两阶段提交协议保证分布式事务的一致性。两阶段提交分为prepare和commit两个阶段，协议的内容参考 分布式事务两阶段提交， 在Prepare阶段前，进入函数ha_commit_trans。这里有个参数all。’all为false’ 表示这是用户发出的显式提交，’all为true’表示是 DDL 发出的隐式提交。某些DDL在执行完成后会隐式提交，也就是无需用户调用commit等结束语句而自发提交，这就意味着一条DDL是一个单独的事务，用户无法回滚它，详见Statements That Cause an Implicit Commit。如果打开了autocommit，DML也会自发提交，详见 autocommit。所以XA事务有很多种情况（内部、外部、是否开Binlog、是否为DDL等），接下来主要介绍开启row_based格式的Binlog，开启GTID，存储引擎只有innodb的内部XA执行过程。以DDL和DML语句为例，整个过程如图6所示。 图6 由Binlog担任协调者的XA事务处理过程 Prepare阶段 prepare阶段分为binlog的prepare和innodb的prepare。进入binlog和innodb prepae前会设置durability_property = HA_IGNORE_DURABILITY, 表示在innodb prepare和finish_commit()时，不刷redo log到磁盘。 Binlog Prepare 入口：binlog_prepare 对于all为false的事务，会更新该事务的last_commited为此时most recently commited事务的sequence_number，sequence_number是Binlog提交的逻辑时间戳，可用于在slave节点上并行执行Binlog事务，生成和自增策略参考Binlog事务依赖策略。 Innodb Prepare 入口：innobase_xa_prepare 初始化Innodb事务，将事务的状态由TRX_STATE_ACTIVE设置为TRX_STATE_PREPARED，标志事务进入prepare阶段，在undo log page中写入TRX_UNDO_PREPARED状态，若有xid则会在undo中也记录xid信息。 Commit阶段 Commit成功意味着在当前事务中所做的更改是永久性的，并且对其他session可见。Commit阶段分为Binlog的Commit和Innodb的Commit。 Binlog Commit 入口：binlog_commit 在Commit之前，Binlog已经写入到局部Binlog（见4.2.2），Commit时只需在结尾写入Binlog事务结尾的标识，例如XID_EVENT，在recover的时候据此判断事务的完整性。因为每条DDL都会implicit提交，所以一个DDL事务只会记录一条QUERY EVENT，所以结尾不需要记录XID_EVENT就可判断DDL事务是否完整。 随后开始提交Binlog，也就是将各个thd的Binlog事务写到Binlog文件中，Binlog文件中事务之间要彼此独立的顺序排列，不会交错，因为交错的事务难以被slave apply。然而一个一个写binlog并落盘显然效率极低，为了提高效率，MySQL采用Group Commit的方式。整体过程在网上有很多讲解，本文主要从代码层面讲解具体的几个关键函数，关于Group Commit的实现方法本文不介绍了。Group Commit分为三个阶段flush、sync、commit。在flush阶段将redo log持久化，将Binlog 写到文件系统的page cache中。在sync阶段将Binlog刷盘。下面具体介绍（序号对应图中的序号）： （6）ending_trans()函数判断是否对本次事务进行提交，有四种情况，用户发起的一条DDL语句会分别执行显式（explicit）和隐式（implicit）的提交，若为显式，则事务不提交；若为隐式才真正提交，对DDL而言，其会在执行时将autocommit置为0，所以autocommit对DDL不生效，因为不论autocommit是否开启，DDL都会由server自发做提交（implicit commit）。若为DML，则若autocommit为1，自动提交，autocommit为0，则由用户手动提交。若为Begin语句，不论autocommit是否开启，都不提交。 （7）对于需要提交的事务，如果是DML，会在trx_cache结尾append一个Xid_log_event。随后进入ordered_commit。 （8）进入flush阶段，change_stage将线程入队。然后由leader执行process_flush_stage_queue，这里先刷innobase层的日志，也就是刷redo(innobase_flush_logs)，如果innodb_flush_log_at_trx_commit为1，则这里将redo落盘。 （9）assign_automatic_gtids_to_flush_group为每个thd生成GTID。 （10）flush_thread_caches，首先将上一步生成的GTID写到全局Binlog中，然后将局部binlog刷到全局Binlog，此时数据还在IO_CACHE结构中。thd的binlog_cache_mngr管理两种局部Binlog event缓存：stmt_cache和trx_cache，前者记录非事务性Binlog，后者记录事务型Binlog。XA事务中只有trx_cache有数据。所有局部Binlog flush完后判断是否需要rotate，若需要，将在ordered_commit最后完成。 （11）flush_cache_to_file将IO_CACHE中Binlog write到文件。 （12）进入sync阶段，sync_period用于控制sync的周期，比如经过几次flush后做一次sync。sync_binlog_file将Binlog落盘。由配置参数sync_binlog控制。 （13）如果sync_period为1，则sync_binlog_file完更新atomic_binlog_end_pos，这个参数标识binlog结尾。如果sync_period不为1，则flush完就更新atomic_binlog_end_pos。 （14）进入commit阶段，该阶段主要执行finish_commit，如果opt_binlog_order_commits==false，那么事务就不按照之前的顺序，各自进行提交(finish_commit)，这种情况下不能保证innodb commit顺序和binlog写入顺序一致，这不会影响到数据一致性，在高并发场景下还能提升一定的吞吐量。但可能影响到物理备份的数据一致性，例如xtrabackup（而不是基于其上的innobackup脚本）依赖于事务页上记录的binlog的end位点（flush_thread_caches会更新），如果位点发生乱序，就会导致备份的数据不一致。 （15）执行finish_commit， update_max_committed更新最大commit事务的序号。 （16）分别执行Binlog和innodb的commit。Binlog Commit在前面已经完成了，所以这里什么也不做。实际只有Innodb的Commit。 （17）dec_prep_xids: 清除 m_atomic_prep_xids，rotate Binlog时通过它判断当前Binlog是否有正在提交的事务。 （18）将commit事务的GTID加入executed_gtids。 （19）在第10步判断的，如果Binlog文件大小超过了max_binlog_size，则会rotate新的Binlog。 Innodb Commit 入口：innobase_commit 将undo头的状态修改为TRX_UNDO_CACHED或TRX_UNDO_TO_FREE或TRX_UNDO_TO_PURGE (undo相关知识参阅之前的月报)；并释放事务锁，清理读写事务链表、readview等一系列操作。每个事务在commit阶段也会去更新事务页的binlog位点。然后根据该session已执行的GTID去更新全局GTID SET。在8.0.17版本会将GTID持久化到undo日志中（原因）。 写入效率 本节介绍Binlog写入之前，先介绍IO_CACHE结构，该结构贯穿了任何与Binlog相关文件（index文件，purge_index_file, crash_safe_index_file等）的读写过程，随后介绍XA过程中局部的Binlog和全局的Binlog。最后介绍仍然存在的性能瓶颈和解决方案。 IO_CACHE 文件系统虽然向上呈现一段连续的空间，但是其内部以页的形式管理，页的大小通常为4K，满足4K对齐的读写对文件系统的性能会有很大的提高。而IO_CACHE的作用就是充当一层缓存，将连续的数据写入进行4K对齐后写入文件系统。 知道了IO_CACHE的作用后，来看看其Binlog是如何利用它的。Binlog文件初始化的过程如下： class IO_CACHE_ostream { bool IO_CACHE_ostream::open() { file = mysql_file_open(log_file_key, file_name, O_CREAT | O_WRONLY, MYF(MY_WME)); init_io_cache(&amp;m_io_cache, file, cache_size, WRITE_CACHE, 0, 0, flags); } IO_CACHE m_io_cache; } init_io_cache | ----&gt;init_io_cache_ext(){ info-&gt;file = file; info-&gt;buffer = (uchar *)my_malloc(key_memory_IO_CACHE, buffer_block, flags); init_functions(info); } 可以看出MySQL在打开Binlog文件后将文件描述符交给IO_CACHE结构管理，IO_CACHE初始化过程中，会申请一个缓冲，默认大小是8K，随后计算读写缓冲区的位点以便对齐写入，还定义了对IO_CACHE的读写函数。IO_CACHE详见 IO_CACHE源码解析。如图7所示，IO_CACHE会对齐PageCache进行写入，满足对齐条件后就会刷到Page Cache中，之后sync到Binlog文件。 图7 数据写IO_CACHE的过程 局部Binlog和全局Binlog Binlog中的事务是顺序独立的，不能交错，原因是交错的Binlog事务无法被slave重放。但是多个客户端连接MySQL，并对其并发写入的场景经常出现。为了解决高并发过程中顺序写入的问题，MySQL为每个连接都配置了一个局部的Binlog文件，各个连接产生的Binlog会事先写到各个局部Binlog中，等到group commit时再将各个局部Binlog合并到全局Binlog文件中。 局部Binlog 局部Binlog通过Binlog_cache_storage结构管理，实际上也是对IO_CACHE结构的包裹，可以通过binlog_cache_size来控制它的大小，如果事务的binlog日志大小超出了binlog_cache_size的定义的大小，多出来的部分会存在临时文件中，但是事务总大小不能超过max_binlog_cache_size。上面我们说到IO_CACHE在初始化的时候会关联一个磁盘文件，这里也不例外，但是这里特殊在是临时文件，通过下面这个函数创建。 bool real_open_cached_file(IO_CACHE *cache) { if ((cache-&gt;file = mysql_file_create_temp( cache-&gt;file_key, name_buff, cache-&gt;dir, cache-&gt;prefix, (O_RDWR | O_TRUNC), MYF(MY_WME))) &gt;= 0) { error = 0; /* Remove an open tempfile so that it doesn't survive if we crash. */ (void)my_delete(name_buff, MYF(MY_WME)); } } 该文件以“ML”为前缀，如果创建成功，则会被立刻删除，但是由于文件的描述符并没有被释放，所以该文件依然能被读写，当程序crash后，该文件会被真正的删除。由于其中保留的数据未提交，所以重启后无需恢复，其实删除就是最好的恢复。 全局Binlog 全局Binlog是当前打开的Binlog，在MySQL启动时构造在m_binlog_file变量中，管理Binlog写入流。其底层依然是通过IO_CACHE管理文件写入的。 group commit 在SQL执行的过程中，Binlog会伴随着产生并写入到局部Binlog中，在xa事务提交时，局部Binlog中的事务会被顺序拷贝到全局Binlog中。关于Group Commit可以参考4.1.3和MySQL组提交，图8展示了Binlog Event写入局部Binlog，并在提交时由局部Binlog拷贝至全局Binlog的过程。 /*将局部Binlog拷贝到全局Binlog*/ bool MYSQL_BIN_LOG::do_write_cache(Binlog_cache_storage *cache, Binlog_event_writer *writer) { cache-&gt;copy_to(writer, &amp;error) } 图8 Binlog Event写入局部Binlog和全局Binlog 性能问题 开启Binlog后的性能一直被诟病，对于AWS Aurora在开启Binlog后，通常有50%到60%的性能损耗；对于PolarDB也差不多是这个数值；Oracle则不写入Binlog，而是通过物理redo日志去生成Binlog。即使Binlog会带来如此严重的性能问题，但它仍然在业务中不可或缺的。所以数据库厂商采取了一些方法去解决这些问题。本节将介绍PolarDB和Aurora是如何提升Binlog性能的。 PolarDB 考虑到在Group Commit的过程中，在flush阶段redo会被sync到磁盘，在sync阶段Binlog会被sync到磁盘，这两个过程是串行的，两次对云盘的写入会造成很大的性能损耗，所以PolarDB采用logic redo的方法，将Binlog数据记录到redo日志中，在sync阶段，将redo和Binlog一起刷盘。关于Logic的详细介绍，可以移步 Logic redo。 Aurora Aurora面临的问题一样，瓶颈依然在全局Binlog sync到磁盘的时候。但是它的切入点和PolarDB不同，它的做法是enhance Binlog：将全局Binlog的sync过程打散到SQL执行的过程中，将局部Binlog下推到存储节点，这样在SQL执行过程中，Binlog就向存储节点写入，等到最后提交时，只需要存储节点对这些局部Binlog进行合并即可。详见 AWS re:Invent2022 Aurora 发布了啥 Binlog Recover 说完了Binlog写入过程，很容易想到如果写入过程中程序崩溃了怎么办，所以下面将介绍Binlog的Recover过程。Recover是基于xa过程的，本质上是根据已经落盘的Binlog决定如何处理未提交的事务。因为rotate新的Binlog时会recover老的Binlog中所有事务，因此在Binlog启动时，只需对最新的Binlog文件执行Recover即可，对于某个事务而言，如果它记录的Binlog是完整的（关于完整的Binlog事务参考第三章小结部分），说明它可以提交，反之，如果缺失任意一条Event都是不完整的Binlog，不完整的Binlog会被删除，与之关联的事务（binlog事务，innodb的事务）都会回滚。 XID 在前面介绍Binlog事务和XA过程的时候，可知每个Binlog事务都有个对应XID，对于非DDL Binlog事务，XID会以XID_EVENT的类型在事务提交时写到事务结尾；对于DDL事务，XID包含在DDL所在的QUERY_EVENT里。这个XID其实是xa事务的id，唯一标识每个xa事务。在xa过程的Innodb prepare时，会设置事务的状态为prepare，并记录在undo page中。这样Binlog recover时候根据xid能去Innodb层找哪些事务是prepare状态的，对这些事务提交或回滚。 recover 入口：int MYSQL_BIN_LOG::open_binlog 打开index 文件中最后一个 binlog，若该文件没有正常关闭（LOG_EVENT_BINLOG_IN_USE_F 置位），则recover它。从头开始，挨个扫描每个Binlog Event，只要发现某个Binlog事务不完整，那么该Binlog和其后面的Binlog都会被truncate。前面完整的Binlog的事务依据它们的xid去innodb层提交，其他事务进行回滚。原因是事务的binlog已经完整落盘，所以redolog也落盘了，该事务是可提交的。至此Binlog的recover完成，但是为了体现Binlog是参与者，之后会调用空函数binlog_dummy_recover()，该函数为空，因而后续的也不会调用commit和rollback函数。实际只进行innodb的recover。 innodb的recover函数为innobase_xa_recover()，函数的主要目的是找到innobase层所有prepare状态的事务，这些事务的XID与前面Binlog找到的XID进行比对，从而决定哪些需要回滚innobase_rollback_by_xid，哪些需要提交innobase_commit_by_xid。提交和回滚可参考innodb事务系统。 最后，Binlog会truncate到保留最后一个完整的事务，清除LOG_EVENT_BINLOG_IN_USE_F，表示binlog文件正常关闭，并rotate出一个新的Binlog进行写入。 recover讲完，primary上的Binlog基本讲完了，下面将介绍Binlog是如何完成数据同步的——Binlog复制。 Binlog复制 首先需要建立连接，MySQL将对应的连接称为channel，slave节点通过change master指令可以与master建立一个channel并对其命名，change master指令可以指定复制开始文件和位点。随后由slave发起start slave开启复制，slave可以为所有channel都开启复制（start slave）,也可以只为特定的channel开启复制（start slave for channel ‘channel_1’），开启复制是通过建立复制Binlog的IO线程和对其回放的SQL线程，本文不讨论SQL线程。下面来看看连接建立与复制过程，如图9所示。 图9 连接建立与复制过程 （1）这里根据设置的thread_mask会启动相应的线程：handle_slave_io或handle_slave_sql线程。这里介绍handle_slave_io线程。handle_slave_sql是slave回放binlog的线程，执行完线程启动后，在handle_slave_io线程初始化完并被加到thd_manager后，客户端就能收到该指令执行的响应了。连接master的操作在后续执行。 （2）slave通过safe_connect-&gt;connect_to_master与master建立连接。 （3）slave向master发送COM_REGISTER_SLAVE指令，在master端register_slave，检查slave的权限，将其serverid、host、user、passwd等信息放在slave_list结构中。 （4）slave发起dump请求，command是COM_BINLOG_DUMP，若开启gtid和auto_position(GTID Auto-Positioning)，通过设置gtid_mode=ON和在change master时指定MASTER_AUTO_POSITION=1，则使用GTID复制，command是COM_BINLOG_DUMP_GTID，区别是后者会发送slave上的m_exclude_gtid（slave上已有的Binlog事务gtid集合），master只会复制不在该gtid集合中的Binlog事务。 （5）read_event调用mysql_binlog_fetch读取从master发来的packet，阻塞等待。 （6）master响应request_dump()发起的COM_BINLOG_DUMP(_GTID)请求，如图10所示。 - 首先初始化Binlog sender，如果slave未指定复制起始位点（change master指令可指定位点，还有是位点会保存在master.info文件，由slave启动时读取。），则在sender init时初始化位点为index文件中第一个binlog文件的第一个event位置（pos=4）。 - 然后打开Binlog文件send_binlog，在该函数中：1）函数get_binlog_end_pos会判断当前正在复制的Binlog文件是否和全局Binlog文件相同，如果不同说明该文件不是最后一个Binlog，复制完该文件后需要rotate到下一个继续复制。如果相同，则复制完后会等待该Binlog文件中新的写入（end_pos更新）。2）函数send_events()一次读一个完整的event并发送，如果开启GTID和auto_position，则不发送gtid包含在m_exclude_gtid中的事务。如果没有event发送，则会等待超时并发送heartbeat event，heatbeat还可用于告知slave：master复制位点； 图10 master响应com_binlog_dump （7）来自master的Binlog会被存储到slave的relay log中，通常slave的Binlog被称为relay log。后续SQL线程会解析并应用relay log。 杂记——Binlog相关文件 Binlog文件 图11 Binlog文件 Binlog文件命名由log_bin_log或log-bin指定，这里假定为binlog，后面的例子中也一样，如图11所示，写入方式为顺序追加写。通过mysqlbinlog可以查看文件内容。 Index文件 每行都是Binlog文件名。如图12所示。 图12 Binlog Index文件内容 crash_safe_index_file 临时文件，内容为Binlog文件名。保证了修改index文件时，写入的Binlog文件名是原子的，图13是在index文件中写文件名的一个例子，可以很容易看出crash_safe_index_file的功能。该文件命名方式为：./binlog.index_crash_safe 图13 在index文件中写文件名的过程 purge_index_file 临时文件，内容为Binlog文件名。保证index内容和Binlog文件互相匹配。图14展示的是新建Binlog文件时需要在purge_index_file文件中写入一条新的文件名，在Binlog文件创建完成后将该Binlog文件名写入index文件，随后删除purge_index_file。由此可见该文件中记录的Binlog文件名都是在创建过程中并且还未来得及被记录在index文件的Binlog文件，所以每次打开index文件时，会检查并删除purge_index_file中记录的Binlog文件，避免index文件和Binlog文件不匹配。该文件的命名方式为：./binlog.rec 图14 新建Binlog文件过程 物理复制解读 MySQL Replication Events – Statement versus Row-Based Formats Mysql Binlog Event InnoDB 事务子系统介绍 ","link":"https://phantomma.top/post/mysql-binlog-yuan-ma-xue-xi/"},{"title":"系统压力测试工具之 sysbench","content":"sysbench 是一个模块化的、跨平台、多线程、流行的开源基准测试工具，主要用于评估测试各种不同系统参数下的数据库负载情况 安装 软件版本: sysbench-1.0.20-6.el7.x86_64 curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.rpm.sh | sudo bash yum -y install sysbench 命令选项 sysbench --help --time # 压测时间 --threads # 压测线程数 --events # 请求数, 0 无限制 --rate # 请求速率, 0 无限制 --tables # 压测表数量 --table_size # 压测的单表大小,单位行 prepare # 准备测试数据 run # 开始压测 cleanup # 清除压测数据 report-interval # 每多少秒钟报告一次测试结果 File IO 压测 # 查看 fileio 测试模块下的帮助信息 sysbench fileio help # 准备测试数据, 生成多个测试文件 sysbench fileio --file-total-size=5G prepare # 运行测试 sysbench fileio --file-total-size=5G --file-test-mode=rndrw --time=30 --events=0 run # 清除测试数据 sysbench fileio --file-total-size=5G cleanup 测试结果如下: sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2) Running the test with following options: Number of threads: 1 Initializing random number generator from current time Extra file open flags: (none) 128 files, 40MiB each 5GiB total file size Block size 16KiB Number of IO requests: 0 Read/Write ratio for combined random IO test: 1.50 Periodic FSYNC enabled, calling fsync() each 100 requests. Calling fsync() at the end of test, Enabled. Using synchronous I/O mode Doing random r/w test Initializing worker threads... Threads started! File operations: # 磁盘io操作 reads/s: 47.68 writes/s: 31.79 fsyncs/s: 104.27 Throughput: # 磁盘吞吐量 read, MiB/s: 0.75 written, MiB/s: 0.50 General statistics: # 测试时间30s, 总请求数 5421 total time: 30.1950s total number of events: 5421 Latency (ms): # 延迟 min: 0.01 avg: 5.53 max: 181.94 95th percentile: 20.74 sum: 29988.08 Threads fairness: events (avg/stddev): 5421.0000/0.00 execution time (avg/stddev): 29.9881/0.00 MySQL 压测 读写测试: 3 个表、每个表 1000 行，测试时间 120s, 请求数及请求频率无限制，12 个线程 首次测试 准备测试数据 sysbench --test=/usr/share/sysbench/oltp_read_write.lua \\ --mysql-host=127.0.0.1 --mysql-port=3306 --mysql-user=root --mysql-password=root \\ --mysql-db=db01 --db-driver=mysql --report-interval=30 \\ --time=120 --threads=12 --events=0 --rate=0 --table_size=1000 --tables=3 \\ prepare # 输出如下 Creating table 'sbtest3'... Creating table 'sbtest1'... Creating table 'sbtest2'... Inserting 1000 records into 'sbtest3' Inserting 1000 records into 'sbtest2' Inserting 1000 records into 'sbtest1' Creating a secondary index on 'sbtest1'... Creating a secondary index on 'sbtest2'... Creating a secondary index on 'sbtest3'... 开始压测 sysbench --test=/usr/share/sysbench/oltp_read_write.lua \\ --mysql-host=127.0.0.1 --mysql-port=3306 --mysql-user=root --mysql-password=root \\ --mysql-db=db01 --db-driver=mysql --report-interval=30 \\ --time=120 --threads=12 --events=0 --rate=0 --table_size=1000 --tables=3 \\ run # 输出如下 [ 30s ] thds: 12 tps: 15.73 qps: 326.69 (r/w/o: 230.50/63.99/32.20) lat (ms,95%): 1771.29 err/s: 0.33 reconn/s: 0.00 [ 60s ] thds: 12 tps: 17.90 qps: 366.32 (r/w/o: 257.61/72.40/36.30) lat (ms,95%): 1678.14 err/s: 0.50 reconn/s: 0.00 [ 90s ] thds: 12 tps: 17.13 qps: 345.72 (r/w/o: 242.66/68.60/34.47) lat (ms,95%): 1618.78 err/s: 0.20 reconn/s: 0.00 [ 120s ] thds: 12 tps: 17.37 qps: 356.20 (r/w/o: 250.13/70.83/35.23) lat (ms,95%): 1561.52 err/s: 0.50 reconn/s: 0.00 SQL statistics: queries performed: read: 29428 write: 8292 other: 4158 total: 41878 transactions: 2056 (17.04 per sec.) queries: 41878 (346.99 per sec.) ignored errors: 46 (0.38 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 120.6877s total number of events: 2056 Latency (ms): min: 132.72 avg: 702.06 max: 3397.70 95th percentile: 1678.14 sum: 1443438.80 Threads fairness: events (avg/stddev): 171.3333/9.13 execution time (avg/stddev): 120.2866/0.21 清除压测数据 sysbench --test=/usr/share/sysbench/oltp_read_write.lua \\ --mysql-host=127.0.0.1 --mysql-port=3306 --mysql-user=root --mysql-password=root \\ --mysql-db=db01 --db-driver=mysql --report-interval=30 \\ --time=120 --threads=12 --events=0 --rate=0 --table_size=1000 --tables=3 \\ cleanup ","link":"https://phantomma.top/post/xi-tong-ya-li-ce-shi-gong-ju-zhi-sysbench/"},{"title":"frp 内网穿透","content":" config server side login into vps ssh root@155.254.193.168 cat frps.ini [common] bind_port = 7000 token = areyouok dashboard_port = 7500 dashboard_user = maksim dashboard_pwd = zxc123123 config auto start in deamon mod ubuntu or centos both support the configruation like this cat /etc/systemd/system/frps.service [Unit] Description=frps daemon After=syslog.target network.target Wants=network.target [Service] Type=simple ExecStart=/root/frp/frps -c /root/frp/frps.ini Restart= always RestartSec=1min [Install] WantedBy=multi-user.target config client side [common] server_addr = some_ip server_port = 7000 token = some_secret_string [ssh] type = tcp local_ip = 127.0.0.1 local_port = 22 remote_port = 6000 cat /etc/systemd/system/frpc.service [Unit] Description=frpc daemon After=syslog.target network.target Wants=network.target [Service] Type=simple ExecStart=/home/maxbee/frp/frpc -c /home/maxbee/frp/frpc.ini Restart= always RestartSec=1min ExecStop=/usr/bin/killall frpc [Install] WantedBy=multi-user.target 使用frp内网穿透进行ssh登录 通过 SSH 访问内网机器 ","link":"https://phantomma.top/post/frp-nei-wang-chuan-tou/"},{"title":"Kafka 体系架构分解","content":"基本概念 Kafka 体系架构 Kafka 体系架构包括若干 Producer、若干 Broker、若干 Consumer，以及一个 ZooKeeper 集群。 在 Kafka 中还有两个特别重要的概念—主题（Topic）与分区（Partition）。 Kafka 中的消息以主题为单位进行归类，生产者负责将消息发送到特定的主题（发送到 Kafka 集群中的每一条消息都要指定一个主题），而消费者负责订阅主题并进行消费。 主题是一个逻辑上的概念，它还可以细分为多个分区，一个分区只属于单个主题，很多时候也会把分区称为主题分区（Topic-Partition）。 Kafka 为分区引入了多副本（Replica）机制，通过增加副本数量可以提升容灾能力。同一分区的不同副本中保存的是相同的消息（在同一时刻，副本之间并非完全一样），副本之间是“一主多从”的关系，其中 leader 副本负责处理读写请求，follower 副本只负责与 leader 副本的消息同步。当 leader 副本出现故障时，从 follower 副本中重新选举新的 leader 副本对外提供服务。 如上图所示，Kafka 集群中有4个 broker，某个主题中有3个分区，且副本因子（即副本个数）也为3，如此每个分区便有1个 leader 副本和2个 follower 副本。 数据同步 分区中的所有副本统称为 AR（Assigned Replicas）。所有与 leader 副本保持一定程度同步的副本（包括 leader 副本在内）组成ISR（In-Sync Replicas），ISR 集合是 AR 集合中的一个子集。 与 leader 副本同步滞后过多的副本（不包括 leader 副本）组成 OSR（Out-of-Sync Replicas），由此可见，AR=ISR+OSR。在正常情况下，所有的 follower 副本都应该与 leader 副本保持一定程度的同步，即 AR=ISR，OSR 集合为空。 Leader 副本负责维护和跟踪 ISR 集合中所有 follower 副本的滞后状态，当 follower 副本落后太多或失效时，leader 副本会把它从 ISR 集合中剔除。默认情况下，当 leader 副本发生故障时，只有在 ISR 集合中的副本才有资格被选举为新的 leader。 HW 是 High Watermark 的缩写，俗称高水位，它标识了一个特定的消息偏移量（offset），消费者只能拉取到这个 offset 之前的消息。 LEO 是 Log End Offset 的缩写，它标识当前日志文件中下一条待写入消息的 offset。 如上图所示，第一条消息的 offset（LogStartOffset）为0，最后一条消息的 offset 为8，offset 为9的消息用虚线框表示，代表下一条待写入的消息。日志文件的 HW 为6，表示消费者只能拉取到 offset 在0至5之间的消息，而 offset 为6的消息对消费者而言是不可见的。 Kafka生产者客户端的整体结构 整个生产者客户端由两个线程协调运行，这两个线程分别为主线程和 Sender 线程（发送线程）。 在主线程中由 KafkaProducer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（RecordAccumulator，也称为消息收集器）中。Sender 线程负责从 RecordAccumulator 中获取消息并将其发送到 Kafka 中。 RecordAccumulator RecordAccumulator 主要用来缓存消息以便 Sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性能。 主线程中发送过来的消息都会被追加到 RecordAccumulator 的某个双端队列（Deque）中，在 RecordAccumulator 的内部为每个分区都维护了一个双端队列。 消息写入缓存时，追加到双端队列的尾部；Sender 读取消息时，从双端队列的头部读取。 Sender 从 RecordAccumulator 中获取缓存的消息之后，会进一步将原本&lt;分区, Deque&lt; ProducerBatch&gt;&gt; 的保存形式转变成 &lt;Node, List&lt; ProducerBatch&gt; 的形式，其中 Node 表示 Kafka 集群的 broker 节点。 KafkaProducer 要将此消息追加到指定主题的某个分区所对应的 leader 副本之前，首先需要知道主题的分区数量，然后经过计算得出（或者直接指定）目标分区，之后 KafkaProducer 需要知道目标分区的 leader 副本所在的 broker 节点的地址、端口等信息才能建立连接，最终才能将消息发送到 Kafka。 所以这里需要一个转换，对于网络连接来说，生产者客户端是与具体的 broker 节点建立的连接，也就是向具体的 broker 节点发送消息，而并不关心消息属于哪一个分区。 InFlightRequests 请求在从 Sender 线程发往 Kafka 之前还会保存到 InFlightRequests 中，InFlightRequests 保存对象的具体形式为 Map&lt;NodeId, Deque&gt;，它的主要作用是缓存了已经发出去但还没有收到响应的请求（NodeId 是一个 String 类型，表示节点的 id 编号）。 拦截器 生产者拦截器既可以用来在消息发送前做一些准备工作，比如按照某个规则过滤不符合要求的消息、修改消息的内容等，也可以用来在发送回调逻辑前做一些定制化的需求，比如统计类工作。 生产者拦截器的使用也很方便，主要是自定义实现 org.apache.kafka.clients.producer. ProducerInterceptor 接口。ProducerInterceptor 接口中包含3个方法： public ProducerRecord&lt;K, V&gt; onSend(ProducerRecord&lt;K, V&gt; record); public void onAcknowledgement(RecordMetadata metadata, Exception exception); public void close(); KafkaProducer 在将消息序列化和计算分区之前会调用生产者拦截器的 onSend() 方法来对消息进行相应的定制化操作。一般来说最好不要修改消息 ProducerRecord 的 topic、key 和 partition 等信息。 KafkaProducer 会在消息被应答（Acknowledgement）之前或消息发送失败时调用生产者拦截器的 onAcknowledgement() 方法，优先于用户设定的 Callback 之前执行。这个方法运行在 Producer 的I/O线程中，所以这个方法中实现的代码逻辑越简单越好，否则会影响消息的发送速度。 close() 方法主要用于在关闭拦截器时执行一些资源的清理工作。 序列化器 生产者需要用序列化器（Serializer）把对象转换成字节数组才能通过网络发送给 Kafka。而在对侧，消费者需要用反序列化器（Deserializer）把从 Kafka 中收到的字节数组转换成相应的对象。 生产者使用的序列化器和消费者使用的反序列化器是需要一一对应的，如果生产者使用了某种序列化器，比如 StringSerializer，而消费者使用了另一种序列化器，比如 IntegerSerializer，那么是无法解析出想要的数据的。 序列化器都需要实现org.apache.kafka.common.serialization.Serializer 接口，此接口有3个方法： public void configure(Map&lt;String, ?&gt; configs, boolean isKey) public byte[] serialize(String topic, T data) public void close() configure() 方法用来配置当前类，serialize() 方法用来执行序列化操作。而 close() 方法用来关闭当前的序列化器。 如下： public class StringSerializer implements Serializer&lt;String&gt; { private String encoding = &quot;UTF8&quot;; @Override public void configure(Map&lt;String, ?&gt; configs, boolean isKey) { String propertyName = isKey ? &quot;key.serializer.encoding&quot; : &quot;value.serializer.encoding&quot;; Object encodingValue = configs.get(propertyName); if (encodingValue == null) encodingValue = configs.get(&quot;serializer.encoding&quot;); if (encodingValue != null &amp;&amp; encodingValue instanceof String) encoding = (String) encodingValue; } @Override public byte[] serialize(String topic, String data) { try { if (data == null) return null; else return data.getBytes(encoding); } catch (UnsupportedEncodingException e) { throw new SerializationException(&quot;Error when serializing &quot; + &quot;string to byte[] due to unsupported encoding &quot; + encoding); } } @Override public void close() { // nothing to do } } configure() 方法，这个方法是在创建 KafkaProducer 实例的时候调用的，主要用来确定编码类型。 serialize用来编解码，如果 Kafka 客户端提供的几种序列化器都无法满足应用需求，则可以选择使用如 Avro、JSON、Thrift、ProtoBuf 和 Protostuff 等通用的序列化工具来实现，或者使用自定义类型的序列化器来实现。 分区器 消息经过序列化之后就需要确定它发往的分区，如果消息 ProducerRecord 中指定了 partition 字段，那么就不需要分区器的作用，因为 partition 代表的就是所要发往的分区号。 如果消息 ProducerRecord 中没有指定 partition 字段，那么就需要依赖分区器，根据 key 这个字段来计算 partition 的值。分区器的作用就是为消息分配分区。 Kafka 中提供的默认分区器是 org.apache.kafka.clients.producer.internals.DefaultPartitioner，它实现了 org.apache.kafka.clients.producer.Partitioner 接口，这个接口中定义了2个方法，具体如下所示。 public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster); public void close(); 其中 partition() 方法用来计算分区号，返回值为 int 类型。partition() 方法中的参数分别表示主题、键、序列化后的键、值、序列化后的值，以及集群的元数据信息，通过这些信息可以实现功能丰富的分区器。close() 方法在关闭分区器的时候用来回收一些资源。 在默认分区器 DefaultPartitioner 的实现中，close() 是空方法，而在 partition() 方法中定义了主要的分区分配逻辑。如果 key 不为 null，那么默认的分区器会对 key 进行哈希，最终根据得到的哈希值来计算分区号，拥有相同 key 的消息会被写入同一个分区。如果 key 为 null，那么消息将会以轮询的方式发往主题内的各个可用分区。 自定义的分区器，只需同 DefaultPartitioner 一样实现 Partitioner 接口即可。由于每个分区下的消息处理都是有顺序的，我们可以利用自定义分区器实现在某一系列的key都发送到一个分区中，从而实现有序消费。 Broker Broker处理请求流程 在Kafka的架构中，会有很多客户端向Broker端发送请求，Kafka 的 Broker 端有个 SocketServer 组件，用来和客户端建立连接，然后通过Acceptor线程来进行请求的分发，由于Acceptor不涉及具体的逻辑处理，非常得轻量级，因此有很高的吞吐量。 接着Acceptor 线程采用轮询的方式将入站请求公平地发到所有网络线程中，网络线程池默认大小是 3个，表示每台 Broker 启动时会创建 3 个网络线程，专门处理客户端发送的请求，可以通过Broker 端参数 num.network.threads来进行修改。 那么接下来处理网络线程处理流程如下： 当网络线程拿到请求后，会将请求放入到一个共享请求队列中。Broker 端还有个 IO 线程池，负责从该队列中取出请求，执行真正的处理。如果是 PRODUCE 生产请求，则将消息写入到底层的磁盘日志中；如果是 FETCH 请求，则从磁盘或页缓存中读取消息。 IO 线程池处中的线程是执行请求逻辑的线程，默认是8，表示每台 Broker 启动后自动创建 8 个 IO 线程处理请求，可以通过Broker 端参数 num.io.threads调整。 Purgatory组件是用来缓存延时请求（Delayed Request）的。比如设置了 acks=all 的 PRODUCE 请求，一旦设置了 acks=all，那么该请求就必须等待 ISR 中所有副本都接收了消息后才能返回，此时处理该请求的 IO 线程就必须等待其他 Broker 的写入结果。 控制器 在 Kafka 集群中会有一个或多个 broker，其中有一个 broker 会被选举为控制器（Kafka Controller），它负责管理整个集群中所有分区和副本的状态。 控制器是如何被选出来的？ Broker 在启动时，会尝试去 ZooKeeper 中创建 /controller 节点。Kafka 当前选举控制器的规则是：第一个成功创建 /controller 节点的 Broker 会被指定为控制器。 在ZooKeeper中的 /controller_epoch 节点中存放的是一个整型的 controller_epoch 值。controller_epoch 用于记录控制器发生变更的次数，即记录当前的控制器是第几代控制器，我们也可以称之为“控制器的纪元”。 controller_epoch 的初始值为1，即集群中第一个控制器的纪元为1，当控制器发生变更时，每选出一个新的控制器就将该字段值加1。Kafka 通过 controller_epoch 来保证控制器的唯一性，进而保证相关操作的一致性。 每个和控制器交互的请求都会携带 controller_epoch 这个字段，如果请求的 controller_epoch 值小于内存中的 controller_epoch 值，则认为这个请求是向已经过期的控制器所发送的请求，那么这个请求会被认定为无效的请求。 如果请求的 controller_epoch 值大于内存中的 controller_epoch 值，那么说明已经有新的控制器当选了。 控制器是做什么的？ 主题管理（创建、删除、增加分区） 分区重分配 Preferred 领导者选举 Preferred 领导者选举主要是 Kafka 为了避免部分 Broker 负载过重而提供的一种换 Leader 的方案。 集群成员管理（新增 Broker、Broker 主动关闭、Broker 宕机） 控制器组件会利用 Watch 机制检查 ZooKeeper 的 /brokers/ids 节点下的子节点数量变更。目前，当有新 Broker 启动后，它会在 /brokers 下创建专属的 znode 节点。一旦创建完毕，ZooKeeper 会通过 Watch 机制将消息通知推送给控制器，这样，控制器就能自动地感知到这个变化，进而开启后续的新增 Broker 作业。 数据服务 控制器上保存了最全的集群元数据信息。 控制器宕机了怎么办？ 当运行中的控制器突然宕机或意外终止时，Kafka 能够快速地感知到，并立即启用备用控制器来代替之前失败的控制器。这个过程就被称为 Failover，该过程是自动完成的，无需你手动干预。 消费者 消费组 在Kafka中，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者。每个消费者只能消费所分配到的分区中的消息。而每一个分区只能被一个消费组中的一个消费者所消费。 入上图所示，我们可以设置两个消费者组来实现广播消息的作用，消费组A和组B都可以接受到生产者发送过来的消息。 消费者与消费组这种模型可以让整体的消费能力具备横向伸缩性，我们可以增加（或减少）消费者的个数来提高（或降低）整体的消费能力。对于分区数固定的情况，一味地增加消费者并不会让消费能力一直得到提升，如果消费者过多，出现了消费者的个数大于分区个数的情况，就会有消费者分配不到任何分区。 如下：一共有8个消费者，7个分区，那么最后的消费者C7由于分配不到任何分区而无法消费任何消息。 消费端分区分配策略 Kafka 提供了消费者客户端参数 partition.assignment.strategy 来设置消费者与订阅主题之间的分区分配策略。 RangeAssignor分配策略 默认情况下，采用 RangeAssignor 分配策略。 RangeAssignor 分配策略的原理是按照消费者总数和分区总数进行整除运算来获得一个跨度，然后将分区按照跨度进行平均分配，以保证分区尽可能均匀地分配给所有的消费者。对于每一个主题，RangeAssignor 策略会将消费组内所有订阅这个主题的消费者按照名称的字典序排序，然后为每个消费者划分固定的分区范围，如果不够平均分配，那么字典序靠前的消费者会被多分配一个分区。 假设消费组内有2个消费者 C0 和 C1，都订阅了主题 t0 和 t1，并且每个主题都有4个分区，那么订阅的所有分区可以标识为：t0p0、t0p1、t0p2、t0p3、t1p0、t1p1、t1p2、t1p3。最终的分配结果为： 消费者C0：t0p0、t0p1、t1p0、t1p1 消费者C1：t0p2、t0p3、t1p2、t1p3 假设上面例子中2个主题都只有3个分区，那么订阅的所有分区可以标识为：t0p0、t0p1、t0p2、t1p0、t1p1、t1p2。最终的分配结果为： 消费者C0：t0p0、t0p1、t1p0、t1p1 消费者C1：t0p2、t1p2 可以明显地看到这样的分配并不均匀。 RoundRobinAssignor分配策略 RoundRobinAssignor 分配策略的原理是将消费组内所有消费者及消费者订阅的所有主题的分区按照字典序排序，然后通过轮询方式逐个将分区依次分配给每个消费者。 如果同一个消费组内所有的消费者的订阅信息都是相同的，那么 RoundRobinAssignor 分配策略的分区分配会是均匀的。 如果同一个消费组内的消费者订阅的信息是不相同的，那么在执行分区分配的时候就不是完全的轮询分配，有可能导致分区分配得不均匀。 假设消费组内有3个消费者（C0、C1 和 C2），t0、t0、t1、t2主题分别有1、2、3个分区，即整个消费组订阅了 t0p0、t1p0、t1p1、t2p0、t2p1、t2p2 这6个分区。 具体而言，消费者 C0 订阅的是主题 t0，消费者 C1 订阅的是主题 t0 和 t1，消费者 C2 订阅的是主题 t0、t1 和 t2，那么最终的分配结果为： 消费者C0：t0p0 消费者C1：t1p0 消费者C2：t1p1、t2p0、t2p1、t2p2 可以看 到 RoundRobinAssignor 策略也不是十分完美，这样分配其实并不是最优解，因为完全可以将分区 t1p1 分配给消费者 C1。 StickyAssignor分配策略 这种分配策略，它主要有两个目的： 分区的分配要尽可能均匀。 分区的分配尽可能与上次分配的保持相同。 假设消费组内有3个消费者（C0、C1 和 C2），它们都订阅了4个主题（t0、t1、t2、t3），并且每个主题有2个分区。也就是说，整个消费组订阅了 t0p0、t0p1、t1p0、t1p1、t2p0、t2p1、t3p0、t3p1 这8个分区。最终的分配结果如下： 消费者C0：t0p0、t1p1、t3p0 消费者C1：t0p1、t2p0、t3p1 消费者C2：t1p0、t2p1 再假设此时消费者 C1 脱离了消费组，那么分配结果为： 消费者C0：t0p0、t1p1、t3p0、t2p0 消费者C2：t1p0、t2p1、t0p1、t3p1 StickyAssignor 分配策略如同其名称中的“sticky”一样，让分配策略具备一定的“黏性”，尽可能地让前后两次分配相同，进而减少系统资源的损耗及其他异常情况的发生。 再均衡（Rebalance） 再均衡是指分区的所属权从一个消费者转移到另一消费者的行为，它为消费组具备高可用性和伸缩性提供保障，使我们可以既方便又安全地删除消费组内的消费者或往消费组内添加消费者。 弊端： 在再均衡发生期间，消费组内的消费者是无法读取消息的。 Rebalance 很慢。如果一个消费者组里面有几百个 Consumer 实例，Rebalance 一次要几个小时。 在进行再均衡的时候消，费者当前的状态也会丢失。比如消费者消费完某个分区中的一部分消息时还没有来得及提交消费位移就发生了再均衡操作，之后这个分区又被分配给了消费组内的另一个消费者，原来被消费完的那部分消息又被重新消费一遍，也就是发生了重复消费。 Rebalance 发生的时机有三个： 组成员数量发生变化 订阅主题数量发生变化 订阅主题的分区数发生变化 后两类通常是业务的变动调整所导致的，我们一般不可控制，我们主要说说因为组成员数量变化而引发的 Rebalance 该如何避免。 当 Consumer Group 完成 Rebalance 之后，每个 Consumer 实例都会定期地向 Coordinator 发送心跳请求，表明它还存活着。如果某个 Consumer 实例不能及时地发送这些心跳请求，Coordinator 就会认为该 Consumer 已经“死”了，从而将其从 Group 中移除，然后开启新一轮 Rebalance。 Consumer端可以设置 session.timeout.ms，默认是10s，表示如果 Coordinator 在 10 秒之内没有收到 Group 下某 Consumer 实例的心跳，它就会认为这个 Consumer 实例已经挂了。 Consumer端还可以设置 heartbeat.interval.ms，表示发送心跳请求的频率。 以及max.poll.interval.ms 参数，它限定了 Consumer 端应用程序两次调用 poll 方法的最大时间间隔。它的默认值是 5 分钟，表示你的 Consumer 程序如果在 5 分钟之内无法消费完 poll 方法返回的消息，那么 Consumer 会主动发起“离开组”的请求，Coordinator 也会开启新一轮 Rebalance。 所以知道了上面几个参数后，我们就可以避免以下两个问题： 非必要 Rebalance 是因为未能及时发送心跳，导致 Consumer 被“踢出”Group 而引发的。 所以我们在生产环境中可以这么设置： 设置 session.timeout.ms = 6s。 设置 heartbeat.interval.ms = 2s。 必要 Rebalance 是 Consumer 消费时间过长导致的。如何消费任务时间达到8分钟，而max.poll.interval.ms设置为5分钟，那么也会发生Rebalance，所以如果有比较重的任务的话，可以适当调整这个参数。 Consumer 端的频繁的 Full GC导致的长时间停顿，从而引发了 Rebalance。 消费者组再平衡全流程 重平衡过程是靠消费者端的心跳线程（Heartbeat Thread），通知到其他消费者实例的。 当协调者决定开启新一轮重平衡后，它会将“REBALANCE_IN_PROGRESS”封装进心跳请求的响应中，发还给消费者实例。当消费者实例发现心跳响应中包含了“REBALANCE_IN_PROGRESS”，就能立马知道重平衡又开始了，这就是重平衡的通知机制。 所以，实际上heartbeat.interval.ms不止是设置了心跳的间隔时间，还可以控制重平衡通知的频率。 消费者组状态机 重平衡一旦开启，Broker 端的协调者组件就要完成整个重平衡流程，Kafka 设计了一套消费者组状态机（State Machine）来实现。 Kafka 为消费者组定义了 5 种状态，它们分别是：Empty、Dead、PreparingRebalance、CompletingRebalance 和 Stable。 状态机的各个状态流转： 当有新成员加入或已有成员退出时，消费者组的状态从 Stable 直接跳到 PreparingRebalance 状态，此时，所有现存成员就必须重新申请加入组。当所有成员都退出组后，消费者组状态变更为 Empty。Kafka 定期自动删除过期位移的条件就是，组要处于 Empty 状态。因此，如果你的消费者组停掉了很长时间（超过 7 天），那么 Kafka 很可能就把该组的位移数据删除了。 组协调器（GroupCoordinator） GroupCoordinator 是 Kafka 服务端中用于管理消费组的组件。协调器最重要的职责就是负责执行消费者再均衡的操作。 消费者端重平衡流程 在消费者端，重平衡分为两个步骤：分别是加入组和等待领导者消费者（Leader Consumer）分配方案。即JoinGroup 请求和 SyncGroup 请求。 加入组 当组内成员加入组时，它会向协调器发送 JoinGroup 请求。在该请求中，每个成员都要将自己订阅的主题上报，这样协调器就能收集到所有成员的订阅信息。 选择消费组领导者 一旦收集了全部成员的 JoinGroup 请求后，协调者会从这些成员中选择一个担任这个消费者组的领导者。 这里的领导者是具体的消费者实例，它既不是副本，也不是协调器。领导者消费者的任务是收集所有成员的订阅信息，然后根据这些信息，制定具体的分区消费分配方案。 选举分区分配策略 这个分区分配的选举是根据消费组内的各个消费者投票来决定的。 协调器会收集各个消费者支持的所有分配策略，组成候选集 candidates。每个消费者从候选集 candidates 中找出第一个自身支持的策略，为这个策略投上一票。计算候选集中各个策略的选票数，选票数最多的策略即为当前消费组的分配策略。 如果有消费者并不支持选出的分配策略，那么就会报出异常 IllegalArgumentException：Member does not support protocol。 发送 SyncGroup 请求 协调器会把消费者组订阅信息封装进 JoinGroup 请求的响应体中，然后发给领导者，由领导者统一做出分配方案，然后领导者发送 SyncGroup 请求给协调器。 响应SyncGroup 组内所有的消费者都会发送一个 SyncGroup 请求，只不过不是领导者的请求内容为空，然后就会接收到一个SyncGroup响应，接受订阅信息。 ","link":"https://phantomma.top/post/kafka-ti-xi-jia-gou-fen-jie/"},{"title":"vps 折腾小记","content":"购买vps 使用的是俄罗斯的vps厂商：justhost，它的定价是最有性价比的一档。最便宜的俄罗斯区域的，120多人民币一年。一开始选择的俄罗斯region，不过流媒体对俄罗斯ip是封禁的。最后还是换成美西😓，价格也变成了360¥一年。 美西LOS，ping的延迟在150ms PING 155.254.193.168 (155.254.193.168): 56 data bytes 64 bytes from 155.254.193.168: icmp_seq=0 ttl=52 time=154.847 ms 64 bytes from 155.254.193.168: icmp_seq=1 ttl=52 time=177.327 ms 64 bytes from 155.254.193.168: icmp_seq=2 ttl=52 time=153.530 ms 64 bytes from 155.254.193.168: icmp_seq=3 ttl=52 time=151.163 ms https://justhost.asia/ 申请域名 域名丛https://freenom.com/ 申请的免费域名。 可以将域名的解析服务设置到国内的dnspod。 一键安装v2ray脚本 域名搞定以后，可以在主机上一键安装服务端 https://github.com/wulabing/V2Ray_ws-tls_bash_onekey vmess://ewogICJ2IjogIjIiLAogICJwcyI6ICJ3dWxhYmluZ19jcm9zcy5waGFudG9tbWEudGsiLAogICJhZGQiOiAiY3Jvc3MucGhhbnRvbW1hLnRrIiwKICAicG9ydCI6ICI0NDMiLAogICJpZCI6ICI4NmNiYjU0Mi1iYzRkLTQwYTMtYmQzOS1hNWM2OTkzOGE1NTQiLAogICJhaWQiOiAiMCIsCiAgIm5ldCI6ICJ3cyIsCiAgInR5cGUiOiAibm9uZSIsCiAgImhvc3QiOiAiY3Jvc3MucGhhbnRvbW1hLnRrIiwKICAicGF0aCI6ICIvMWJjODgvIiwKICAidGxzIjogInRscyIKfQo= 加速tcp 压缩加速tcp的连接 https://github.com/tcp-nanqinlang/wiki/wiki/general 配置软路由科学上网插件 bypass luci-app-ssr-plus （额外多了 DNS防污染服务） other 查看流媒体网站对该vps的支持情况。https://www.vpsgo.com/vps-ip-regionrestrictioncheck.html ","link":"https://phantomma.top/post/vps-zhe-teng-xiao-ji/"},{"title":"hbase orm 中间层 hbasedao","content":"背景 hbase 是分布式的 kv(key value) 存储系统，hbase 提供的针对底层数据的操作也是基于 kv 维度的，使用起来更像是Map的操作方式。但是上层业务应用一般是采用的面向对象的设计，这就导致了使用 hbase 的底层 api 必须要写很多的代码来做KV原始数据到上层业务对象的转化。 使用关系型数据库如 MySQL，也会遇到关系型数据跟对象之间的适配问题，所以出现了很多成熟的做对象关系映射的产品，像 hibernate 和 mybatis。 hbase 当然也需要一个类似的东西，来做对象到 kv 数据的适配，这样上层应用可以继续专注于上层 面向对象的方式的开发，而不用直接操作kv数据，提高工作效率。 hbase使用场景 hbase 要解决的问题是海量数据的分布式存储，传统数据库如 mysql 解决这个问题也是有办法的，可以通过分库分表的方式做到。 所以，hbase 和 mysql 的竞争点是在需要 mysql 分库分表的情况。下面以一个很常见的场景举例，来看之前数据存储使用mysql的场景，迁移hbase上的过程。 问题描述：用户的基本信息（包括：用户头像，上次登录时间），好友关系（正向 我关注的人，反向关注我的人） mysql的方式： 用户信息表 列名 说明 id 用户id head_image 头像 last_visit_time 上次登录时间 用户关注表 列名 说明 userId 用户id follow_user 关注的人 用户粉丝表 列名 说明 userId 用户id be_followed_user 关注我的人 之所以用户关系两张表来保存是因为，用户关系数据量比较大的情况下，采用分库分表存储的方案，又要提供正向、反向的查询，所以需要分别以关注者和被关注者为路由字段存储两份。 hbase的方式： rowKey 列簇 列 userId info_cf（单version） head_image last_visit_time relation_cf (多个version) follow_user be_followed_user 存入 hbase 的数据的逻辑结构会是： hbase_data.png 通过上面 mysql 表到 hbase 表的迁移过程，可以清楚地看到：mysql 中 和 userId 一对一的用户基本信息迁移到 hbase 可以用无 version 特性的列簇保存；一对多的关注关系迁移到 hbase，可以利用hbase 多 version 的列簇保存。下面介绍的hbasedao中间层就会主要解决mysql迁移到 hbase 的适配，包括单 version（一对一）和多 version（一对多）的情形。 NOTE： hbase 多 version 的列簇，不同column之间没有任何对应关系，所以不要尝试在不同的 column 之间找行的对应关系。 hbasedao介绍 hbasedao 是一个简单地解决kv数据到业务对象适配的中间层，类似关系型数据库orm中间层mybatis。 针对于 hbase 存储结构抽象出来的类结构： habasedao_realtion.png 使用 封装业务对象成 DO 类，一个 rowKey 对应一个 DO 类的对象，通过指定DO类里的 column，中间层可以做到针对于指定 column 的查询。使用 hbasedao 之后，查询的方式如下： public UserHBaseDO get(String rowKey) throws HBaseDAOException { HBaseDO hBaseDO = new HBaseDO(); hBaseDO.setTableName(UserHBaseDO.TABLE_NAME); hBaseDO.setRowKey(rowKey); hBaseDO.addColumnFamily(UserHBaseDO.CF_NAME_info_cf) .putColumn(UserHBaseDO.CL_NAME_head_image) .putColumn(UserHBaseDO.CL_NAME_last_visit_time); hBaseDO.addColumnFamily(UserHBaseDO.CF_NAME_relation_cf, 10) .putColumn(UserHBaseDO.CL_NAME_follow_user) .putColumn(UserHBaseDO.CL_NAME_be_followed_user); super.getHbaseDao().get(hBaseDO); UserHBaseDO userHBaseDO = null; try { userHBaseDO = new UserHBaseDO(hBaseDO); } catch (UnsupportedEncodingException e) { new HBaseDAOException(e); } return userHBaseDO; } 支持的 API 以对象的方式插入数据 批量插入数据 传入 rowKey，查询对象。可支持针对不同的列簇指定查询的 version 数量，支持指定列簇中数据的时间范围 传入多个 rowKey，查询对象的列表 删除指定的 rowKey 的一行记录 指定开始扫描的 rowKey，开始按行扫描数据，可以限制扫描的结果集大小 public interface HBaseDAO { public void put(HBaseDO hBaseDO) throws HBaseDAOException; public void putList(List&lt;HBaseDO&gt; hBaseDOList, String tableName) throws HBaseDAOException; public void get(HBaseDO hBaseDO) throws HBaseDAOException; public void getList(List&lt;HBaseDO&gt; hBaseDOList, String tableName) throws HBaseDAOException; public void delete(HBaseDO hBaseDO) throws HBaseDAOException; public List&lt;HBaseDO&gt; scan(String tableName, String startRow, String endRow, int maxVersion, int maxResultSize, Map&lt;String, List&lt;String&gt;&gt; columnFamilyMap) throws HBaseDAOException; } 使用举例（step by step） 代码工程结构，sample 包里提供了详细的使用举例。可以看到，使用时只需要定义 hbase 表对应的 DO 类，在此类中编写对象和 hbase 里数据的对应关系，在 DAO 层以上就可以提供和 mybatis 类似的接口访问形式。 ├── pom.xml ├── src │ ├── main │ │ ├── java │ │ │ └── com │ │ │ └── taobao │ │ │ └── hbasedao │ │ │ ├── ColumnFamilyInfo.java │ │ │ ├── HBaseCell.java │ │ │ ├── HBaseClientDaoSupport.java │ │ │ ├── HBaseColumnFamily.java │ │ │ ├── HBaseConsole.java │ │ │ ├── HBaseDAO.java │ │ │ ├── HBaseDAOException.java │ │ │ ├── HBaseDAOFactory.java │ │ │ ├── HBaseDAOImpl.java │ │ │ ├── HBaseDO.java │ │ │ ├── HBaseVO.java │ │ │ ├── MapAble.java │ │ │ └── sample │ │ │ ├── dao │ │ │ │ └── UserHBaseDAO.java │ │ │ ├── dataobject │ │ │ │ └── UserHBaseDO.java │ │ │ └── vo │ │ │ └── FollowerVO.java │ │ └── resources │ └── test │ ├── java │ │ └── com │ │ └── taobao │ │ └── hbasedao │ │ └── sample │ │ └── test │ │ └── UserHBaseDAOTest.java │ └── resources │ └── hbase-dao.xml 步骤 引入依赖 &lt;dependency&gt; &lt;groupId&gt;com.taobao.hbasedao&lt;/groupId&gt; &lt;artifactId&gt;hbasedao&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; 定义hbae表对应的DO类，相当于使用 mybati s时，定义的 sqlmap，需要给出 hbase 表的 schema 信息，如：列簇名、列名、列簇的最大 version，列簇和列的绑定关系等。 public class UserHBaseDO implements MapAble&lt;UserHBaseDO&gt; { public static final String TABLE_NAME = &quot;hbasedao_user&quot;; public static final String CF_NAME_info_cf = &quot;info_cf&quot;; public static final int MAX_SIZE_CF_NAME_info_cf = 1; public static final String CF_NAME_relation_cf = &quot;relation_cf&quot;; public static final int MAX_SIZE_CF_NAME_relation_cf = 1000; public static final String CL_NAME_head_image = &quot;head_image&quot;; public static final String CL_NAME_last_visit_time = &quot;last_visit_time&quot;; public static final String CL_NAME_follow_user = &quot;follow_user&quot;; public static final String CL_NAME_be_followed_user = &quot;be_followed_user&quot;; private static Map&lt;String, List&lt;String&gt;&gt; columnFamilyMap; static { columnFamilyMap = new HashMap&lt;String, List&lt;String&gt;&gt;(); columnFamilyMap.put(CF_NAME_info_cf, Arrays.asList(new String[] { CL_NAME_head_image, CL_NAME_last_visit_time })); columnFamilyMap.put(CF_NAME_relation_cf, Arrays.asList(new String[] { CL_NAME_follow_user, CL_NAME_be_followed_user })); } private String rowKey; private String headImage; private long lastVisitTime; private final List&lt;FollowerVO&gt; followVOList = new ArrayList&lt;FollowerVO&gt;(); private final List&lt;FollowerVO&gt; beFollowedVOList = new ArrayList&lt;FollowerVO&gt;(); 实现 MapAble 接口，MapAble 接口包括两个方法，一个是插入数据时将用户定义的 UserHBaseDO 转化为框架使用的 HBaseDO，一个是查询时将 HBaseDO 里包含的数据转为为用户定义 UserHBaseDO。 @Override public HBaseDO converDOToHBaseDO() { HBaseDO hbaseDO = new HBaseDO(); hbaseDO.setRowKey(this.rowKey); hbaseDO.setTableName(UserHBaseDO.TABLE_NAME); for (Map.Entry&lt;String, List&lt;String&gt;&gt; columnFamilyEntry : columnFamilyMap.entrySet()) { String columnFamilyName = columnFamilyEntry.getKey(); List&lt;String&gt; columnNameList = columnFamilyEntry.getValue(); HBaseColumnFamily columnFamily = new HBaseColumnFamily(); for (String columnName : columnNameList) { if (CF_NAME_info_cf.equals(columnFamilyName) &amp;&amp; CL_NAME_head_image.equals(columnName)) { if (this.headImage != null) { List&lt;HBaseCell&gt; cellDOList = new ArrayList&lt;HBaseCell&gt;(); HBaseCell cellDO = new HBaseCell(); cellDO.setValue(this.headImage); cellDOList.add(cellDO); columnFamily.putColumn(columnName, cellDOList); } } ... } hbaseDO.getColumnFamilyMap().put(columnFamilyName, columnFamily); } return hbaseDO; } @Override public UserHBaseDO convertHBaseDOToDO(HBaseDO hBaseDO) throws UnsupportedEncodingException { this.rowKey = hBaseDO.getRowKey(); List&lt;KeyValue&gt; results = hBaseDO.getResults(); for (Map.Entry&lt;String, List&lt;String&gt;&gt; columnFamilyEntry : columnFamilyMap.entrySet()) { String columnFamilyName = columnFamilyEntry.getKey(); List&lt;String&gt; columnNameList = columnFamilyEntry.getValue(); for (String columnName : columnNameList) { if (CF_NAME_info_cf.equals(columnFamilyName) &amp;&amp; CL_NAME_head_image.equals(columnName)) { for (KeyValue kv : results) { if (columnFamilyName.equals(new String(kv.getFamily())) &amp;&amp; columnName.equals(new String(kv.getQualifier()))) { this.headImage = new String(kv.getValue(), &quot;UTF-8&quot;); } } } ... } } return this; } 然后就是定义 DAO 类，主要的逻辑已经在 UserHBaseDO 写了，这里的实现可以足够简单，查询的代码在 hbasedao 介绍里已经给出，插入数据的代码如下： public void insert(UserHBaseDO userHBaseDO) throws HBaseDAOException { super.getHbaseDao().put(userHBaseDO.converDOToHBaseDO()); } 编写测试用例 @Test public void test_insert() throws HBaseDAOException { ApplicationContext context = new ClassPathXmlApplicationContext(&quot;hbase-dao.xml&quot;); UserHBaseDAO userHBaseDAO = context.getBean(&quot;userHBaseDAO&quot;, UserHBaseDAO.class); UserHBaseDO userHBaseDO = new UserHBaseDO(); userHBaseDO.setRowKey(&quot;2222&quot;); userHBaseDO.setHeadImage(&quot;icon1.jpg&quot;); userHBaseDO.setLastVisitTime(4820023); int i = 0; FollowerVO followerVO1 = new FollowerVO(&quot;444_errik&quot;); followerVO1.setTimeStamp(System.currentTimeMillis() + i++); FollowerVO followerVO2 = new FollowerVO(&quot;555_wiie&quot;); followerVO2.setTimeStamp(System.currentTimeMillis() + i++); FollowerVO followerVO3 = new FollowerVO(&quot;666_gate&quot;); followerVO3.setTimeStamp(System.currentTimeMillis() + i++); userHBaseDO.getFollowVOList().add(followerVO1); userHBaseDO.getFollowVOList().add(followerVO2); userHBaseDO.getFollowVOList().add(followerVO3); FollowerVO followerVO4 = new FollowerVO(&quot;999_hena&quot;); userHBaseDO.getBeFollowedVOList().add(followerVO4); userHBaseDAO.insert(userHBaseDO); } @Test public void test_get() throws HBaseDAOException { ApplicationContext context = new ClassPathXmlApplicationContext(&quot;hbase-dao.xml&quot;); UserHBaseDAO userHBaseDAO = context.getBean(&quot;userHBaseDAO&quot;, UserHBaseDAO.class); UserHBaseDO userHBaseDO = userHBaseDAO.get(&quot;2222&quot;); System.out.println(userHBaseDO); } 完整地代码示例请参考 com.taobao.hbasedao.sample 代码工程在：hbasedao ","link":"https://phantomma.top/post/hbase-orm-zhong-jian-ceng-hbasedao/"},{"title":"Canal 源码走读","content":"canal 是什么？ 引用一下官方回答： 阿里巴巴mysql数据库binlog的增量订阅&amp;消费组件 ** canal 能做什么？** 基于日志增量订阅&amp;消费支持的业务： 数据库镜像 数据库实时备份 多级索引 (卖家和买家各自分库索引) search build 业务cache刷新 价格变化等重要业务消息 比如 LZ 目前就使用 canal 实现数据实时复制，搜索引擎数据构建等功能。既然要使用，就好好的研究一下。 时间有限，一起来简单看看。 软件架构 关于 canal 的工作原理，我就不展开了，有兴趣的可以看看官方文档，或者这个 (ppt)[https://docs.google.com/presentation/d/1MkszUPYRDkfVPz9IqOT1LLT5d9tuwde_WC8GZvjaDRg/edit?pli=1#slide=id.p16]. 说白了， canal 就是伪装成 mysql 的 slave，dump binlog，解析 binlog，然后传递给应用程序，总体还是蛮简单的。 好，我们来看看 canal 的代码架构。 我们看到，canal server 内部由几个模块组成， 最外部的是 Server，该 Server 接收 Canal Client 请求，并返回 Client 数据。一个 Server 就是一个 JVM。每个 Server 内部由多个 CanalInstance，每个 CanalInstance 其实就是我们设置的 destination，通常是一个数据库。 每个 CanalInstance 内部由 5 个模块，分别是 parser 解析，sink 过滤，store 存储，metaManager 元数据管理，Alarm 报警。 这 5 个模块是干嘛的呢？ 简单说一下： 当 Canal Server 启动后，会根据配置启动 N 个 CanalInstance， 每个 CanalInstance 都会使用 socket 连接 mysql，dump binlog，然后将数据交给 parser 解析，sink 过滤，store 存储，当 client 连接时，会从 zk 上读取该 client 的信息，而 metaManager 元数据管理就是管理 zk（当然有多种实现，比如存储在文件中） 信息的，如果发生错误了，就调用 Alarm 发送报警信息（你可以接入自己公司的监控系统），目前是打印日志。 Canal 启动流程 canal 代码量目前有 6 万多行，去除 2 个 ProtocolBuffer 生成类大概 1.7 万行，也还有 4.3 万行，代码还是不少的。 启动过程也比较绕。这里我简单画了一个流程图： 解释一下这个图： canal 脚本从 CanalLauncher main 方法启动，然后调用 CanalController 的 start 方法，CanalController 调用 InstanceConfigMonitor 的 start 方法，最后调用 canal 关键组件 CanalServerWithEmbedded 的 start 方法。 在 Canal 内部， 有 CanalServerWithEmbedded 和 CanalServerWithNetty，前者是没有 Server 端口的，是一个无端口的代理。后者是基于 Netty 实现的服务器，在 channelRead 方法中，会调用 CanalServerWithEmbedded 的相关方法。 CanalServerWithEmbedded 是单例的， 内部会有多个 CanalInstance， 他有多个实现，独立版本中使用的是 CanalInstanceWithSpring 版本，基于 Spring 管理组件的生命周期。 每个 CanalInstance 内部有 5 个组件，也就是上面说的几个组件，他们会分别启动。 其中，比较关键的是 parser，sink，store。 CanalEventParser 启动后，会启动一个叫做 parseThread 线程，不停的循环。主要是：构造与 mysql 的连接，然后启动心跳线程，然后开始 dump binlog。 dump 出来的 binlog 通过 disruptor 无锁队列发布，内部由 3 个消费者按照顺序消费 binlog，处理完之后，交给了 sink 模块。 然后是 sink，这个比较简单，就不说了。sink 处理完之后，交给了 store 模块。 store 模式是一个类似 RingBuffer 的循环数组，存储着从 mysql dump 出来的数据，client 也是从这里获取数据的。该数组维护着 3 个指针，get，put， ack。 这里比较奇怪的是，为什么不使用责任链模式够组装组件？ Canal 数据流向 看了启动流程，再来看看 canal 内部运行的数据流向是什么样子的。我这里简单画了一个图。 独立版本的 Canal 使用 Netty 暴露端口，使用自己构造的 SessionHandler 处理 TCP 请求，SessionHandler 将请求交给 CanalServerWithEmbedded 来处理。 我们看 CanalServerWithEmbedded 的一些方法，例如 subscribe，get，ack 等，都是和 client 对应的方法，也就是说，CanalServerWithEmbedded 是和 client 打交道的一个类。 CanalServerWithEmbedded 内部管理所有的 CanalInstance，通过 Client 的信息，找到 Client 订阅的 CanalInstance，然后调用 CanalInstance 内部的 Store 模块，也就是那个 RingBuffer 的 get 方法，获取 RingBuffer 的数据。 从 Myslq 的角度看，MysqlConnection 从 Myslq dump 数据，交给 parser 解析，parser 解析完，交给 sink，sink 处理完，交给 store 保存，等待 client 前来获取。 看完了数据流向，如果对哪里有什么疑问，就可以看看哪个模块对应的代码是什么，直接看是看就好了。 总结 花了点时间看了看 Canal 的代码，总体上还是非常好的，只是有些地方有点疑问，例如 parser，sink，store 为什么不使用过滤器模式。 Client 和 CanalServerWithEmbedded 为什么不使用 RPC 的方式交互，这样更简单明了。 ","link":"https://phantomma.top/post/canal-yuan-ma-zou-du/"},{"title":"docker 技巧","content":"docker服务 重启docker服务 sudo service docker restart 关闭docker服务 docker service docker stop 开启docker服务 docker service docker start docker镜像 查看镜像：docker images 删除镜像：docker rmi IMAGEID 强制删除镜像：docker rmi -f IMAGEID 利用镜像创建容器：docker run --name centos -itd centos:latest 删除全部image的: docker rmi $(docker images -q) 直接删除带none的镜像：docker rmi $(docker images | grep &quot;none&quot; | awk '{print $3}') docker容器 查看当前运行的容器：docker ps 查询存在的容器：docker ps -a 命令后面加上--no-trunc，大概是不省略的意思，可以显示列的完整信息 删除容器：docker -rm CONTAINERID 强制删除容器：docker -rm -f CONTAINERID 不能够删除一个正在运行的容器，会报错。需要先停止容器。 进入后台运行的容器：docker exec -it containname /bin/bash 启动容器：docker start containername 停止容器：docker stop containername 停止所有的container，这样才能够删除其中的images： docker stop $(docker ps -a -q) 如果想要删除所有container的话再加一个指令： docker rm $(docker ps -a -q) 注：-a标志列出所有容器，-q标志只列出容器的ID，然后传递给rm命令 重命名一个容器：docker rename old_name new_name 要获取所有容器名称及其IP地址：docker inspect -f '{{.Name}} - {{.NetworkSettings.IPAddress }}' $(docker ps -aq) 覆盖dockerfile里的entrypoint：docker run -it --entrypoint /bin/bash [docker_image] docker hub仓库 docker login 配置账号信息 docker tag chatroomserver:v1 mh494078416/chatroomserver:v2 docker push mh494078416/chatroomserver:v2 无需sudo sudo usermod -aG docker ${USER} su - ${USER} How To Install and Use Docker on Ubuntu 22.04 | DigitalOcean ","link":"https://phantomma.top/post/docker-ji-qiao/"},{"title":"数据库的复制与分区","content":"在分布式数据库中通过复制让数据库的分布式节点拥有 copy 备份，主要为了达到以下目的： 扩展性，因为单机所承载的数据量是有限的； 容错、高可用，在分布式系统中，单机故障是常态，多做些冗余在遇到单机故障时其他机器就可以及时接管； 性能，如果出现用户跨地区访问的情况， 可以通过多地就近部署减少访问的时延； 那么如何复制呢？复制算法在数据库里面有很多，单主（single leader）复制、多主（multi-leader）复制、无主（leaderless）复制。 但是对于大量的数据与并发，仅仅复制是不够的，所以引入了分区或者是叫分区。分区可以让每个分区都是自己的小型数据库，每条数据属于且仅属于一个分区，这样数据库就可以支持多个分区就可以并行操作，可以让数据库支持非常高的吞吐量和数据量。 单主复制 同步 or 异步？半同步！ 同步 or 异步其实指是分布式节点的数据是否和主节点保持一致。同步复制就是主库需要等待从库的确认才能返回，而异步不等待该从库的响应就返回。 同步复制的优点是，从库能保证有与主库一致的最新数据副本。如果主库突然失效，我们可以确信这些数据仍然能在从库上找到。缺点是，如果同步从库没有响应（比如它已经崩溃，或者出现网络故障，或其它任何原因），主库就无法处理写入操作。主库必须阻止所有写入，并等待同步副本再次可用。 异步复制虽然不会有阻塞的情况，但是如果从副本承担读请求，副本同步的数据由于复制速度的差异可能会出现不同用户从不同副本上面访问到的数据不一致的情况。 为了避免这种情况，一个是可以让客户端只从主副本读取数据，这样，在正常情况下，所有客户端读到的数据一定是一致的；另一种就是 使用半同步（semi-synchronous ）的方式，例如设置一个从库和主库进行同步，其他库则是异步。如果该同步从库变得不可用或缓慢，则将一个异步从库改为同步运行。这保证你至少在两个节点上拥有最新的数据副本。 拿 kafka 举例，在 kafka 的 leader 中会维护 ISR(In-Sync Replication) 列表，follower 从 leader 同步数据有一些延迟（由参数 replica.lag.time.max.ms 设置超时阈值），超过阈值的 follower 将被剔除出 ISR，只有将消息成功复制到所有 ISR 后，这条消息才算被提交，这样不用等待所有节点确认既保证了性能又能保证数据的可用性。 故障处理 高可用的目标是，即使个别节点失效，也能保持整个系统运行，并尽可能控制节点停机带来的影响。 从节点失效——快速恢复 如果从库崩溃并重新启动，或者，如果主库和从库之间的网络暂时中断，则比较容易恢复：从库可以从日志中知道，在发生故障之前处理的最后一个事务，然后接着这个事务进行处理即可。 我们看一下 Redis 主从同步是如何恢复的。主节点会将那些对自己的状态产生修改性影响的指令记录在本地的内存 buffer 中，然后异步将 buffer 中的指令同步到从节点，从节点一边执行同步的指令流来达到和主节点一样的状态，一边向主节点反馈自己同步到哪里了 (偏移量)。 如果从节点在短时间内无法和主节点进行同步，那么当网络状况恢复时，Redis 的主节点中那些没有同步的指令在 buffer 中有可能已经被后续的指令覆盖掉了，那么就需要使用快照同步。 它首先需要在主库上进行一次 bgsave 将当前内存的数据全部快照到磁盘文件中，然后再将快照文件的内容全部传送到从节点。从节点将快照文件接受完毕后，立即执行一次全量加载，加载之前先要将当前内存的数据清空。加载完毕后通知主节点继续进行增量同步。 主从切换 主库故障其中一个从库需要被提升为新的主库，需要重新配置客户端，以将它们的写操作发送给新的主库，其他从库需要开始拉取来自新主库的数据变更。 主库的故障切换通常由几步组成：1.确认主库失效；2.选择一个新的主库；3.配置启用新的主库。看似把大象塞进冰箱里的简单步骤实际上可能存在很多地方可能出错： 如果使用异步复制，则新主库可能没有收到老主库宕机前最后的写入操作。在选出新主库后，如果老主库重新加入集群，新主库在此期间可能会收到冲突的写入，那这些写入该如何处理？最常见的解决方案是简单丢弃老主库未复制的写入，这很可能打破客户对于数据持久性的期望，如果数据库需要和其他外部存储相协调，那么丢弃写入内容是极其危险的操作； 可能会出现两个节点都以为自己是主库的情况，也就是脑裂。如果两个主库都可以接受写操作，却没有冲突解决机制，那么数据就可能丢失或损坏。一些系统采取了安全防范措施：当检测到两个主库节点同时存在时会关闭其中一个节点，但设计粗糙的机制可能最后会导致两个节点都被关闭； 超时时间应该如何配置？越长意味着恢复时间也越长，太短又可能会出现不必要的故障切换。临时的负载峰值可能导致节点超时，如果系统已经处于高负载或网络问题的困扰之中，那么不必要的故障切换可能会让情况变得更糟糕。 我们看看Redis Sentinel 集群是怎么解决这些问题的。Redis Sentinel 负责持续监控主从节点的健康，当主节点挂掉时，自动选择一个最优的从节点切换为主节点。客户端来连接集群时，会首先连接 sentinel，通过 sentinel 来查询主节点的地址，然后再去连接主节点进行数据交互。当主节点发生故障时，客户端会重新向 sentinel 要地址，sentinel 会将最新的主节点地址告诉客户端。 因为 master 节点是 Redis Sentinel 通过投票选出来的，所以我们可以设置 sentinel 为3个节点以上，且为奇数，选举法定人数设置为（n/2+1）那么每次选出 master 都需要半数以上的节点同意才能通过，少数服从多数，不会出现两个票数一样的 leader同时被选上。 Redis 主从采用异步复制，当主节点挂掉时，从节点可能没有收到全部的同步消息，Sentinel 无法保证消息完全不丢失，但是也尽可能保证消息少丢失。通过这两个配置： min-slaves-to-write 1 min-slaves-max-lag 10 min-slaves-to-write 表示主节点必须至少有一个从节点在进行正常复制，否则就停止对外写服务，丧失可用性； min-slaves-max-lag 表示多少秒没有收到从节点的反馈，那么此时master就不会接受任何请求。我们可以减小min-slaves-max-lag参数的值，这样就可以避免在发生故障时大量的数据丢失。 复制日志该如何实现？ 最简单的想法可能是基于 SQL 语句的复制，将每个 INSERT、UPDATE 或 DELETE 语句都被转发给每个从库，就像直接从客户端收到一样。但这样也会有很多问题： 如果语句中使用了诸如 NOW() 或 RAND()这样的函数，该怎么处理？使用了用户定义的函数、触发器、存储过程又该怎么处理； 如果使用了自增id，或依赖于数据库中的现有数据（例如，UPDATE ... WHERE &lt;某些条件&gt;），每个副本上按照完全相同的顺序执行它们，否则可能会产生不同的效果。当有多个并发执行的事务时，这可能成为一个限制； 基于语句的复制在 5.1 版本前的 MySQL 中被使用到。但现在在默认情况下，如果语句中存在任何不确定性，MySQL 会切换到基于行的复制。 基于预写日志（WAL）复制，WAL 是在进行任何数据更改（更新、删除等）之前，先将这些更改操作写入到日志中，所以通过这个日志从库可以构建一个与主库一模一样的数据结构拷贝。缺点是与存储引擎紧密耦合，如果数据库的版本的变更修改了日志格式，这会让复制没法进行。 所以 WAL 复制对运维来说是很难受的， 如果复制协议不允许版本不匹配，则此类升级需要停机。 还有一种就是基于行的逻辑日志复制，逻辑日志是关系数据库用来表示行操作写入记录的序列， MySQL 的二进制日志（binlog）使用了这种方法。由于逻辑日志与存储引擎的内部实现是解耦的，系统可以更容易地做到向后兼容，从而使主库和从库能够运行不同版本的数据库软件，或者甚至不同的存储引擎。 写后读一致性问题 Reading Your Own Writes 许多应用让用户提交一些数据，然后查看他们提交的内容。可能是用户数据库中的记录，也可能是对讨论主题的评论，或其他类似的内容。提交新数据时，必须将其发送给主库，但是当用户查看数据时，可以通过从库进行读取。如果数据经常被查看，但只是偶尔写入，这是非常合适的。 但对于异步复制，如果用户在写入后马上就查看数据，则新数据可能尚未到达副本。对用户而言，看起来好像是刚提交的数据丢失了。 在这种情况下，我们需要写后读一致性（read-after-write consistency），也被称作 read-your-writes consistency 。这是一个保证，如果用户重新加载页面，他们总会看到他们自己提交的任何更新。 基于领导者的复制系统中实现写后读一致性： 对于用户 可能修改过的内容，总是从主库读取；这就要求得有办法不通过实际的查询就可以知道用户是否修改了某些东西。 如果应用中的大部分内容都可能被用户编辑，在这种情况下可以使用其他标准来决定是否从主库读取。例如可以跟踪上次更新的时间，在上次更新后的一分钟内，从主库读。 客户端可以记住最近一次写入的时间戳，系统需要确保从库在处理该用户的读取请求时，该时间戳前的变更都已经传播到了本从库中。如果当前从库不够新，则可以从另一个从库读取，或者等待从库追赶上来。这里的时间戳可以是逻辑时间戳（表示写入顺序的东西，例如日志序列号）或实际的系统时钟 如果你的副本分布在多个数据中心（为了在地理上接近用户或者出于可用性目的），还会有额外的复杂性。任何需要由主库提供服务的请求都必须路由到包含该主库的数据中心。 另一种复杂的情况发生在同一位用户从多个设备（例如桌面浏览器和移动 APP）请求服务的时候。这种情况下可能就需要提供跨设备的写后读一致性：如果用户在一个设备上输入了一些信息，然后在另一个设备上查看，则应该看到他们刚输入的信息。 在这种情况下，还有一些需要考虑的问题： 记住用户上次更新时间戳的方法变得更加困难，因为一个设备上运行的程序不知道另一个设备上发生了什么。需要对这些元数据进行中心化的存储。 如果副本分布在不同的数据中心，很难保证来自不同设备的连接会路由到同一数据中心。如果你的方法需要读主库，可能首先需要把来自该用户所有设备的请求都路由到同一个数据中心。 Monotonic Reads 单调读 如果用户从不同从库进行多次读取，就可能发生时光倒流（moving backward in time）的情况。例如用户 2345 两次进行相同的查询，首先查询了一个延迟很小的从库，然后是一个延迟较大的从库，第一个查询返回了最近由用户 1234 添加的评论，但是第二个查询不返回任何东西，因为滞后的从库还没有拉取到该写入内容。用户 2345 先看见用户 1234 的评论，然后又看到它消失，这就会让人觉得非常困惑了。 单调读（monotonic reads）这是一个比 强一致性（strong consistency） 更弱，但比 最终一致性（eventual consistency） 更强的保证。当读取数据时，你可能会看到一个旧值；单调读仅意味着如果一个用户顺序地进行多次读取，则他们不会看到时间回退，也就是说，如果已经读取到较新的数据，后续的读取不会得到更旧的数据。 实现单调读的一种方式是确保每个用户总是从同一个副本进行读取，例如可以基于用户 ID 的散列来选择副本。 多主复制 假如你有一个数据库，副本分散在好几个不同的数据中心（可能会用来容忍单个数据中心的故障，或者为了在地理上更接近用户）。多主配置中可以在每个数据中心都有主库。 在运维多个数据中心多主是有很多优点的，如：在多主配置中，每个写操作都可以在本地数据中心进行就近处理，性能会好一些；每个数据中心可以独立于其他数据中心继续运行，其他数据中心出现故障不会导致全局瘫痪； 多主配置也有一个很大的缺点：两个不同的数据中心可能会同时修改相同的数据，写冲突是必须解决的。 处理写入冲突 避免冲突 处理冲突的最简单的策略就是避免它们，如果应用程序可以确保特定记录的所有写入都通过同一个主库，那么冲突就不会发生。例如，在一个用户可以编辑自己数据的应用程序中，可以确保来自特定用户的请求始终路由到同一数据中心，并使用该数据中心的主库进行读写。不同的用户可能有不同的 “主” 数据中心。 但是也有可能因为数据中心出现故障，你需要将流量重新路由到另一个数据中心，在这种情况下，冲突避免将失效，你必须处理不同主库同时写入的可能性。 收敛至一致的状态 在多主配置中，由于没有明确的写入顺序，如果每个副本只是按照它看到写入的顺序写入，那么数据库最终将处于不一致的状态。所以需要以一种方式在所有变更复制完成时收敛至一个相同的最终值。 例如可以为每个副本分配一个唯一的 ID，ID 编号更高的写入具有更高的优先级，但是这种方法也意味着数据丢失。 用户自行处理 把这个操作直接交给用户，让用户自己在读取或写入前进行冲突解决，这种例子也是屡见不鲜，Github采用就是这种方式。 无主复制 在一些无主复制的实现中，客户端直接将写入发送到几个副本中，而另一些情况下，由一个 协调者（coordinator） 节点代表客户端进行写入。 在无主配置中，不存在故障转移。假设客户端（用户 1234）并行发送写入到所有三个副本，并且两个可用副本接受写入，但是不可用副本错过了它。假设三个副本中的两个承认写入是足够的：在用户 1234 已经收到两个确定的响应之后，我们认为写入成功。 因为有可能会有节点没写入成功，所以当一个客户端从数据库中读取数据时，它不仅仅把它的请求发送到一个副本，而是并行地发送到多个副本。客户可能会从不同的副本获得不同的响应，然后用版本号确定哪个值是更新的。 那么对于一个不可用的节点重新联机之后，它如何赶上它错过的写入？一般来说两种方法： 读修复（Read repair），当客户端并行读取多个节点时发现有的副本的值是旧的就将新值写回到该副本； 反熵过程（Anti-entropy process），用后台进程不断查找副本之间的数据差异，并将任何缺少的数据从一个副本复制到另一个副本。 在上面的例子中，在三个副本中的两个上进行处理，写入就算成功了。那么多个副本的集群中需要多少个副本写入成功才算成功？ 一般地说，如果有 n 个副本，每个写入必须由 w 个节点确认才能被认为是成功的，并且我们必须至少为每个读取查询 r 个节点。只要 w + r &gt; n，我们可以预期在读取时能获得最新的值，r 和 w 是有效读写所需的最低票数。 然而，法定人数（如迄今为止所描述的）并不像它们可能的那样具有容错性。网络中断可以很容易地将客户端从大量的数据库节点上切断。虽然这些节点是活着的，而其他客户端可能也能够连接到它们，但是从数据库节点切断的客户端来看，它们也可能已经死亡。在这种情况下，剩余的可用节点可能会少于 w 或 r，因此客户端不再能达到法定人数。 检测并发写入 由于可变的网络延迟和部分节点的故障，事件可能以不同的顺序到达不同的节点。那么就会有数据不一致的情况，比如下图显示了两个客户机 A 和 B 同时写入三节点数据存储中的键 X： 节点 1 接收来自 A 的写入，但由于暂时中断，未接收到来自 B 的写入。 节点 2 首先接收来自 A 的写入，然后接收来自 B 的写入。 节点 3 首先接收来自 B 的写入，然后从 A 写入。 如果每个节点只要接收到来自客户端的写入请求就简单地覆写某个键值，那么节点就会永久地不一致。 有一种解决的办法就是 Last write wins (discarding concurrent writes)，所谓 Last write wins 就是只需要存储 “最近” 的值，并允许 “更旧” 的值被覆盖和抛弃。那么实现上其实可以为每个写入附加一个时间戳，然后挑选最大的时间戳作为 “最近的”，并丢弃具有较早时间戳的任何写入。 分区 分区通常与复制结合使用，使得每个分区的副本存储在多个节点上，一个节点也可能存储多个分区。每个分区领导者（主库）被分配给一个节点，追随者（从库）被分配给其他节点。 每个节点可能是某些分区的主库，同时是其他分区的从库。 我用 TiDB 的存储 TiKV 来举例好了，TiKV 的数据会按 Region 进行存放，一个 Region 就是一个分区。当某个 Region 的大小超过一定限制（默认是 144MB），TiKV 会将它分裂为两个或者更多个 Region，以保证各个 Region 的大小是大致接近的，同样，当某个 Region 因为大量的删除请求导致 Region 的大小变得更小时，TiKV 会将比较小的两个相邻 Region 合并为一个。 将数据划分成 Region 后，TiKV 会尽量保证每个节点上服务的 Region 数量差不多，并以 Region 为单位做 Raft 的复制和成员管理。也就是如下图，你可以看到不同的 node 节点其实都有一份 Region 的复制。 分区的划分 假设你有大量数据并且想要分区，如何决定在哪些节点上存储哪些记录呢？分区目标是将数据和查询负载均匀分布在各个节点上。如果分区是不公平的，一些分区比其他分区有更多的数据或查询，我们称之为偏斜（skew）。不均衡会导致负载可能不同节点不一致，高负载高的分区被称为热点（hot spot），这就丧失了分区的平衡负载的特性，这是需要避免的。 其实最简单的方法就是将记录随机分配给节点。这将在所有节点上平均分配数据，但是它有一个很大的缺点：当你试图读取一个特定的值时，你无法知道它在哪个节点上，所以你必须并行地查询所有的节点，这显然是不科学的。 所以现在有两种比较典型的方案： Range：按照 Key 分 Range，某一段连续的 Key 都保存在一个存储节点上。 Hash：按照 Key 做 Hash，根据 Hash 值选择对应的存储节点。 根据键的 Range 分区 那么不随机的话，一种分区的方法是为每个分区指定一块连续的键范围（从最小值到最大值）。但是键的范围不一定均匀分布，因为数据也很可能不均匀分布。为了均匀分配数据，分区边界需要依据数据调整。 分区边界可以由管理员手动选择，也可以由数据库自动选择，然后在每个分区中，我们可以按照一定的顺序保存键。 对于 TiDB 来说也是根据键的范围进行分区，每个分区被称作 Region，因为有序的，所以可以用 [StartKey，EndKey) 这样一个左闭右开区间来描述。然后它是根据 Region 的大小来进行边界的调整，默认一个 Region 在 96 MiB 的时候就会创建新的 Region。 根据范围来划分分区还有个问题就是特定的访问模式会导致热点， 如果主键是时间戳，给每天分配一个分区，那么当天的数据的分区可能会因写入而过载，所以需要使用除了时间戳以外的其他东西作为主键的第一个部分。 根据键的 Hash 分区 hash 分区就是通过 hash 散列函数，无论何时给定一个新的字符串输入，它将返回一个 0 到 2^32 -1 之间的 “随机” 数。即使输入的字符串非常相似，它们的散列也会均匀分布在这个数字范围内。然后为每个分区分配一个散列范围，每个通过哈希散列落在分区范围内的键将被存储在该分区中。 但是这样也有缺点，失去了高效执行范围查询的能力，所以对于关系型的数据库，因为经常性的需要表扫描或者索引扫描，基本上都会使用范围的分片策略，像 nosql 数据库 redis cluster 就使用的是 Hash 分区。 分区再平衡 固定分区数 上面说了根据键的 Hash 分区，那么怎么根据 hash 值存放入分区中呢？一个简单的想法是让分区数等于机器节点数，根据分区数取模，也就是 hash(key) % 分区数。那么如果想要增加新的分区该怎么办呢？怎么把一个分区的数据迁移到另一个分区？这种数据的迁移成本其实会很高。 除此之外还可以用另一种方式：创建比节点更多的分区，并为每个节点分配多个分区。例如，运行在 10 个节点的集群上的数据库可能会从一开始就被拆分为 1,000 个分区，因此大约有 100 个分区被分配给每个节点。如果一个节点被添加到集群中，新节点可以从当前每个节点中窃取一些一些分区，直到分区再次公平分配。 只有分区在节点之间的移动。分区的数量不会改变，键所指定的分区也不会改变。唯一改变的是分区所在的节点。 redis cluster 就是采用这种方式，Redis Cluster为整个集群定义了一共 16384 个 slot，slot 就是分区，每个节点负责一部分的 slot。然后 key 会根据 crc16 计算出得结果和 16384 取模进行 slot 定位，从而定位到具体节点。 动态分区 对于根据键的 Range 分区的数据库，具有固定边界的固定数量的分区将非常不便，所以一般是当分区增长到超过配置的大小时进行自动分区，我上面也讲到了 TiDB 它是根据分区的大小来进行边界的调整，默认一个 分区在 96MB的时候就会创建新的分区。 并且除了分裂以外还能进行分区的合并，TiDB 会根据分区的大小和 key 的数量，默认小于 20MB 并且 key 数量小于 200000 会触发合并。 路由 最后要说的就是路由了，客户端怎么知道我要查的数据在哪个分区。一般来说有三种方式： 让 client 随便连接哪个节点都行，如果正好数据在这个节点上，那么直接查询即可；如果数据不在这个节点上，这个节点会将请求转发到数据所在的节点上，这种集群是去中心化的，Redis Cluster 就是这种架构，节点与节点之间通过 gossip 协议来交互信息； client首先要连接集群的路由层，路由层里面知道该请求的数据在哪个分区的哪个节点上，TiDB 就是这种架构的，TiDB 集群里面的 PD 负责管理所有分区的路由信息，TiDB 要访问某个分区的数据时，需要先向 PD 查询此分区的状态和位置； 最后一种方式就是 client 自己保存了分区和节点的信息，这样客户端就直接查询到想要的数据返回了。 总结 本篇主要是总结了复制和分区的一些主要技术的细节点，讨论了各种实现方式的异同，以及可能出现的问题，然后结合目前主流数据库来讲解如何应用落地。 无论是复制还是分区，都是围绕可用性、性能、扩展性这几个方面展开的。对于复制来说主要有单主复制、多主复制、无主复制。 目前最流行的还是单主复制，很多分布式数据库都是单主复制+分区架构来提供大吞吐的支持，并且单主复制不需要担心冲突解决，实现起来更简单。但是单主复制也可能因为异步复制 leader 宕机而造成数据丢失，所以很多都是半同步（semi-synchronous ）的方式进行复制，例如 kafka 加入 ISR 防丢机制，ISR 一组可靠的备份集合，只有当 ISR 里机器都成功复制，才认为这条消息被成功提交。 再来就是讲了由复制延迟引起的奇怪行为，比如在主库写了数据，但是还没同步到从库，那么将查不出来数据；还有就是从延迟大的从库查的数据和延迟小的从库查的数据也不一样。 接下来就是分区，这个技术在很多分布式数据库都有应用，使用它主要是为了伸缩性，因为不同的分区可以放在不共享集群中的不同节点上所以，并发负载都可以分摊到不同的处理器上，当负载增加时可以添加新的分区，当负载降低时也可以合并分区。 分区就需要考虑到分区怎么划分，一般有按range划分，按 hash 划分。还需要考虑分区的再平衡问题，添加新的节点分区数怎么分配，才能让负载均摊。然后就是路由，客户端的请求过来应该路由到哪个分区，哪个节点。 Reference 《Designing Data-Intensive Application》 ","link":"https://phantomma.top/post/shu-ju-ku-de-shu-ju-fu-zhi-yu-fen-qu/"},{"title":"函数式编程语言的柯里化含义","content":"函数式编程语言的几个特征中比较难于理解的就是柯里化了，下面一个简单的例子帮助理解一下，一句话概括的话就是: 柯里化使函数式编程语言具有动态创建函数的能力。 // 普通过程式编程语言的写法 def fn1(x: Int, y: Int) = x + y // 函数式编程语言的写法 def fn2(x: Int)(y: Int) = x + y fn1(1, 2) fn2(1)(2) // 但是函数式编程语言具有柯里化的特性，允许动态的创建一个函数，比如 var add10 = fn2(10)(_) add10(3) // 函数式编程的柯里化其实相当于实现了数学公式：fa(x, y) = fb(x)(y)，把函数fa拆解成两个函数fb(x)和fb(x)(y) output: scala&gt; def fn1(x: Int, y: Int) = x + y fn1: (x: Int, y: Int)Int scala&gt; def fn2(x: Int)(y: Int) = x + y fn2: (x: Int)(y: Int)Int scala&gt; fn1(1, 2) res43: Int = 3 scala&gt; fn2(1)(2) res44: Int = 3 scala&gt; var add10 = fn2(10)(_) add10: Int =&gt; Int = &lt;function1&gt; scala&gt; add10(3) res45: Int = 13 ","link":"https://phantomma.top/post/han-shu-shi-bian-cheng-yu-yan-de-ke-li-hua-han-yi/"},{"title":"计算地图上一个点附近的点的两种方法","content":"随着移动终端的普及，LBS的应用慢慢增多起来，比如附近的人功能成了陌陌微信等im的标配，以及大众点评的附近的餐馆、酒店等。 通过一个点的gps信息找出它附近的点，就是上面这样功能的核心。通常有两种实现方法，各有优缺点，需要根据场景不同选择适合的。 一、geohash + sql数据库 首先按照地址的经纬度按照下图的方法得出0101的编码，编码位数越多，描述地址的精确度越高。具体方法是：先计算维度，范围(18.09, 53.33)平分成两个区间(18.09, 35.71)、(35.71, 53.33)， 如果目标纬度位于前一个区间，则编码为0，否则编码为1。由于39.92324属于(35.71, 53.33)，所以取编码为1。然后再将(35.71, 53.33)分成 (35.71, 44.52), (44.52, 53.33)两个区间，而39.92324位于(35.71, 44.52)，所以编码为0。以此类推，直到精度符合要求为止，得到纬度编码为1011 1000 1100 0111 1001。 然后再对得出的类似这种形式的地址11100 11101 00100 01111 00000 01101 01011 00001进行base32编码，用0-9、b-z（去掉a, i, l, o）这32个字母进行base32编码，得到(39.92324, 116.3906)的编码为wx4g0ec1，这个就是地址的geohash值。 可以把编码之后的地址数据放入数据库，以geohash字段建索引。 查找一个点附近的店就可以通过这种方式，sql数据库：SELECT * FROM address WHERE geohash LIKE 'wx4g0%'。wx4g0代表的是地图上一个正方形的块，如下图阴影选择的区域。 二、经度、维度取交集 + 内存排序 保存地址数据的时候，冗余一个城市的字段 系统启动时加载所有的地址点到内存 以城市对数据分片，每个城市内分别建立经度、纬度上地址点的索引，使用TreeMap保存 给出一个经度120.024082，计算附近1000m以内的点，比如：事先计算好的1000m距离的经度差是0.01042，那么就可以查询在[120.018872, 120.029292]范围内的点，同样的方法，查询到纬度距离1000米以内的点 然后取两个集合的交集，如果取得的交集是空，那么可以重复步骤2，扩大查找范围到1000m*5，再次查询经度、纬度上的点，然后再取交集 取得交集后的点就是图中以红点为中心，正方型圈出来的点，然后计算与红点的具体距离，距离计算公式可以参考这篇文章里的算法 然后对交集里的点按距离排序，就能够得到按照距离排序后的点了 为什么按照城市分片是因为，如果在全国范围内构件经纬度的索引，查询一个点附近的点，这个跨度就会很大，可能1000m内纬度距离内的点就会遍布在内蒙古和海南，这就会把距离很远的点也列入附近的点考虑的范围，实际取交集后重合的点却很少。所以一种优化的做法是，按照城市的维度分别构建经度、纬度的map。给出一个点时首先根据经纬度计算出它所在的城市，然后在城市的范围内查找，这样取交集成功的机会就会大很多，查找效率会提高不少 优缺点比较： geohash 优点： 可以支持地址点的位置信息变化的情况，需要更新数据库里地geohash字段 数据量小时，数据库很好的承接了附近点的计算 缺点： 特殊情况时计算的附近的点不准确，比如正好处于正方形边缘的点 需要提前计算geohash值，并持久化到数据库中 数据量增多，like查询会变慢，数据库查询、写入压力大，容易形成单点 开发成本较高 内存计算 优点： 计算附近的点相对准确 开发成本低，不需要额外建表 可以通过不同机器加载分片的数据，解决数据量大的问题 缺点： 需要系统启动时加载数据并构件经度、纬度上地址点的索引，在构建完索引之前，是没法提供服务的 地址点的位置信息不能改变，对于经常变化位置的场景需要采用geohash 对于两个属于不同市的相距很近的点是计算不到的，这个需要根据场景取舍，此方法适用于附近的餐馆、银行等场景，因为它们有极大可能处在同一市内，而不适用于周围的人 ","link":"https://phantomma.top/post/ji-suan-di-tu-shang-yi-ge-dian-fu-jin-de-dian-de-liang-chong-fang-fa-shi-jian/"},{"title":"2PC/3PC 学习小结","content":"2PC 2PC是分布式系统中最简单的一致性协议，其实质是通过把事务提交分成预提交和确认提交两个阶段，来给所有参与者一个表态(赞成或者反对)的机会。 在两阶段提交的过程中有两个重要的时间节点：一个是参与者投票。每个参与者在作出决定之后，这个决定是无法自己单方面更改和撤销的；另一个是协调者决定。协调者根据参与者反馈作出全局决定，该决定被持久化到日志之后也无法变更。 这个协议最让人诟病的地方在于它是一个阻塞式的协议，体现在： 若协调者发送完预提交请求之后出现故障，所有的参与者在执行完事务后都会阻塞。由于参与者无法提交或者退出事务，导致自己占有的本地资源无法释放，影响本地其它事务执行。 若参与者在投赞成票之后出现故障（投反对票可以单方面退出），协调者无法将决定告知该参与者。为了保证数据的强一致性，协调者必须一直重试提交直到参与者应答为止。 上述缺点在大型分布式系统中无法容忍，因为随着集群规模增大，节点故障几乎是必然事件。 3PC 针对2PC的不足，论文A Formal Model of Crash Recovery in a Distributed System提出了3PC的初始模型，可以看作是2PC的升级版。3PC协议有两个重要改进，一是将2PC的commit阶段进一步分成pre-commit和do-commit两步，二是引入超时机制。这两个改进提升了系统的容错性。 3PC中若协调者在第一阶段发送完can-commit请求之后发生故障，所有参与者都会因为超时退出事务，避免了阻塞；若协调者在发送完pre-commit请求后发生故障，参与者则会在等待do-commit上超时后提交事务。这里之所以能安全提交，关键在于参与者已经处于pre-commit状态（即收到并处理了协调者的pre-commit请求），这说明所有参与者都已经在can-commit阶段投票赞成。回过头看2PC，如果第二阶段收不到commit请求，参与者此时既不能提交也不能退出事务，因为它无法得知协调者作出的全局决定。这也是为什么即使2PC引入超时机制也没有作用的原因。 在容错方面，无论是协调者还是参与者，节点在恢复之后都需要向其它参与者询问之前的事务状态来保证数据一致性。协调者在恢复后，如果发现有任何参与者已经提交或者退出，自己会相应作出一致的决定；参与者同理。不过要注意的是，即使参与者恢复后发现自己已经处于pre-commit状态也不能就马上提交，仍然需要询问其他参与者事务状态，因为协调者可能在等待自己对pre-commit的确认时超时而让其它参与者退出。 然而，我某天偶然想到这样一个场景：假设有A，B和C三个节点，其中C是协调者，A和B是参与者。C在发送完pre-commit请求给A后还没来得及把相同请求发送给B就停机，协议怎么处理？按照我之前的理解，A收到请求后进入pre-commit状态，等待do-commit超时提交事务；B尚处于can-commit状态，等待pre-commit超时退出事务。C恢复之后询问A和B，A和B都处于最终状态，且状态不一致。 我查了好多资料都没有找到满意答案，直到读到Three-phase commit protocol这篇对3PC协议总结性的文章。虽然文章内容没有直接涉及上述问题，但是我在其参考文献MaBE Agents and n-Phase-Commit里找到了协议的处理方法。 If a participant cannot receive a message in the wait state wi, it asks all re-maining participants if one of them is in PRE-COMMIT state. If there is one, the participant will shift to PRE-COMMIT and finally commit, otherwise it will go to ABORT. 简单来说，参与者B在can-commit状态中等待协调者的pre-commit请求时如果发生超时，并不是简单退出，而是需要查看有没有其它参与者已经处于了pre-commit状态。如果存在至少一个这样的参与者，那么说明协调者C是在发送了部分pre-commit请求之后才停机的，于是为了保证数据一致性自己也提交事务。 看上去3PC比2PC似乎好了不少，但是实际使用仍然存在重大缺陷： 仅容忍单点故障，协议不允许协调者和参与者同时停机。继续沿用上面的例子，C停机后B询问A的状态，A这时也停机，B等待A超时后会陷入两难境地。 同步通信模型。该模型假设网络延迟有上限，不能出现网络分区。3PC的超时机制基于这个假设，一旦超时则表明通信节点出现永久故障。而实际系统一般都只符合部分同步（partially synchronous）假设，通信超时的情况下无法准确分辨到底是网络问题还是节点故障。 总体感觉3PC协议理论价值大于实用价值。 ","link":"https://phantomma.top/post/2pc3pc-xue-xi-xiao-jie/"},{"title":"MySQL中的 redo log，binlog 和 change buffer","content":"网上有很多讲WAL机制的文章，介绍MySQL如何通过日志来保证在故障场景下的数据完整性的。简单来说，数据库在写入数据之前会先记录修改日志，然后崩溃重启时能够通过日志来重建数据。这个表述非常笼统，表现在 这里的日志到底指什么，redo log还是binlog，还是两者都有？ 既然能够通过日志来重建数据，当数据库崩溃时它就一定存在于磁盘上。那么，事务日志任意时刻都在磁盘上吗？ 故障场景下数据库真的能保证数据零丢失？ 这些问题涉及到MySQL实现中的一些细节，下面一一讨论。 以下篇幅不再赘述WAL，redo log及其checkpoint机制，binlog等概念本身。 先简单回顾下2PC流程。当前session设置成autocommit=1的情况下，单条更新语句作为事务自动提交的过程如下： 内存中应用修改 写入redo log，redo log处于prepare状态 写入binlog redo log变为commit状态，事务提交成功 需要注意的是，该流程仅仅描述了事务执行commit语句时的中间状态。大多数事务内会包含多条更新语句，每条更新都会在redo log中有对应的记录，那么对于正在执行还未最后提交的事务，其对应的redo log是什么状态呢？这里涉及到另一个问题，上述过程中的”写入”到底是写入内存还是磁盘？其实两者都是有可能的，取决于innodb_flush_log_at_trx_commit和sync_binlog的具体设置，两个参数分别控制了redo log和binlog的落盘时机。 1 innodb_flush_log_at_trx_commit = 1并且sync_binlog = 1 这是系统默认也是最为简单的情形，即事务提交时强制fsync落盘log。但是这里的落盘只是提交时落盘，如若一个事务内多条语句，执行期间每条更新对应的redo log只写入了内存中的redo log buffer，buffer中的redo log在事务commit的时候才会多条一起写盘。 为什么未提交的事务的redo log不需要写盘呢？因为没有必要。假设事务执行过程中数据库挂掉，内存中的redo log丢失，重启之后对于数据库来说这个事务好像根本没有存在过一样，这种行为对于业务是无损的，因为事务根本没有提交。 这里又有一个微妙的问题，既然redo log在事务commit的时候才写盘，那么是不是所有已经写盘的redo log都对应了已经提交的事务呢？答案是否。redo log从内存buffer中写入磁盘有三个时机： 第一个是本事务commit； 第二是后台线程每隔1s异步刷入； 第三是在成组提交时和正在commit的并发事务一起写盘。 很明显，对于第二和第三两种情况，会出现把未提交事务的redo log写盘的情况。这样做会不会产生问题？假设数据库宕机，重启时该怎么处理未提交事务的redo log？这个问题需要回到开始时描述的2PC流程，redo log是有状态的，事务提交时会先变成prepare状态再变commit状态。一个未提交的事务，中间更新所对应的redo log连prepare状态都不是，于是数据库重启时只要从checkpoint按顺序依次播放redo log，跳过非prepare非commit状态的记录便可恢复宕机前一刻的内存状态。另外要注意的是，崩溃恢复的时候对于prepare状态的redo log是需要特殊处理的，这时候根据redo log的trx id找到对应的binlog，如果binlog完整，即使redo log还处于prepare状态也认为事务提交成功；如果binlog缺失，认定事务提交失败。 和redo log类似，binlog也是有binlog cache的。事务还未提交时记录写入cache，事务提交时完成写盘。与redo log buffer是一块所有事务线程共享的全局内存不同，每个线程都有自己私有的binlog cache，但是binlog文件只有一个。 在参数双1设置下，事务提交时redo log和binlog都必须要落盘。因为有落盘的保证，只要磁盘完好，任何情况下都不会有数据丢失。 2 innodb_flush_log_at_trx_commit = 2或者sync_binlog = N 根据官方文档的说明，事务提交的时候redo log写缓存但并不强制fsync；binlog写到binlog文件中不强制fsync, 但是保证每N个事务fsync一次。初看有点糊涂，这里的缓存和binlog文件是什么东西？实际上redo log和binlog真正写入磁盘前是有两级缓存的，以redo log为例，第一级是redo log buffer，它属于server进程；第二级是文件系统的页面缓存，这是归操作系统内核管理的；最下面一级才是磁盘。 在上述设置下，事务提交时redo log和binlog只保证写入文件系统的页面缓存。相比设置1）中强制fsync写盘，这里commit返回的速度会非常快，log只是从一块内存被复制到了另一块内存，与写盘相比大大提高了IOPS，但是这样系统出现故障的时候会不会产生问题？为了方便说明，这里把故障种类按照严重程度递增顺序分为三级：第一级是数据库server本身挂掉，第二级是操作系统崩溃，第三级是整机故障比如停电。 在innodb_flush_log_at_trx_commit = 2或者sync_binlog = N的时候，如果只发生第一级故障，数据依然会是完整的，因为log记录已经保证在页面缓存中了，只要操作系统能正常运行，刷回磁盘只是时间问题。但如果发生二级甚至三级故障，处于页面缓存但还没来得及fsync的记录将会全部丢失。考虑到redo log有后台线程刷磁盘，binlog每隔N个事务也刷一次磁盘，最坏情况下数据库会丢失最近1s内的redo log和N个事务的binlog。从业务角度来看，就是事务提交成功确认，但是对应的更新却没有反映在数据库中，破坏了事务ACID特性中的持久化要求。1s时间看似很短，但是对于现在动辄上万tps的大型应用来说是无法容忍的。 该参数设置下，redo log和binlog只写入页面缓存，不保证最终落盘，提高了数据库吞吐量但是同时带来了数据丢失风险。 3 innodb_flush_log_at_trx_commit = 0并且sync_binlog = 0 该设置下，事务提交时除了把redo log写入redo log buffer外没有任何保证，对应binlog只调用write写入binlog文件（即操作系统页缓存)，没有fsync。和设置2）类似，在发生第二和第三级故障时数据丢失，且由于binlog不会每N个事务刷一次磁盘，数据丢失量只会更大。比设置2）更危险的是，哪怕只是数据库本身崩溃重启，未落盘的redo log和binlog也会全部丢失。 总结一下上面内容，事务提交时的2PC机制保证了redo log和binlog的一致性。在数据库故障重启的场景下，恢复过程是根据redo log来完成的，换句话说，redo log具备完全的crash-recovery能力。innodb_flush_log_at_trx_commit和sync_binlog这两个参数分别控制了事务提交时redo log和binlog的写盘时机，其具体的设置值决定了redo log和binlog在极端情况下的完整程度。通过调整设置，可以在数据完整性和吞吐量之间作权衡。 由于redo log的存在，数据更新发生时其对应的修改只存在于内存的buffer pool中，磁盘数据并没有变化，也就是说，内存中存放了数据脏页。数据脏页会定期从内存刷回磁盘。但是，刷盘操作和redo log是没有关系的，并不会在redo log中留下记录。容易混淆的原因是redo log的checkpoint在顺序推移的过程中会主动写盘。但是数据脏页刷盘除了checkpoint推进，还会在buffer pool占满需要淘汰数据页以及后台线程定期刷盘的时候进行。那么按照redo log和数据页刷盘过程的设计，可能会出现同一个数据页被多次修改，这些修改均在当前checkpoint之后，而该脏页也还没来得及刷回磁盘的情况。在这种情况下，checkpoint在推进的过程中只需在执行第一个针对该数据页的修改的时候将脏页刷回磁盘即可，后面的记录可以不予理会。这样做的原因是第一次刷回磁盘的脏页已经包含了所有的修改，之后不必重复刷盘，进一步提升了IO效率。 针对非唯一索引的change buffer优化又让redo log机制变得更复杂了些。上一段简单总结了数据脏页如何刷盘，其前提是buffer pool中的数据页已经包含了真实的修改。然而change buffer的引入，会出现内存中数据页完全”干净”的情况。一个基于非唯一索引的修改，真实包含该修改的数据页可能在内存中没有，磁盘上也没有。前面分析了由于有redo log存在，即使内存中的脏页没来得及写盘，在数据库崩溃重启的时候，也不会造成数据丢失。那么加上change buffer后，内存中可能连脏页都不是的情况，数据库是如何故障恢复的呢？ change buffer名字中带buffer，看似是内存中的数据，其实它既在内存中有，同时也会被持久化到磁盘，物理上存在于系统表空间ibdata1中。其中一个原因是内存中的change buffer共享了buffer pool的空间，最大比例可以通过innodb_change_buffer_max_size控制。既然有空间限制，那么必然会出现buffer占满的情况，这时候除了持久化到磁盘别无选择；另外一个因素就是故障恢复的需要。 数据库故障的情况下，已经持久化的change buffer自然没有什么问题，内存中那部分的丢失如何处理？又要回到文章开头的2PC机制a）步骤中先应用修改到内存。这里面包含了两种可能，一是真实修改写到内存，二是能利用change buffer优化的情况下不立即将真实修改应用到内存，而是把修改意图写到change buffer。第一种情况下数据变动会记录到redo log，第二种情况中写change buffer这个动作本身也是会被记录到redo log的，这点非常关键。如果数据库在b）步骤中或者之前挂掉（包括事务正在执行还没commit以及commit时进行到b）步骤），redo log没有记录change buffer中的改动，重启之后这部分数据是会被丢弃的，但是由于事务未成功提交所以业务无损。只要至少c）步骤完成，redo log和binlog一致，事务会被认定执行成功。数据库重启恢复的时候，可以根据崩溃前已经持久化的change buffer和redo log上的内容重建完整的change buffer。 既然change buffer有持久化的需求，那么磁盘上的change buffer会不会无限变大？其实不会，和内存中的change buffer一样，每次目标数据页被读取或者后台线程定期启动将change buffer的内容和目标数据页进行merge的时候，对应的change buffer记录就可以被丢弃了。不过merge的时候，也是需要写redo log的。但凡对数据页进行修改，都必须写redo log，因为只有这样才能保证即使数据库宕机脏页也不丢失。 change buffer中的记录保证把内存中的数据页变成脏页，它的使命就完成了，至于后面的数据脏页刷盘和它通通没关系。但是比较微妙的一点是，change buffer本身从内存到磁盘的持久化过程，和普通数据页共享了同一刷盘机制。 本文理了理在数据库故障重启场景下，redo log，binlog和change buffer如何协同配合来保证数据完整性的。最后回答一下开头的三个问题，a）故障恢复主要依靠redo log；b）事务日志并不一定在磁盘；c）数据零丢失是有前提条件的，取决于参数设置。 ","link":"https://phantomma.top/post/mysql-zhong-de-redo-logbinlog-he-change-buffer/"},{"title":"mysql的一致性视图（MVCC）","content":"提到事务，你肯定会想到ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性），我们就来说说其中I，也就是“隔离性”。 当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，所以下面我们来说说隔离级别。 SQL标准的事务隔离级别包括：读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）、串行化（serializable）。 读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。 读提交指，一个事务提交之后，它做的变更才会被其他事务看到。 可重复读指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据时一致的。当然可重复读隔离级别下，未提交变更对其他事务也是不可见的。 串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 MySQL中支持的四种隔离级别 MySQL虽然支持4种隔离级别，但与SQL标准中所规定的各级隔离级别允许发生的问题却有些出入，MySQL在REPEATABLE READ隔离级别下，是可以禁止幻读问题的发生的。 我们可以通过： SET [GLOBAL|SESSION] TRANSACTION ISOLATION LEVEL level; 来设置隔离级别。 其中的level可选值有4个： REPEATABLE READ READ COMMITTED READ UNCOMMITTED SERIALIZABLE MVCC原理 对于使用InnoDB存储引擎的表来说，它的聚簇索引记录中都包含必要的隐藏列： trx_id：每次一个事务对某条聚簇索引记录进行改动时，都会把该事务的事务id赋值给trx_id隐藏列。 ReadView ReadView所解决的问题是使用READ COMMITTED和REPEATABLE READ隔离级别的事务中，不能读到未提交的记录，这需要判断一下版本链中的哪个版本是当前事务可见的。 ReadView中主要包含4个比较重要的内容： m_ids：表示在生成ReadView时当前系统中活跃的读写事务的事务id列表。 min_trx_id：表示在生成ReadView时当前系统中活跃的读写事务中最小的事务id，也就是m_ids中的最小值。 max_trx_id：表示生成ReadView时系统中应该分配给下一个事务的id值。 creator_trx_id：表示生成该ReadView的事务的事务id。 ReadView是如何工作的？ 有了这些信息，这样在访问某条记录时，只需要按照下边的步骤判断记录的某个版本是否可见： 如果被访问版本的trx_id属性值与ReadView中的creator_trx_id值相同，意味着当前事务在访问它自己修改过的记录，所以该版本可以被当前事务访问。 如果被访问版本的trx_id属性值小于ReadView中的min_trx_id值，表明生成该版本的事务在当前事务生成ReadView前已经提交，所以该版本可以被当前事务访问。 如果被访问版本的trx_id属性值大于ReadView中的max_trx_id值，表明生成该版本的事务在当前事务生成ReadView后才开启，所以该版本不可以被当前事务访问。 如果被访问版本的trx_id属性值在ReadView的min_trx_id和max_trx_id之间，那就需要判断一下trx_id属性值是不是在m_ids列表中，如果在，说明创建ReadView时生成该版本的事务还是活跃的，该版本不可以被访问；如果不在，说明创建ReadView时生成该版本的事务已经被提交，该版本可以被访问。 如果某个版本的数据对当前事务不可见的话，那就顺着版本链找到下一个版本的数据，继续按照上边的步骤判断可见性，依此类推，直到版本链中的最后一个版本。如果最后一个版本也不可见的话，那么就意味着该条记录对该事务完全不可见，查询结果就不包含该记录。 在MySQL中，READ COMMITTED和REPEATABLE READ隔离级别的的一个非常大的区别就是它们生成ReadView的时机不同。 我们这里使用一个示例来解释： mysql&gt; CREATE TABLE `t` ( `id` int(11) NOT NULL, `k` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; insert into t(id, k) values(1,1) ; 事务A 事务B begin begin update t set k= k+1 where id=1; commit； update t set k = k+1 where id=1; select k from t where id =1; commit; 在这个例子中，我们做如下假设: 事务A、B的版本号分别是100、200，且当前系统里只有这3个事务； 三个事务开始前，(1,1）这一行数据的row trx_id是90。 READ COMMITTED —— 每次读取数据前都生成一个ReadView 继续上面的例子，假设现在有一个使用READ COMMITTED隔离级别的事务开始执行： # 使用READ COMMITTED隔离级别的事务 BEGIN; # SELECT1：Transaction 100、200未提交 select k from t where id=1 ; # 得到值为1 这个SELECT1的执行过程如下： 在执行SELECT语句时会先生成一个ReadView，ReadView的m_ids列表的内容就是[100, 200]，min_trx_id为100，max_trx_id为201，creator_trx_id为0。 然后从版本链中挑选可见的记录，最新的版本trx_id值为200，在m_ids列表内，所以不符合可见性要求 下一个版本的trx_id值也为100，也在m_ids列表内，所以也不符合要求，继续跳到下一个版本。 下一个版本的trx_id值为90，小于ReadView中的min_trx_id值100，所以这个版本是符合要求的。 之后，我们把事务B的事务提交一下，然后再到刚才使用READ COMMITTED隔离级别的事务中继续查找，如下： # 使用READ COMMITTED隔离级别的事务 BEGIN; # SELECT1：Transaction 100、200均未提交 SELECT * FROM hero WHERE number = 1; # 得到值为1 # SELECT2：Transaction 200提交，Transaction 100未提交 SELECT * FROM hero WHERE number = 1; # 得到值为2 这个SELECT2的执行过程如下： 在执行SELECT语句时会又会单独生成一个ReadView，该ReadView的m_ids列表的内容就是[100]（事务id为200的那个事务已经提交了，所以再次生成快照时就没有它了），min_trx_id为100，max_trx_id为201，creator_trx_id为0。 然后从版本链中挑选可见的记录，从图中可以看出，最新版本trx_id值为100，在m_ids列表内，所以不符合可见性要求 下一个版本的trx_id值为200,小于max_trx_id，并且不在m_ids列表中，所以可见，返回的值为2 REPEATABLE READ —— 在第一次读取数据时生成一个ReadView 假设现在有一个使用REPEATABLE READ隔离级别的事务开始执行： # 使用REPEATABLE READ隔离级别的事务 BEGIN; # SELECT1：Transaction 100、200未提交 SELECT * FROM hero WHERE number = 1; # 得到值为1 这个SELECT1的执行过程如下： 在执行SELECT语句时会先生成一个ReadView，ReadView的m_ids列表的内容就是[100, 200]，min_trx_id为100，max_trx_id为201，creator_trx_id为0。 然后从版本链中挑选可见的记录，该版本的trx_id值为100，在m_ids列表内，所以不符合可见性要求 下一个版本该版本的trx_id值为200，也在m_ids列表内，所以也不符合要求，继续跳到下一个版本。 下一个版本的trx_id值为90，小于ReadView中的min_trx_id值100，所以这个版本是符合要求的。 之后，我们把事务B的事务提交一下。然后再到刚才使用REPEATABLE READ隔离级别的事务中继续查找: # 使用REPEATABLE READ隔离级别的事务 BEGIN; # SELECT1：Transaction 100、200均未提交 SELECT * FROM hero WHERE number = 1; # 得到值为1 # SELECT2：Transaction 200提交，Transaction 100未提交 SELECT * FROM hero WHERE number = 1; # 得到值为1 这个SELECT2的执行过程如下： 因为当前事务的隔离级别为REPEATABLE READ，而之前在执行SELECT1时已经生成过ReadView了，所以此时直接复用之前的ReadView，之前的ReadView的m_ids列表的内容就是[100, 200]，min_trx_id为100，max_trx_id为201，creator_trx_id为0。 然后从版本链中挑选可见的记录，该版本的trx_id值为100，在m_ids列表内，所以不符合可见性要求 下一个版本该版本的trx_id值为200，也在m_ids列表内，所以也不符合要求，继续跳到下一个版本。 下一个版本的trx_id值为90，小于ReadView中的min_trx_id值100，所以这个版本是符合要求的。 ","link":"https://phantomma.top/post/mysql-de-yi-zhi-xing-shi-tu-mvcc/"},{"title":"中间件性能挑战赛 比赛小结","content":"上周参加中间件性能挑战赛，赛题的大致意思就是充分利用多核CPU，看谁开发的程序最快。对于这类题目，语言的选择上就显得很重要，因为对于同样的算法，在都做了充分的优化的情况下，就剩下纯粹的比拼语言的执行效率了。 所以我选择了用go，可能有些同学还不太了解，首先看一下go语言的显著的几个特点： 和c\\c++一样都是编译型语言，直接编译成本地机器码执行，官方所说执行效率逼近c 语言自带垃圾回收，编译时编译进运行时 语言内建协程，以非常廉价的方式实现并行 编程多范式支持，可以既面向过程又面向对象，还可以函数式编程 下面简单说一下我的思路: 我使用的字典树的数据结构，这种数据结构对于小文本的来说是比较有利的，后面换成大文本，字典树就有些力不从心了。基本结构就是一个节点代表一个字符，里面包含从这个节点向上遍历形成的单词的count。 type CharTreeNode struct { Count int Children [26]*CharTreeNode } 总体的流程图是： 申请一个很大的byte数组，一次性把文本读入内存。这个时间没什么好优化的了，因为即使是1.1G的文件，这个时间大约是600多ms。 多协程构建字典树。引入生产者消费者模型，有生产线程不断从byte数组中拆分byte片段，这里的拆分需要注意的是不能把一个单词拆分成两半。 然后把拆分出来的byte片段的指针不断放到缓冲通道里去，同时会启动和cpu核数相当的消费者协程不断从缓冲通道里消费byte片段。 消费者协程共享同一棵字典树，一旦拿到byte片段，就调用构建字典树的方法。这里为什么采用这样的方案是因为，最初可以分为cpu核数 大的byte片段，然后每个线程分别去构建这几个大的片段，这时会发现，分割的粒度太粗，每个协程构建字典树的时间差异很大，有的很快完成，有的会慢一些。那么最终的总耗时时间会由最慢的协程决定。所以引入生产者消费者来达到使分割的粒度足够小，每个协程的构建时间能比较平均，最后总的耗时会整体变小。这里也是利用了go语言的高效并发通信的优势，go内建了协程间的通信通道，不同执行体之间的通信不像java一样采用共享内存的方式，而是采用erlang的方案，通过语言提供的高效通信通道来通信，换用go的话说就是Do not communicate by sharing memory; instead, share memory by communicating。通过这样的优化确实能达到各个执行体的耗时比较平均，总体的提升有1秒多的样子，但是依然是主要耗时任务，这个过程差不多就要3s了。 等待所有协程构建字典树完成，然后遍历字典树，count的值大于1的节点向上遍历就可以形成一个单词，然后把单词和他的count插入到一个长度固定的排序的链表，就可以拿到最后的结果。这个遍历字典树和排序的过程会总共耗时30~40ms。 所有消费者线程共享构建同一棵字典树，其中CharTreeNode中的count会更新非常频繁，为保证线程安全使用了和java中的AtomInteger同样的方式：atomic.AddInt32(&amp;current.Count, 1)。CharTreeNode另外的成员变量Children，在对某一个child赋值的时候同样存在线程安全问题，不过在所有的构建字典树协程同时对同一个节点的同一个child赋值的情况并不多，这里采用了加锁的方式保证线程安全： lock := &amp;sync.Mutex{} lock.Lock() current.Children[offset] = new(CharTreeNode) lock.Unlock() 因为执行到这里代码块的概率非常小，我做过实验，去掉加锁，对结果的准确性和耗时的影响都微乎其微。 比赛总结 所以上面的主要四个过程，可以说构建字典树占用了绝大多数的时间。这也就是文本开始是20m大小时，比较快的几个中有好多字典树的方案，到后面使用1G的文本时，他们多消失不见了。字典树这种结构个人感觉比较适合于小文本的情况，文本越大，它的劣势也就越明显。粗略估计，问题在于，字典树以字符为维度统计，而map以单词为维度统计。例如，一个小文本，可以共有100w个字符、2w种单词，比例为50；一个大文本，字符共有10000w个，单词可能是4w种，比例为2500。所以文本越大map就会相对越节省时间。 语言的选择固然重要，所以看到前五名都是c++。但是现代语言都已经很成熟，运行效率相差不大的情况下，采用的数据结构和算法才是决定因素。 ","link":"https://phantomma.top/post/coding4fun-dai-ma-bi-sai-xiao-jie/"},{"title":"go 递归函数如何传递数组切片 slice","content":"数组切片slice这个东西看起来很美好，真正用起来会发现有诸多的不爽。 第一，数组、数组切片混淆不清，使用方式完全一样，有时候一些特性又完全不一样，搞不清原理很容易误使用。 第二，数组切片的append操作，每次对slice append操作，都返回一个新的slice的引用，对slice的引用没法保持，这样在函数传递slice的情况下append，在调用函数的上下文中看不到slice append的效果。如果想要这种方式凑效，不得不另辟蹊径。本文主要说一下如何解决这个窘境的方法。 函数传递slice存在什么问题？ func sliceModify(slice []int) { // slice[0] = 88 slice = append(slice, 6) } func main() { slice := []int{1, 2, 3, 4, 5} sliceModify(slice) fmt.Println(slice) } 输出： [1 2 3 4 5] 问题所在： 虽然说数组切片在函数传递时是按照引用的语义传递的，比如说在sliceModify函数里面slice[0] = 88，在方法调用的上下文中，调用函数对slice引用的改表是看得见的。 但是在对slice进行append操作的时候，我们惊奇的发现，这次又不管用了。原因就是append操作会返回这个扩展了的slice的引用，必须让原引用重新赋值为新slice的引用，说白了就是，传递过来的这个指针原来指了内存中的A区域，A区域是原数组的真正所在。经过一次 append之后，要把这个指针改为指向B，B对应append后新的slice的引用。但是方法调用的上下文里的slice指针还是指向了老的A内存区域。 这个逻辑实在有些奇葩，这里我不得不再次吐槽append的设计。有人说这个问题好解决啊，只需要在sliceModify函数的返回值中把append后新的slice引用返回就好了。这样做当然是可以滴，但是像递归调用的函数就不好解决了。 下面就说一下这个问题的解决办法，方法也很简单，就是传递指针的指针。虽然有些绕，但是总算把问题解决了。当然也有其他的办法，比如按照java等语言的方式，自己实现一个ArrayList，在对可变数组扩展的时候，千万表改变引用了。 func sliceModify(slice *[]int) { *slice = append(*slice, 6) } func main() { slice := []int{1, 2, 3, 4, 5} sliceModify(&amp;slice) fmt.Println(slice) } 这次就可以输出预期的结果了： [1 2 3 4 5 6] 递归调用的例子： func insertTo10(arr *[]int) { length := len(*arr) if length == 10 { return } *arr = append(*arr, length) insertTo10(arr) } func main() { arr10 := []int{} insertTo10(&amp;arr10) fmt.Println(arr10) } ","link":"https://phantomma.top/post/go-di-gui-han-shu-ru-he-chuan-di-shu-zu-qie-pian-slice/"},{"title":"go slice 和数组的区别","content":" 使用方式 数组和slice长的很像，操作方式也都差不多，并且slice包含了数组的基本的操作方式，如下标、range循环，还有一些如len()则是多种类型共用，所以根据操作根本搞不清数组和切片的区别，能够看出区别的地方主要看如何声明的。 数组的声明方式很单一，通常就是下面这样： array1 := [5]int{1, 2, 3, 4, 5} array2 := [5]int{} slice的声明方式就非常多样了，如前面介绍的几种： var slice1 = []int{1, 2, 3, 4, 5} var slice2 = make([]int, 0, 5) var slice3 = make([]int, 5, 5) var slice4 = make([]int, 5) 加上对数组的切片和append操作都会产生数组切片(slice) 2.值传递or引用传递 func arrayModify(array [5]int) { newArray := array newArray[0] = 88 } func sliceModify(slice []int) { newSlice := slice newSlice[0] = 88 } func main() { array := [5]int{1, 2, 3, 4, 5} slice := []int{1, 2, 3, 4, 5} arrayModify(array) sliceModify(slice) fmt.Println(array) fmt.Println(slice) } 输出 [1 2 3 4 5] [88 2 3 4 5] 其实不只是数组，go语言中的大多数类型在函数中当作参数传递都是值语义的。也就是任何值语义的一种类型当作参数传递到调用的函数中，都会经过一次内容的copy，从一个方法栈中copy到另一个方法栈。这对于熟练java的同学需要进行一次彻底的观念转变，在java中除了少数的值类型是按照值传递，所有的类在函数传递时都是具有引用语义的，也就是通过指针传递。所以在使用时传递对象，不需要去分别值和引用。 go说到底不是一种纯粹的面向对象的语言，更多的是一种更简单高效的C，所以在参数传递上跟C保持着基本的一致性。一些较大的数据类型，比如结构体、数组等，最好使用传递指针的方式，这样就能避免在函数传递时对数据的copy。 虽然slice在传递时是按照引用语义传递，但是又因为append()操作的问题，导致即使是引用传递还是不能顺利解决一些问题，后面一篇文章将说明一下如何解决递归函数中传递slice的问题： go递归函数如何传递数组切片slice ","link":"https://phantomma.top/post/go-slice-he-shu-zu-de-qu-bie/"},{"title":"go 语言中的数组切片 slice","content":"初看go语言中的slice，觉得是可变数组的一种很不错的实现，直接在语言语法的层面支持，操作方面比起java中的ArrayList方便了许多。但是在使用了一段时间后，觉得这东西埋的坑不少，使用方式上和arrayList也有很大的不同，在使用时要格外注意。 slice的数据结构 首先说一下slice的数据结构，源码可以在google code上找到，http://code.google.com/p/go/source/browse/src/pkg/runtime/runtime.h struct Slice { byte* array; // actual data uintgo len; // number of elements uintgo cap; // allocated number of elements }; 可以看出主要保存了三个信息： 一个指向原生数组的指针 元素的个数 数组分配的存储空间 slice的基本操作 go中生成切片的方式有以下几种，这几种生成方式也对应了对slice的基本操作，每个操作后面go隐藏了很多的细节，如果没有对其足够了解，在使用时很容易被这些坑绊倒。 make函数生成 这是最基本，最原始生成slice切片的方式，通过其他方式生成的切片最终也是通过这种方式来完成。因为无论如何都需要填充上面slice结构的三个最基本信息。 通过查找源码，发现最终都是经过下面的c代码实现的： static void makeslice1(SliceType *t, intgo len, intgo cap, Slice *ret) { ret-&gt;len = len; ret-&gt;cap = cap; ret-&gt;array = runtime·cnewarray(t-&gt;elem, cap); } make函数在生成slice时的写法： var slice1 = make([]int, 0, 5) var slice2 = make([]int, 5, 5) // 省略len的写法，len默认等于cap，相当于make([]int, 5, 5) var slice3 = make([]int, 5) 这个简便的写法实在是有点坑爹，如果你写成make([]int, 5)，go会默认把数组长度len当作slice的容量，按照上面的例子，便生成了这样的结构：[0 0 0 0 0] 对数组进行切片 首先来看下面的代码： arr := [5]int{1, 2, 3, 4, 5} slice := arr[3 : 5] // slice:[4, 5] slice[0] = 0 // slice:[0, 5] fmt.Println(slice) fmt.Println(arr) 输出结果： [0 5] [1 2 3 0 5] 从上面可以看出，对数组进行了切片操作，生成的切片里的array指针实际指向了原数组的一个位置，相当于c的代码中对原数组截取生成新的数组[2]arrNew，数组的指针指向arr[3]，所以改变切片里0下标对应元素的值，实际上也就改变了原数组相应数组位置3中元素的值。 关于这个问题这篇博文说的比较详细：对Go的Slice进行Append的一个“坑” 对数组或切片进行append 个人认为这个append是go语言中实现地不太优雅的一个地方，比如对一个slice进行append必须要这样写：slice = append(slice, 1)。说白了就是，对一个slice进行append时，必须把新的引用重新赋值给slice。如果只是语法上怪异，那问题还好，只是代码写起来麻烦一点。但是实际情况是这个append操作导致的问题多多，不小心很容易走到append埋的坑里面去。 先来看一个比较奇怪的现象： var sliceA = make([]int, 0, 5) sliceB := append(sliceA, 1) fmt.Println(sliceA) fmt.Println(sliceB) 输出结果是： [] [1] 刚看到这样的结果时让人很难以理解，明明声明了容量是5的切片，现在sliceA的len是0，远没有达到切片的容量。按理说对sliceA进行append操作，在没有达到切片容量的情况下根本不需要重新申请一个新的大容量的数组，只需要在原本数组内修改元素的值。而且，go函数在传输切片时是引用传递，这样的话，sliceB和sliceA应该输出一样才对。看到这样的结果，着实让人困惑了很长时间，难道每次append操作都会重新分配数组吗？ 答案肯定不是这样的，如果真是这样的话，go也就不用再混了，性能肯定会出问题。下面从go实现append的源码中去找答案，源码位置在：http://code.google.com/p/go/source/browse/src/pkg/runtime/slice.c 代码很长，这里只截取关键的片段来说明问题： void runtime·appendslice(SliceType *t, Slice x, Slice y, Slice ret) { intgo m = x.len+y.len; void *pc; if(m &gt; x.cap) growslice1(t, x, m, &amp;ret); else ret = x; // read x[:len] if(m &gt; x.cap) runtime·racereadrangepc(x.array, x.len*w, pc, runtime·appendslice); // read y runtime·racereadrangepc(y.array, y.len*w, pc, runtime·appendslice); // write x[len(x):len(x)+len(y)] if(m &lt;= x.cap) runtime·racewriterangepc(ret.array+ret.len*w, y.len*w, pc, runtime·appendslice); ret.len += y.len; FLUSH(&amp;ret); } 函数定义 appendslice(SliceType *t, Slice x, Slice y, Slice ret)，对应 slice3 = append(slice1, slice1...)操作，分别代表：数组里的元素类型、slice1, slice2, slice3。虽然append()语法中，第二个参数不能为slice，但是第二个参数其实是一个可变参数 elems ...Type，可以传输打散的数组，所以go在处理时同样是转换为slice来操作的。 从上面的代码很清楚的看到，如果x.len + y.len 超过了x.cap，那么就会重新扩展新的切片，如果x.len + y.len还没有超过x.cap，则还是在原切片的数组中进行元素的填充。那么这样跟我们理性的认识是一致的。可以打消掉之前误解的对go append的担心。那问题出在哪呢？ 上面忽略了一点，append函数是有go的代码的，不是直接语言级c的实现，在c的实现上还加了go语言自己的处理，在/pkg/builtin/bulitin.go里有函数的定义。这里我只能假设在go的层面对scliceA做了一些隐秘的处理，go如何去调用c的底层实现，我现在还不甚了解，这里也只能分析到这里。以后了解之后再来补充这篇博客，如果有了解的朋友，也非常感激你告诉我。 声明无长度的数组 声明无长度的数组其实就是声明了一个可变数组，也就是slice切片。只不过这个切片的len和cap都是0。这个方法写起来非常方便，如果不了解其背后的实现，那么这样用起来是性能最差的一种。因为会导致频繁的对slice进行重新申请内容的操作，并且需要把，原数组中的元素copy到新的大容量的数组里去。每次重新分配数组容量的步长是len*2，如果进行n次append，那么需要经过log2(n)次的重新申请内存和copy的开销。 后面的一篇文章会继续介绍切片和数组的一些区别: go slice和数组的区别 ","link":"https://phantomma.top/post/go-yu-yan-zhong-de-shu-zu-qie-pian-slice/"},{"title":"go 实现排序的链表","content":"链表的数据结构比较线性数组，优点是 可以方便的对任意的位置进行插入和删除。 这一特性使得它很适合于应用在排序等场景下，由于golang目前类库还不是很完善，在java中可以很简单的使用api提供的支持完成对list或者map的排序，在使用go时就没有那么幸运了，可能需要自己去实现。 下面的例子就是使用go package 中的LinkedList实现的排序的链表。 有几个功能特性： 支持固定的长度 可自定义排序的规则 组合LinkedList功能 package codeforfun import ( &quot;container/list&quot; ) type SortedLinkedList struct { *list.List Limit int compareFunc func (old, new interface{}) bool } func NewSortedLinkedList(limit int, compare func (old, new interface{}) bool) *SortedLinkedList { return &amp;SortedLinkedList{list.New(), limit, compare} } func (this SortedLinkedList) findInsertPlaceElement(value interface{}) *list.Element { for element := this.Front(); element != nil; element = element.Next() { tempValue := element.Value if this.compareFunc(tempValue, value) { return element } } return nil } func (this SortedLinkedList) PutOnTop(value interface{}) { if this.List.Len() == 0 { this.PushFront(value) return } if this.List.Len() &lt; this.Limit &amp;&amp; this.compareFunc(value, this.Back().Value) { this.PushBack(value) return } if this.compareFunc(this.List.Front().Value, value) { this.PushFront(value) } else if this.compareFunc(this.List.Back().Value, value) &amp;&amp; this.compareFunc(value, this.Front().Value) { element := this.findInsertPlaceElement(value) if element != nil { this.InsertBefore(value, element) } } if this.Len() &gt; this.Limit { this.Remove(this.Back()) } } 使用方法： package main import ( &quot;fmt&quot; &quot;codeforfun&quot; ) type WordCount struct { Word string Count int } func compareValue(old, new interface {}) bool { if new.(WordCount).Count &gt; old.(WordCount).Count { return true } return false } func main() { wordCounts := []WordCount{ WordCount{&quot;kate&quot;, 87}, WordCount{&quot;herry&quot;, 92}, WordCount{&quot;james&quot;, 81}} var aSortedLinkedList = codeforfun.NewSortedLinkedList(10, compareValue) for _, wordCount := range wordCounts { aSortedLinkedList.PutOnTop(wordCount) } for element := aSortedLinkedList.List.Front(); element != nil; element = element.Next() { fmt.Println(element.Value.(WordCount)) } } ","link":"https://phantomma.top/post/go-shi-xian-pai-xu-de-lian-biao/"},{"title":"java 函数式编程之 lambda 表达式","content":"作为比较老牌的面向对象的编程语言java，在对函数式编程的支持上一直不温不火。 认为面向对象式编程就应该纯粹的面向对象，于是经常看到这样的写法：如果你想写一个方法，那么就必须把它放到一个类里面，然后new出来对象，对象调用这个方法。 这种方式在函数式编程语言看来太死板，没有必要在对待多种编程范式上采取非此即彼的做法。 如今比较现代的编程语言也都是多编程范式的支持，不再去对一种编程范式固守一隅，一种语言可能会同时具有面向对象、函数式、元编程等多种特性，这方面java的后来者C#都走在她的前面。 终于在jdk8里发现了lambda表达式的影子，java也开始加入这种函数式编程特性，java码农们终于在之前老土的方法之外有了一种更为简便的选择。 首先来看，lambda之前java的做法： 使用匿名内部类： public void testAnonymousClass() { Integer[] nums = {2, 5, 1, 6}; Arrays.sort(nums, new Comparator&lt;Integer&gt;() { @Override public int compare(Integer o1, Integer o2) { if(o1 &lt; o2) return -1; return 0; } }); for (Integer n : nums) { System.out.println(n); } } 函数式编程语言的做法，这里拿go的代码为例： package main import ( &quot;fmt&quot; ) // 插入排序 func sort(nums []int, compare func (a, b int) int) { length := len(nums) for i := length - 1; i &gt;= 0; i-- { for j := i; j + 1 &lt; length; j++ { cur := nums[j] next := nums[j + 1] if compare(cur, next) &gt; 0 { nums[j], nums[j + 1] = next, cur } } } } func main() { nums := []int{2, 5, 1, 6} sort(nums, func(a, b int) int { if a &gt; b { return 1 } return 0 }) fmt.Println(nums) } go的代码看上去比较长，由于没有像java一样使用类库提供的排序算法，所以go自己实现的插入排序。 这里go语言具有函数里面传函数的能力（也叫高阶函数），所以代码看起来简洁了很多。一般这种场景，函数式编程语言使用匿名函数的方式，在java的看来就必须通过匿名内部类来实现。首先实现一个接口，接口里面定义好方法，匿名内部类实现接口，然后在传入的函数中，通过传递的对象，实现对匿名内部类里的方法的回调。这也就是lambda表达式之前的基本做法。 lambda表达式是对java实现函数式编程一个取巧方式的补充，下面来看lambda方式的做法： public void testAnonymousClass() { Integer[] nums = {2, 5, 1, 6}; Arrays.sort(nums, (o1, o2) -&gt; { if(o1 &lt; o2) return -1; return 0; }); for (Integer n : nums) { System.out.println(n); } } 函数式接口：这是java在解决函数式编程，引入lambda表达式的同时引入的一个概念，具体的意思就是，定义的一个接口，接口里面必须有且只有一个方法，这样的接口就成为函数式接口。 在可以使用lambda表达式的地方，方法声明时必须包含一个函数式的接口。任何函数式接口都可以使用lambda表达式替换。 下面来看lambda的基本逻辑： button.onAction(new EventHandler&lt;ActionEvent&gt;() { @Override public void handle(ActionEvent e) { doSomethingWith(e); } }); 使用lambda表达式替换： button.onAction((ActionEvent e) -&gt; { doSomethingWith(e); }); 此lambda表达式的类型可由编译器推断为EventHandler，因为onAction()方法采用的对象类型为 EventHandler。 由于EventHandler只有一个方法即handle()，此lambda表达式必然是handle()方法的实现。 可以继续简化lambda表达式： button.onAction((e) -&gt; { doSomethingWith(e); }); 此lambda表达式的参数必须是ActionEvent，因为其类型是由EventHandler接口的 handle()方法指定的。 因此，我们可以简化此lambda表达式，因为其参数类型可以推断。 还可以继续简化： button.onAction(e -&gt; doSomethingWith(e)); 当lambda表达式只有一个参数且参数类型可以推断时，则不需要括号。 lambda表达式中的代码块只包含一个语句，因此可以去掉大括号和分号。 可以猜测lambda表达式的实现可能是由java编译器在编译java字节码时，会翻译这样的语法糖，最终还是转化为匿名内部类来实现，至少从语义上看来是这样的。那么它究竟怎样做到的，这里的文章可以给出答案： 和Lambdas的第一次亲密接触 采用的办法是在使用lambda表达式的类中生成一个实例方法，那么当然能够访问到这个类中定义的实例变量、静态变量和公开、私有方法。 那和函数式编程相随相生的闭包问题是否支持了呢？ 通过上面的介绍可以看出java对函数式编程的实现，主要还是在编译时对lambda表达式的一些转化。 让人看起来像是支持了匿名函数等函数式编程的特性，其实还是使用java自己的一套实现。所以在使用lambda表达式的时候最好头脑清醒，不要纠结是否闭包了。 以上谈的是jdk8的预览版本，也可能正式版会做很多的改进，那就不得而知了。 相关文章： Java 8为什么需要Lambda表达式 Java 8的Lambda表达式 ","link":"https://phantomma.top/post/java-han-shu-shi-bian-cheng-zhi-lambda-biao-da-shi/"},{"title":"go 语言 channel 的别样用法","content":"1.返回值使用通道 func main() { // 生成随机数作为一个服务 randService := randGenerator() // 从服务中读取随机数并打印 fmt.Printf(&quot;%d\\n&quot;,&lt;-randService) } func randGenerator() chan int { // 创建通道 out := make(chan int) // 创建协程 go func() { for { //向通道内写入数据，如果无人读取会等待 out &lt;- rand.Int() } }() return out } 2.参数使用通道 //一个查询结构体 type query struct { //参数Channel sql chan string //结果Channel result chan string } //执行Query func execQuery(q query) { //启动协程 go func() { //获取输入 sql := &lt;-q.sql //访问数据库，输出结果通道 q.result &lt;- &quot;get&quot; + sql }() } func main() { //初始化Query q := query{make(chan string, 1),make(chan string, 1)} //执行Query，注意执行的时候无需准备参数 execQuery(q) //准备参数 q.sql &lt;- &quot;select * from table&quot; //获取结果 fmt.Println(&lt;-q.result) } 3.并发循环 func doSomething(num int) (sum int) { for i := 1; i &lt;= 10; i++ { fmt.Printf(&quot;%d + %d = %d\\n&quot;, num, num + i, num + num + i) sum = sum + num + i } return sum } func testLoop() { // 建立计数器，通道大小为cpu核数 var NumCPU = runtime.NumCPU() fmt.Printf(&quot;NumCPU = %d\\n&quot;, NumCPU) sem :=make(chan int, NumCPU); //FOR循环体 data := []int{1, 11, 21, 31, 41, 51, 61, 71, 81, 91} for _,v:= range data { //建立协程 go func (v int) { fmt.Printf(&quot;doSomething(%d)...\\n&quot;, v) sum := doSomething(v); //计数 sem &lt;- sum; } (v); } // 等待循环结束 var total int = 0 for i := 0; i &lt; len(data); i++ { temp := &lt;- sem fmt.Printf(&quot;%d &lt;- sem\\n&quot;, temp) total = total + temp } fmt.Printf(&quot;total = %d\\n&quot;, total) } func main() { testLoop() } 4.利用channel计算素数 // Send the sequence 2, 3, 4, ... to channel 'in'. func Generate(ch chan int) { for i := 2; ; i++ { ch&lt;- i // Send 'i' to channel 'in'. } } // Copy the values from channel 'in' to channel 'out', // removing those divisible by 'prime'. func Filter(in chan int, out chan int, prime int) { for { i := &lt;-in // Receive valuefrom 'in'. if i%prime != 0 { out &lt;- i // Send'i' to 'out'. } } } func main() { in := make(chan int) go Generate(in) // Launch Generate goroutine. for i := 0; i &lt; 100; i++ { prime := &lt;-in print(prime, &quot;\\n&quot;) out := make(chan int) go Filter(in, out, prime) in = out } } 5.共享变量的读写 //共享变量有一个读通道和一个写通道组成 type shardedVar struct { reader chan int writer chan int } //共享变量维护协程 func whachdog(v shardedVar) { go func() { //初始值 var value int = 0 for { //监听读写通道，完成服务 select { case value = &lt;-v.writer: case v.reader &lt;-value: } } }() } func main() { //初始化，并开始维护协程 v := shardedVar{make(chan int), make(chan int)} whachdog(v) //读取初始值 fmt.Println(&lt;-v.reader) //写入一个值 v.writer &lt;- 1 //读取新写入的值 fmt.Println(&lt;-v.reader) } ","link":"https://phantomma.top/post/go-yu-yan-channel-de-bie-yang-yong-fa/"},{"title":"树莓派开机启动程序及启动脚本的制作","content":"网上有不少关于如何让Linux自动运行自己编写的脚本或者程序的方法，但是大多数都是把命令写到/etc/rc.local里，这样虽然能够实现随机运行，但是并不够灵活。不能像mysql，apache等服务一样能够使用service命令或者调用init.d下的脚本启动、关闭或者重启进程。例如， service mysql restart service apache2 stop 或者 /etc/init.d/mysql restart /etc/init.d/apache2 stop 因为不同的Linux发行版本，对后台服务的处理方式不大一样，如redhat使用chkconfig来管理开机程序。所以下面的方法以debian类系统为例，如何写一个简单的开机启动脚本。所以，此方法适用于raspbian pi的系统。 以svn为例： 制作开机启动脚本svn_serve #!/bin/sh ### BEGIN INIT INFO # Provides: svn_serve # Required-Start: $remote_fs # Required-Stop: $remote_fs # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: Start or stop the HTTP Proxy. ### END INIT INFO case $1 in start) svnserve -d -r /home/pi/svn_repository ;; stop) killall svnserve ;; *) echo &quot;Usage: $0 (start|stop)&quot; ;; esac 如果不加上面的注释，执行下面步骤3时，update-rc.d会报如下的警告信息 update-rc.d: warning: /etc/init.d/proxy missing LSB information update-rc.d: see &lt;http://wiki.debian.org/LSBInitScripts&gt; 启动关闭服务 sudo service svn_serve start sudo service svn_serve stop 让svn_serve开机启动 sudo update-rc.d svn_serve defaults 取消svn_serve的开机自动启动 sudo update-rc.d -f svn_serve remove ","link":"https://phantomma.top/post/shu-mei-pai-kai-ji-qi-dong-cheng-xu-ji-qi-dong-jiao-ben-de-zhi-zuo/"},{"title":"树莓派折腾手记","content":"OS X下通过下面的命令能够把镜像写入SD unzip 2013-02-09-wheezy-raspbian.zip df -h sudo diskutil unmount /dev/rdisk1s1 sudo dd bs=1m if=2013-02-09-wheezy-raspbian.img of=/dev/rdisk1 sudo diskutil eject /dev/rdisk1 更换了一个更加快速的源 pi的源列表: http://www.raspbian.org/RaspbianMirrors 测试了之后发现这个源在国内更新最快 http://mirror.devunt.kr/raspbian/raspbian/ 更换源：sudo vi /etc/apt/sources.list 更改为： deb http://mirror.devunt.kr/raspbian/raspbian/ wheezy main contrib non-free rpi 更新系统: sudo apt-get update &amp;&amp; sudo apt-get upgrade 更新固件： $ sudo apt-get install git-core $ sudo wget http://goo.gl/1BOfJ -O /usr/bin/rpi-update &amp;&amp; sudo chmod +x /usr/bin/rpi-update $ sudo rpi-update 命令行下显示中文 sudo apt-get install ttf-wqy-zenhei 中文安装完成之后还需要一个输入法： sudo apt-get install scim-pinyin 然后sudo raspi-config选择change_locale， 用空格键选择四个中文字体zh_CN.GB2312, zh_CN.GB18030, zh_CN GBK, zh_CN.UTF-8， 选择zh_CN.UTF-8作为系统环境默认区域设置，然后重启 安装需要的软件 vim sudo apt-get install vim chrome sudo apt-get install chromium-browser chromium-l10n 远程桌面 sudo apt-get install xrdp 通过x2x和Raspberry Pi共享鼠标键盘 http://blog.xming.me/?p=72 安装node.js http://www.jeremymorgan.com/tutorials/raspberry-pi/how-to-install-node-js-raspberry-pi/ 编译Go语言 http://dave.cheney.net/2012/09/25/installing-go-on-the-raspberry-pi 树莓派shell命令 重新配置树莓派：sudo raspi-config 查看cpu温度：cat /sys/class/thermal/thermal_zone0/temp 配置无线网络和固定ip 命令查看USB设备列表 lsusb 扫描无线网络 sudo iwlist wlan0 scan $ sudo vim /etc/wpa.conf network={ ssid=&quot;TP-LINK_B3A8BC&quot; proto=RSN key_mgmt=WPA-PSK pairwise=CCMP TKIP group=CCMP TKIP psk=&quot;password&quot; } $ sudo vim /etc/network/interfaces auto lo iface lo inet loopback iface eth0 inet dhcp allow-hotplug wlan0 iface wlan0 inet manual wpa-roam /etc/wpa.conf address 192.168.1.109 netmask 255.255.255.0 network 192.168.1.0 broastcast 192.168.1.255 gateway 192.168.1.1 iface default inet dhcp $ sudo ifdown wlan0 $ sudo ifup wlan0 $ iwconfig 安装samba sudo apt-get install samba sudo cp /etc/samba/smb.conf /etc/samba/smb.conf.bak sudo vim /etc/samba/smb.conf 配置security = user 配置共享位置，在文件末添加： [media] comment = pi sd card path = /home/pi/samba valid users = @users force group = users create mask = 0660 directory mask = 0771 read only = no 重启samba sudo service samba restart windows下，在运行里面::[树莓派ip]访问， macox在safari里访问 smb://[树莓派ip] 媒体播放 使用方法：在终端用命令打开：omxplayer + 文件名，如果用HDMI接口的音频输出的话要加上-o hdmi参数 Key Action 1 加速 2 减速 j 上一条音轨 k 下一条音轨 i 上一节 o 下一节 n 上一条字幕轨 m 下一条字幕轨 s 显示/不显示字幕 q 退出 空格或p 暂停/继续 - 减小音量 + 增加音量 左 后退30 右 前进30 上 后退600 下 前进600 树莓派的应用 Hacking a Raspberry Pi into a wireless airplay speaker http://jordanburgess.com/post/38986434391/raspberry-pi-airplay 超频的玩法 http://www.oschina.net/translate/how-to-overclock-raspberry-pi http://jade.is-programmer.com/posts/36984.html http://www.memetic.org/raspberry-pi-overclocking/ 启动配置项说明 http://elinux.org/RPiconfig 很强大的玩法，树莓派集群 http://www.southampton.ac.uk/~sjc/raspberrypi/pi_supercomputer_southampton.htm rpi-update更新firmware sudo wget http://goo.gl/1BOfJ -O /usr/bin/rpi-update &amp;&amp; sudo chmod +x /usr/bin/rpi-update sudo apt-get install ca-certificates sudo apt-get install git-core sudo rpi-update sudo reboot 更多参考： http://blog.sina.com.cn/s/blog_3cb6a78c0101a0fe.html http://hi.baidu.com/nonezk/item/e2c82a03683e2c95a3df4344 http://blog.linguofeng.com/archive/2013/04/04/raspberry-pi.html http://www.leiphone.com/raspberry-pi-hands-on.html 在一张SD卡上安装多个系统 http://www.berryterminal.com/doku.php/berryboot#adding_your_own_custom_operating_systems_to_the_menu 使用ssh远程切换berryboot默认系统 http://rvalbuena.blogspot.com/2013/02/changing-berryboot-selected-os-on.html 用树莓派搭建独立博客 使用disqus，增加评论功能 http://disqus.com 可以参考的静态博客网站 在Pi和Github上搭建自己的个人博客 gor: https://github.com/wendal/gor http://defworld.com/2013-05/build-your-blog-with-gor-and-pi.html Jekyll: http://blog.huatai.me/2013/04/18/using-jekyll.html http://www.soimort.org/posts/101/ ruhoh： http://ruhoh.com/ 在路由器上搭建博客 http://blog.huatai.me/2013/01/24/how-this-blog-relive.html 挂载移动硬盘 lsusb查看是否能找到移动硬盘的硬件 ls -l /dev/查看sda开头的设备 首先安装ntfs-3g sudo apt-get install ntfs-3g sudo mount -t ntfs -o utf-8 /dev/sda5 /mnt/sda5 sda5是取决于你的实际情况，a表示第一个硬盘，5表示第5个分区。 -t ntfs以ntfs文件格式挂载 -o utf-8 设置文件编码 开机自动挂载硬盘 把上述的命令写入 /etc/fstab 文件中cat /etc/fstab proc /proc proc defaults 0 0 #Handled by Berryboot #/dev/mmcblk0p1 /boot vfat defaults 0 2 #/dev/mmcblk0p2 / ext4 defaults,noatime 0 1 /dev/sda5 /mnt/sda5 ntfs-3g utf-8,noexec,umask=0000 0 0 开机启动脚本 在/etc/init.d/ 下新建svnserve，新建完成后：sudo chmod +x svnserve，加可执行权限 #!/bin/bash # description: script to start/stop svnserve case $1 in start) svnserve -d -r /home/pi/svn_repository ;; stop) killall svnserve ;; *) echo &quot;Usage: $0 (start|stop)&quot; ;; esac ","link":"https://phantomma.top/post/shu-mei-pai-zhe-teng-shou-ji/"},{"title":"linux 命令","content":"查看系统版本 lsb_release -a // 适用于RedHat、SUSE、Debian cat /etc/redhat-release // 适用于RedHat cat /etc/issue // 用于所有的Linux发行版 查看操作系统内核版本 uname -a 计算tps tail -f lippi_card.log | grep 'cardManager.correctCardExtension' | awk -F ' ' '{print $1}' | awk -F '.' '{print $1}' | uniq -c 汇总多台机器的count $pgm -Ab -f login &quot;cat /home/admin/logs/lippi-login/lippi_login.log.2019-04-29 | grep 'checkPassword mobile:' | awk -F'mobile:' '{print $2}' | awk -F' ' '{print $1}' | sort | uniq -c | wc -l&quot; | grep -v 'SUCCESS' | grep -v ^$ | awk '{sum += $1};END {print sum}' grep 多行 grep -C 5 foo file 显示file文件里匹配foo字串那行以及上下5行 grep多个关键字 grep -E '123|abc' filename // 找出文件（filename）中包含123或者包含abc的行 egrep '123|abc' filename // 用egrep同样可以实现 格式化时间戳 $ date &quot;+%Y-%m-%d %H:%M:%S&quot; -d @1489739011 2017-03-17 01:23:31 安装Arthas curl -L http://start.alibaba-inc.com/install.sh | sh tail -f 多次grep过滤输出 tail -f log | grep --line-buffer xxx | grep yyy 用cat，sort，uniq命令实现文件行的交集 、并集、补集 交集 F1 ∩ F2 cat f1 f2 | sort | uniq -d 并集 F1 ∪ F2 cat f1 f2 | sort | uniq 并集 - 交集 F1∪F2−F1∩F2 cat f1 f2 | sort | uniq -u 补集 F1−F2=(F1∪F2)∪F2 − (F1∪F2)∩F2 cat f1 f2 | sort | uniq | cat f2 - | sort | uniq -u json内容去掉换行，并格式化 $echo -e &quot;$( curl -s -XGET http://11.27.132.160:7001/r/WorkspaceSceneI/getValuePageList -H 'uid:158250@dingding' --data-binary '[&quot;68003&quot;]' )&quot; | tr -d '\\n' | ./jq 用户相关 useradd –d /home/admin -m admin // 此命令创建admin用户和文件夹 useradd admin -m // centOS 7以上，用此命令 passwd admin // 设置用户密码 添加用户到sudoers列表中 chmod +w /etc/sudoers // 添加admin用户到sudoer list 在root ALL=(ALL:ALL) ALL 下面添加一行：admin ALL=(ALL) ALL 压缩、解压缩 zip -e -r jingwei3-all.zip jingwei3-all unzip jingwei3-all.zip -d jingwei3-all tar -zcvf jingwei3-all.tar.gz ./jingwei3-all // 压缩 tar -xvf jingwei3-all.tar.gz // 解压缩 解密加密的邮件 echo Y2FpcnVucWlhbmdAYnl0ZWRhbmNlLmNvbQ== | base64 --decode vim 技巧 :se so=999 // 光标总在屏幕中间 :noh // 取消搜索高亮 解决vim乱码，编辑~/.vimrc 方法一： set fileencodings=utf-8,ucs-bom,gb18030,gbk,gb2312,cp936 set termencoding=utf-8 set encoding=utf-8 方法二： :set enc=utf8 vim粘贴乱码 进入 paste 模式： :set paste 进入 paste 模式后，按 i 键进入插入模式，然后再粘帖，文本格式不会错乱了 结束 paste 模式： :set nopaste 终端快捷键 移动到所在行首：ctrl + a 移动到所在行尾：ctrl + e 清除光标到行首的内容：ctrl + u 清除光标到行尾的内容：ctrl + k 移动到所在单词的词首：esc + b 移动到所在单词的词尾：esc + f 删除前一个单词：crtl + w 清除屏幕内容：ctrl + l 内容提示或补全：tab 其他 jmap -dump:format=b,file=T_FD_62.bin 62414 // dump内存 dstat -nt // 查看网络流量 curl icanhazip.com // 查看外网ip ","link":"https://phantomma.top/post/linux-ming-ling/"}]}