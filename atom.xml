<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://phantomma.top</id>
    <title>幻影笔记</title>
    <updated>2025-06-10T03:29:41.319Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://phantomma.top"/>
    <link rel="self" href="https://phantomma.top/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://phantomma.top/images/avatar.png</logo>
    <icon>https://phantomma.top/favicon.ico</icon>
    <rights>All rights reserved 2025, 幻影笔记</rights>
    <entry>
        <title type="html"><![CDATA[WSL2 上安装 K3S]]></title>
        <id>https://phantomma.top/post/wsl2-shang-an-zhuang-k3s/</id>
        <link href="https://phantomma.top/post/wsl2-shang-an-zhuang-k3s/">
        </link>
        <updated>2023-08-24T07:06:47.000Z</updated>
        <content type="html"><![CDATA[<p>在wsl2 ubuntu-22.04，使用k3s官网的一键安装命令，是安装不成功的。</p>
<pre><code>curl -sfL https://get.k3s.io | sh - 
# Check for Ready node, takes ~30 seconds 
sudo k3s kubectl get node 
</code></pre>
<p>原因，k3s的运行还依赖systemd。在执行上面命令之前，</p>
<h2 id="先要在wsl2-上开启systemd">先要在wsl2 上开启systemd</h2>
<p>在wsl2 上新建文件：<code>/etc/wsl.conf</code>，添加以下内容：</p>
<pre><code>[boot]
systemd=true
</code></pre>
<h2 id="重启wsl2">重启wsl2</h2>
<p>在windows11 上执行<code>wsl --shutdown</code>，然后开启新的wsl，<code>wsl</code>。<br>
在ubuntu上执行命令，验证systemd 的开启效果。<br>
<code>systemctl list-unit-files --type=service</code></p>
<h2 id="重新执行k3s-的一键安装脚本">重新执行k3s 的一键安装脚本</h2>
<p>则会看到安装成功的输出了：</p>
<pre><code>➜  ~ curl -sfL https://get.k3s.io | sh -
# Check for Ready node, takes ~30 seconds
sudo k3s kubectl get node
....
[INFO]  systemd: Starting k3s
NAME            STATUS   ROLES                  AGE   VERSION
nb-hz20314960   Ready    control-plane,master   14m   v1.28.4+k3s2
</code></pre>
<h2 id="免sudo-执行kubectl">免sudo 执行kubectl</h2>
<p>更改的文件权限：<br>
<code>sudo chmod 644 /etc/rancher/k3s/k3s.yaml</code><br>
重启k3s<br>
<code>sudo systemctl restart k3s</code><br>
然后执行 <code>kubectl get node</code> 则不再需要root权限。</p>
<h2 id="得到k3s的kubeconfig">得到k3s的kubeconfig</h2>
<p>接下来可以去这个路径复制 kubeconfig，添加到你操作电脑的kubeconfig里，即可跟WSL2 下的k3s进行交互了。<br>
<code>/etc/rancher/k3s/k3s.yaml</code></p>
<hr>
<p><a href="https://devblogs.microsoft.com/commandline/systemd-support-is-now-available-in-wsl/#set-the-systemd-flag-set-in-your-wsl-distro-settings">Systemd support is now available in WSL!</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Windows11 开启WSL2 和局域网访问]]></title>
        <id>https://phantomma.top/post/windows11-kai-qi-wsl2/</id>
        <link href="https://phantomma.top/post/windows11-kai-qi-wsl2/">
        </link>
        <updated>2023-08-23T03:43:37.000Z</updated>
        <content type="html"><![CDATA[<h2 id="安装wsl2">安装WSL2</h2>
<p>参考这篇文章：<br>
https://zhuanlan.zhihu.com/p/475462241<br>
https://www.sysgeek.cn/windows-11-install-wsl2/</p>
<h2 id="wsl2开启ssh">WSL2开启ssh</h2>
<p><code>vim /etc/ssh/sshd_config</code><br>
修改如下：</p>
<pre><code># Port 22
AddressFamily any
ListenAddress 0.0.0.0
ListenAddress ::
PasswordAuthentication yes
</code></pre>
<p>重启sshd：<code>sudo service ssh restart</code>。<br>
看到重启成功即可，使用ssh登录WSL2。</p>
<h2 id="开机启动">开机启动</h2>
<p>windows11 + WSL2，整个启动过程如下：<br>
<code>Win11开机 &gt;&gt; Win11开机脚本 &gt;&gt; WSL子系统脚本 &gt;&gt; 启动Linux程序</code></p>
<h3 id="在ubuntu下添加开机启动">在ubuntu下添加开机启动</h3>
<p>为WSL2 开启systemd，那么可以使用<code>systemctl service ssh start</code>，这种服务启动的方式了。具体可参考下篇中的开启<code>systemd</code>部分。</p>
<h3 id="常驻后台运行">常驻后台运行</h3>
<p>正常情况，当关闭WSL的console时，实例会自动关闭。这对于我们想长时间把WSL ubuntu当作一台远程vm主机来说，显然是不合适的，需要长时间打开一个终端窗口。可以采用以下方式，在开机时打开一个实例。0表示，它会一直等待输入，而不会休眠。</p>
<pre><code>set ws=wscript.CreateObject(&quot;wscript.shell&quot;)
ws.run &quot;wsl -d Ubuntu-22.04&quot;, 0
</code></pre>
<h3 id="移动到windows开机启动项">移动到Windows开机启动项</h3>
<p>Windows 下 Win+R 输入 <code>shell:startup</code>，打开文件夹，把<code>linux-wsl.vbs</code>，拖入到文件夹中。</p>
<h4 id="故障排除办法">故障排除办法</h4>
<p>如果出现运行不成功，多半是权限问题，可以在cmd窗口运行如下启动命令<br>
<code>wsl -d Ubuntu-22.04</code></p>
<h2 id="局域网访问windows11-下的wsl2">局域网访问Windows11 下的WSL2</h2>
<p>WSL 2 发布了最新版本 2.0.0，这个版本开始，自带支持新的镜像网络，WSL2可以使用宿主机的IP，开启的端口直接打通到宿主机。这样可以通过宿主机IP + Port访问WSL2 虚拟机里的服务。<br>
从而不用折腾桥接网络下，在windows上端口转发，开启防火墙等复杂操作。而且，ip经常跳的问题，导致没法用。绑定localhost，也要解决几个奇怪的报错。<br>
现在有了镜像网络，前几项的折腾就可以省去了。</p>
<p>更新 WSL：<code>wsl --update --pre-release</code></p>
<pre><code>[experimental]
networkingMode=mirrored # 镜像网络模式
dnsTunneling=true
firewall=true
autoProxy=true
</code></pre>
<p>重启wsl2，就可以使用宿主机windows11 的ip，通过ssh，来访问内部虚拟机WSL2 ubuntu了。</p>
<hr>
<ul>
<li><a href="https://www.v2ex.com/t/975098">WSL2 今天史诗级更新</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/593263088">WSL2 网络的最终解决方案</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[vagrant 一键拉起 github action runner]]></title>
        <id>https://phantomma.top/post/vagrant-yi-jian-la-qi-github-action-runner/</id>
        <link href="https://phantomma.top/post/vagrant-yi-jian-la-qi-github-action-runner/">
        </link>
        <updated>2023-03-06T10:48:56.000Z</updated>
        <content type="html"><![CDATA[<p>一个通用的 github runner 定义。可按需定制为自己想要的</p>
<pre><code># hostname
$host_name = &quot;XXX&quot;
$host_ip = &quot;XXX&quot;
$runner_token = &quot;XXX&quot;

Vagrant.configure(&quot;2&quot;) do |config|
  
  config.vm.box = &quot;ubuntu/jammy64&quot;
  config.vm.box_url = &quot;https://mirrors.tuna.tsinghua.edu.cn/ubuntu-cloud-images/jammy/current/jammy-server-cloudimg-amd64-vagrant.box&quot;

  config.vm.hostname = $host_name
  config.vm.network &quot;private_network&quot;, ip: $host_ip

  # spec config
  config.vm.provider :virtualbox do |vbox|
    vbox.name    = $host_name
    vbox.cpus    = 4
    vbox.memory  = 8000
  end

  # init shell, run as root
  config.vm.provision &quot;shell&quot;, path: &quot;run_as_root.sh&quot;
  # init shell, run as user
  config.vm.provision &quot;shell&quot;, privileged: false, path: &quot;run_as_user.sh&quot;, args: [$runner_token]
end
</code></pre>
<p>以root身份执行的动作：</p>
<pre><code># 启用ssh密码认证
echo &quot;[Step 1] Enable ssh password authentication&quot;
sed -i 's/^PasswordAuthentication .*/PasswordAuthentication yes/' /etc/ssh/sshd_config
echo 'PermitRootLogin yes' &gt;&gt; /etc/ssh/sshd_config
systemctl reload sshd

# ssh以root用户登陆，需要重置root密码
echo &quot;[Step 2] change root password&quot;
echo &quot;root:123456&quot; | sudo chpasswd

# apt install apps
echo &quot;[Step 3] apt install apps&quot;
sed -i '1i deb http://cn.archive.ubuntu.com/ubuntu/ jammy-backports main restricted universe multiverse' /etc/apt/sources.list
sed -i '1i deb http://cn.archive.ubuntu.com/ubuntu/ jammy-updates multiverse' /etc/apt/sources.list
sed -i '1i deb http://cn.archive.ubuntu.com/ubuntu/ jammy multiverse' /etc/apt/sources.list
sed -i '1i deb http://cn.archive.ubuntu.com/ubuntu/ jammy-updates universe' /etc/apt/sources.list
sed -i '1i deb http://cn.archive.ubuntu.com/ubuntu/ jammy universe' /etc/apt/sources.list
sed -i '1i deb http://cn.archive.ubuntu.com/ubuntu/ jammy-updates main restricted' /etc/apt/sources.list
sed -i '1i deb http://cn.archive.ubuntu.com/ubuntu/ jammy main restricted' /etc/apt/sources.list
apt-get update
apt-get install -y \
    ca-certificates \
    curl \
    gnupg \
    lsb-release \
    mysql-client
mkdir -m 0755 -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg
echo \
  &quot;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable&quot; | tee /etc/apt/sources.list.d/docker.list &gt; /dev/null
apt-get update
apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

# add docker registry mirror
# echo &quot;[Step 4] add docker registry mirror&quot;
tee /etc/docker/daemon.json &lt;&lt;-EOF
{
    &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;,&quot;http://hub-mirror.c.163.com&quot;]
}
EOF

# docker cmd need no root
echo &quot;[Step 5] docker cmd need no root&quot;
usermod -aG docker vagrant
</code></pre>
<p>以普通用户身份执行的动作：</p>
<pre><code># install and config github action runner
echo &quot;[Step 6] install and config github action runner&quot;
mkdir actions-runner &amp;&amp; cd actions-runner
curl -o ./actions-runner-linux-x64-2.302.1.tar.gz -L https://github.com/actions/runner/releases/download/v2.302.1/actions-runner-linux-x64-2.302.1.tar.gz
tar xzf ./actions-runner-linux-x64-2.302.1.tar.gz
./config.sh --url https://github.com/AutoMQ --token $1 --unattended
nohup ./run.sh &gt; runner.log 2&gt;&amp;1 &amp;
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Debezium outbox pattern]]></title>
        <id>https://phantomma.top/post/debezium-outbox-pattern/</id>
        <link href="https://phantomma.top/post/debezium-outbox-pattern/">
        </link>
        <updated>2023-02-16T08:07:12.000Z</updated>
        <content type="html"><![CDATA[<h2 id="自部署的mysql上开启binlog">自部署的mysql上开启binlog</h2>
<ol>
<li>查看mysql cnf文件所在位置</li>
</ol>
<pre><code>mysql --help
Default options are read from the following files in the given order:
/etc/my.cnf /etc/mysql/my.cnf /opt/homebrew/etc/my.cnf ~/.my.cnf
</code></pre>
<ol start="2">
<li>修改cnf文件，增加如下内容：</li>
</ol>
<pre><code>server-id         = 1
log_bin           = /tmp/mysql-bin
binlog_format     = ROW
binlog_row_image  = FULL
expire_logs_days  = 1
</code></pre>
<ol start="3">
<li>重启mysql服务</li>
</ol>
<h2 id="在aws-rds-for-mysql上开启binlog">在aws rds for mysql上开启binlog</h2>
<ol>
<li>rds dashboard上创建新的参数组，更改binlog_format为ROW</li>
</ol>
<p>确认数据库是否已开启binlog，再次运行命令：</p>
<pre><code>show variables like 'log_bin';
show variables like 'binlog_format';
</code></pre>
<ol start="2">
<li>
<p>修改rds示例配置。修改参数组为上面新建的；修改备份保留期为1天</p>
</li>
<li>
<p>设置binlog保存时长为24h</p>
</li>
</ol>
<pre><code>call mysql.rds_set_configuration('binlog retention hours', 24);
call mysql.rds_show_configuration;
</code></pre>
<ol start="4">
<li>重启实例</li>
</ol>
<hr>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[使用OrbStack 容器里安装常用软件]]></title>
        <id>https://phantomma.top/post/shi-yong-orbstack-an-zhuang-chang-yong-yi-lai/</id>
        <link href="https://phantomma.top/post/shi-yong-orbstack-an-zhuang-chang-yong-yi-lai/">
        </link>
        <updated>2022-12-22T15:28:48.000Z</updated>
        <content type="html"><![CDATA[<h2 id="安装-orbstack">安装 OrbStack</h2>
<p>OrbStack is a fast, light, and simple way to run Docker containers and Linux machines on macOS. You can think of it as a supercharged WSL and Docker Desktop replacement, all in one easy-to-use app.<br>
<img src="https://phantomma.top/post-images/1703691690612.png" alt="" loading="lazy"></p>
<h2 id="安装mysql">安装MySQL</h2>
<h3 id="拉取镜像">拉取镜像</h3>
<pre><code>docker pull mysql    //拉取镜像
docker images    //查看本地镜像
</code></pre>
<h3 id="创建数据卷">创建数据卷</h3>
<p>将其配置和数据等等挂载到数据卷以持久化到宿主机。<br>
创建三个数据卷，分别用于挂载并持久化MySQL的 数据文件、配置文件 和 日志文件 ：</p>
<pre><code>docker volume create mysql-data
docker volume create mysql-config
docker volume create mysql-log
</code></pre>
<h3 id="创建并运行容器">创建并运行容器</h3>
<pre><code>docker run -id --name=mysql -v mysql-config:/etc/mysql/conf.d \
-v mysql-log:/logs \
-v mysql-data:/var/lib/mysql \
-p 3307:3306 \
-e MYSQL_ROOT_PASSWORD=xxx \
-e LANG=C.UTF-8 mysql
</code></pre>
<ul>
<li>-id 将MySQL容器挂在后台运行</li>
<li>--name=mysql 将容器起名为 mysql</li>
<li>-v mysql-config:/etc/mysql/conf.d 把MySQL容器中的配置文件目录挂载至上述创建的名为mysql-config的数据卷上面，其他两个 -v 挂载数据卷的参数同理</li>
<li>-p 3306:3306 将主机的 3306 端口映射到容器的 3306</li>
<li>-e MYSQL_ROOT_PASSWORD=xxx 设置 root 用户的密码</li>
<li>-e LANG=C.UTF-8 设置容器的语言环境变量 LANG 值为 C.UTF-8</li>
</ul>
<h3 id="通过容器访问mysql">通过容器访问MySQL</h3>
<pre><code>docker exec -it mysql bash
mysql -uroot -p
</code></pre>
<p>在容器外访问：</p>
<pre><code>mysql -u root -pxxx -hmysql.orb.local
</code></pre>
<h2 id="安装redis">安装redis</h2>
<h3 id="拉取镜像-2">拉取镜像</h3>
<pre><code>docker pull redis
</code></pre>
<h3 id="创建数据卷-2">创建数据卷</h3>
<pre><code>docker volume create redis-config
docker volume create redis-data
</code></pre>
<h3 id="修改配置文件">修改配置文件</h3>
<pre><code>cd ~/OrbStack/docker/volumes/redis-config
vim redis.conf

# 启动redis持久化功能
appendonly yes
# 设置密码
requirepass xxx
# 指定数据存储位置
dir /data
</code></pre>
<h3 id="创建容器">创建容器</h3>
<pre><code>docker run -id --name=redis \
    -v redis-config:/usr/local/etc/redis \
    -v redis-data:/data -p 6379:6379 \
    -e LANG=C.UTF-8 redis \ 
    redis-server /usr/local/etc/redis/redis.conf
    # su -l root -c &quot;redis-server /usr/local/etc/redis/redis.conf&quot;
</code></pre>
<ul>
<li>--name redis 指定容器名字</li>
<li>-v 指定数据卷，可见将容器配置文件夹/usr/local/etc/redis挂载至了数据卷redis-config，将容器 内/data挂载至数据卷redis-data，可见这里挂载数据卷的容器内路径和我们上述预先写的配置文件中对应的路径是要一致的</li>
<li>-p 6379:6379 端口映射</li>
<li>-e 用于指定容器内环境变量，设置容器的语言环境变量LANG值为C.UTF-8，这个最好是要设置，否则容器内默认是英文环境，使得Redis可能无法存放中文内容</li>
<li>su -l root -c &quot;redis-server /usr/local/etc/redis/redis.conf&quot; 在容器内以root身份运行redis-server并指定了配置文件位置</li>
</ul>
<h3 id="通过容器执行redis命令">通过容器执行redis命令</h3>
<ul>
<li>运行redis <code>docker start redis</code></li>
<li>查看redis运行状态 <code>docker ps | grep redis</code></li>
<li>进入redis容器内部 <code>docker exec -it redis bash</code></li>
<li>进入Redis控制台 <code>redis-cli</code></li>
</ul>
<h2 id="mongodb">MongoDB</h2>
<h3 id="安装">安装</h3>
<pre><code>docker pull mongo
docker run --name mongo -v mongo-data:/data/db -v mongo-config:/data/configdb --privileged -p 27017:27017 -e MONGO_INITDB_ROOT_USERNAME=admin -e MONGO_INITDB_ROOT_PASSWORD=123456 -d mongo --auth
</code></pre>
<ul>
<li>-name 指定容器名称</li>
<li>-v 指定数据存储位置</li>
<li>--privileged root权限</li>
<li>-p 端口映射</li>
<li>-d 后台运行</li>
<li>-auth 需要认证，默认mongo是不需要认证的</li>
<li>-e MONGO_INITDB_ROOT_USERNAME=admin 指定用户名</li>
<li>-e MONGO_INITDB_ROOT_PASSWORD=123456 指定密码</li>
</ul>
<h3 id="添加用户">添加用户</h3>
<p>进入容器 <code>docker exec -it mongo mongosh admin</code><br>
添加用户 <code>db.createUser({user:'admin',pwd:'123456',roles[{role:'userAdminAnyDatabase', db: 'admin'},&quot;readWriteAnyDatabase&quot;]})</code></p>
<h2 id="kafka">kafka</h2>
<h3 id="安装kafka">安装kafka</h3>
<p>最新Kafka 使用内置的 Raft 组件来管理主题和分区，因此我们不用额外安装ZooKeeper，可以直接安装 Kafka</p>
<ul>
<li>拉取镜像 <code>docker pull bitnami/kafka:latest</code></li>
<li>创建网络 <code>docker network create kafka-net</code></li>
<li>启动Kafka服务实例</li>
</ul>
<pre><code>docker run -d --name kafka \
    --network kafka-net \
    -p 9092:9092 \
    -e ALLOW_PLAINTEXT_LISTENER=yes \
    bitnami/kafka:latest
</code></pre>
<h3 id="测试kafka生产者和消费者">测试Kafka生产者和消费者</h3>
<pre><code>docker exec -it kafka /bin/b
cd opt/bitnami/kafka/bin/
</code></pre>
<p>开启两个终端</p>
<ul>
<li>运行生产者发送消息 <code>./kafka-console-producer.sh --broker-list localhost:9092 --topic test-topic</code></li>
<li>运行消费者接受消息 <code>./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic --from-beginning</code></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[在 Kubernetes 上运行高可用的 Kafka 集群]]></title>
        <id>https://phantomma.top/post/zai-kubernetes-shang-yun-xing-gao-ke-yong-de-kafka-ji-qun/</id>
        <link href="https://phantomma.top/post/zai-kubernetes-shang-yun-xing-gao-ke-yong-de-kafka-ji-qun/">
        </link>
        <updated>2020-12-21T13:28:30.000Z</updated>
        <content type="html"><![CDATA[<p>Apache Kafka 是目前最流行的分布式消息发布订阅系统，虽然 Kafka 非常强大，但它同样复杂，需要一个高可用的强大平台来运行。在微服务盛行，大多数公司都采用分布式计算的今天，将 Kafka 作为核心的消息系统使用还是非常有优势的。</p>
<p>如果你在 Kubernetes 集群中运行你的微服务，那么在 Kubernetes 中运行 Kafka 集群也是很有意义的，这样可以利用其内置的弹性和高可用，我们可以使用内置的 Kubernetes 服务发现轻松地与集群内的 Kafka Pods 进行交互。</p>
<p>下面我们将来介绍下如何在 Kubernetes 上构建分布式的 Kafka 集群，这里我们将使用 Helm Chart 和 StatefulSet 来进行部署，当然如果想要动态生成持久化数据卷，还需要提前配置一个 StorageClass 资源，比如基于 Ceph RBD 的，如果你集群中没有配置动态卷，则需要提前创建 3 个未绑定的 PV 用于数据持久化。</p>
<p>当前基于 Helm 官方仓库的 chartincubator/kafka 在 Kubernetes 上部署的 Kafka，使用的镜像是 confluentinc/cp-kafka:5.0.1，即部署的是 Confluent 公司提供的 Kafka 版本，Confluent Platform Kafka(简称 CP Kafka)提供了一些 Apache Kafka 没有的高级特性，例如跨数据中心备份、Schema 注册中心以及集群监控工具等。</p>
<h1 id="安装">安装</h1>
<p>使用 Helm Chart 安装当然前提要安装 Helm，直接使用最新版本的 Helm v3 版本即可：</p>
<pre><code>&gt; wget https://get.helm.sh/helm-v3.4.0-linux-amd64.tar.gz
&gt; tar -zxvf helm-v3.4.0-linux-amd64.tar.gz
&gt; sudo cp -a linux-amd64/helm /usr/local/bin/helm
&gt; chmod +x /usr/local/bin/helm
</code></pre>
<p>然后添加 Kafka 的 Chart 仓库：</p>
<pre><code>&gt; helm repo add incubator http://mirror.azure.cn/kubernetes/charts-incubator/
&gt; helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the &quot;incubator&quot; chart repository
...Successfully got an update from the &quot;stable&quot; chart repository
Update Complete. ⎈Happy Helming!⎈
</code></pre>
<p>接着我们就可以配置需要安装的 Values 文件了，可以直接使用默认的 values.yaml 文件，然后可以用它来进行定制，比如指定我们自己的 StorageClass：</p>
<pre><code>&gt; curl https://raw.githubusercontent.com/helm/charts/master/incubator/kafka/values.yaml &gt; kfk-values.yaml
</code></pre>
<p>这里我直接使用默认的进行安装：</p>
<pre><code>&gt; helm install kafka incubator/kafka -f kfk-values.yaml
NAME: kafka
LAST DEPLOYED: Sun Nov  1 09:36:44 2020
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
### Connecting to Kafka from inside Kubernetes

You can connect to Kafka by running a simple pod in the K8s cluster like this with a configuration like this:

  apiVersion: v1
  kind: Pod
  metadata:
    name: testclient
    namespace: default
  spec:
    containers:
    - name: kafka
      image: confluentinc/cp-kafka:5.0.1
      command:
        - sh
        - -c
        - &quot;exec tail -f /dev/null&quot;

Once you have the testclient pod above running, you can list all kafka
topics with:

  kubectl -n default exec testclient -- ./bin/kafka-topics.sh --zookeeper kafka-zookeeper:2181 --list

To create a new topic:

  kubectl -n default exec testclient -- ./bin/kafka-topics.sh --zookeeper kafka-zookeeper:2181 --topic test1 --create --partitions 1 --replication-factor 1

To listen for messages on a topic:

  kubectl -n default exec -ti testclient -- ./bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic test1 --from-beginning

To stop the listener session above press: Ctrl+C

To start an interactive message producer session:
  kubectl -n default exec -ti testclient -- ./bin/kafka-console-producer.sh --broker-list kafka-headless:9092 --topic test1

To create a message in the above session, simply type the message and press &quot;enter&quot;
To end the producer session try: Ctrl+C

If you specify &quot;zookeeper.connect&quot; in configurationOverrides, please replace &quot;kafka-zookeeper:2181&quot; with the value of &quot;zookeeper.connect&quot;, or you will get error.
</code></pre>
<p>如果你没配置 StorageClass 或者可用的 PV，安装的时候 kafka 的 Pod 会处于 Pending 状态，所以一定要提前配置好数据卷。</p>
<p>正常情况隔一会儿 Kafka 就可以安装成功了：</p>
<pre><code>&gt; kubectl get pods
NAME                READY   STATUS    RESTARTS   AGE
kafka-0             1/1     Running   0          25m
kafka-1             1/1     Running   0          11m
kafka-2             1/1     Running   0          2m
kafka-zookeeper-0   1/1     Running   0          25m
kafka-zookeeper-1   1/1     Running   0          22m
kafka-zookeeper-2   1/1     Running   0          18m
</code></pre>
<p>默认会安装 3 个 ZK Pods 和 3 个 Kafka Pods，这样可以保证应用的高可用，也可以看下我配置的持久卷信息：</p>
<pre><code>&gt; kubectl get pvc
NAME              STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
datadir-kafka-0   Bound    kfk0     1Gi        RWO                           28m
datadir-kafka-1   Bound    kfk1     1Gi        RWO                           13m
datadir-kafka-2   Bound    kfk2     1Gi        RWO                           4m9s
&gt; kubectl get pv
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                     STORAGECLASS   REASON   AGE
kfk0   1Gi        RWO            Retain           Bound    default/datadir-kafka-0                           23m
kfk1   1Gi        RWO            Retain           Bound    default/datadir-kafka-1                           22m
kfk2   1Gi        RWO            Retain           Bound    default/datadir-kafka-2                           10m
</code></pre>
<p>如果我们配置一个 default 的 StorageClass，则会动态去申请持久化卷，如果你的集群没有启用动态卷，可以修改 values.yaml 来使用静态卷。</p>
<p>然后查看下对应的 Service 对象：</p>
<pre><code>&gt; kubectl get svc
NAME                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
kafka                      ClusterIP   10.100.205.187   &lt;none&gt;        9092/TCP                     31m
kafka-headless             ClusterIP   None             &lt;none&gt;        9092/TCP                     31m
kafka-zookeeper            ClusterIP   10.100.230.255   &lt;none&gt;        2181/TCP                     31m
kafka-zookeeper-headless   ClusterIP   None             &lt;none&gt;        2181/TCP,3888/TCP,2888/TCP   31m
kubernetes                 ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP                      14d
</code></pre>
<p>可以看到又一个叫 kafka-zookeeper 的 zookeeper 服务和一个叫 kafka 的 Kafka 服务，对于 Kafka 集群的管理，我们将与 kafka-zookeeper 服务进行交互，对于集群消息的收发，我们将使用 kafka 服务。</p>
<h1 id="客户端测试">客户端测试</h1>
<p>现在 Kafka 集群已经搭建好了，接下来我们来安装一个 Kafka 客户端，用它来帮助我们产生和获取 topics 消息。</p>
<p>直接使用下面的命令创建客户端：</p>
<pre><code>&gt; cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: testclient
  namespace: default
spec:
  containers:
  - name: kafka
    image: confluentinc/cp-kafka:5.0.1
    command:
      - sh
      - -c
      - &quot;exec tail -f /dev/null&quot;
EOF
&gt; kubectl get pod testclient
NAME         READY   STATUS    RESTARTS   AGE
testclient   1/1     Running   0          23s
</code></pre>
<p>客户端 Pod 创建成功后我们就可以开始进行一些简单的测试了。首先让我们创建一个名为 test1 的有一个分区和复制因子'1'的 topic：</p>
<pre><code>&gt; kubectl exec -it testclient -- /usr/bin/kafka-topics --zookeeper kafka-zookeeper:2181 --topic test1 --create --partitions 1 --replication-factor 1
Created topic &quot;test1&quot;.
</code></pre>
<p>然后创建一个生产者，将消息发布到这个 topic 主题上：</p>
<pre><code>&gt; kubectl  exec -ti testclient -- /usr/bin/kafka-console-producer --broker-list kafka:9092 --topic test1
</code></pre>
<p>然后重新打一个终端页面，让我们打开一个消费者会话，这样我们就可以看到我们发送的消息了。</p>
<pre><code>&gt; kubectl exec -ti testclient -- /usr/bin/kafka-console-consumer --bootstrap-server kafka:9092 --topic test1
</code></pre>
<p>现在我们在生产者的窗口发送消息，在上面的消费者会话窗口中就可以看到对应的消息了：<br>
<img src="https://phantomma.top/post-images/1701437469791.webp" alt="" loading="lazy"></p>
<p>到这里证明 Kafka 集群就正常工作了。比如需要注意 zk 集群我们并没有做持久化，如果是生产环境一定记得做下数据持久化，在 values.yaml 文件中根据需求进行定制即可，当然对于生产环境还是推荐使用 Operator 来搭建 Kafka 集群，比如 <code>strimzi-kafka-operator</code>。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Helm Chart 模板技巧]]></title>
        <id>https://phantomma.top/post/helm-chart-mo-ban-kai-fa-ji-qiao/</id>
        <link href="https://phantomma.top/post/helm-chart-mo-ban-kai-fa-ji-qiao/">
        </link>
        <updated>2020-10-11T10:19:43.000Z</updated>
        <content type="html"><![CDATA[<p>Helm Chart 在我们使用的时候非常方便的，但是对于开发人员来说 Helm Chart 模板就并不一定显得那么友好了，本文主要介绍了 Helm Chart 模板开发人员在构建生产级的 Chart 包时的一些技巧和窍门。</p>
<h1 id="了解你的模板功能">了解你的模板功能</h1>
<p>Helm 使用Go Template来模板化资源文件。在 Go 提供的内置函数基础上，还添加了许多其他功能。</p>
<p>首先，添加了Sprig 库中的几乎所有函数，出于安全原因，删除了两个函数：env和expandenv（这会让 Chart 模板开发者访问到 Tiller 的环境）。</p>
<p>另外还添加了两个特殊的模板函数：include和required，include函数允许你引入另一个模板，然后将结果传递给其他模板函数。</p>
<p>例如，下面的模板片段中引用了一个名为mytpl的模板，然后将结果转成小写，并用双引号包装起来：</p>
<pre><code>value: {{ include &quot;mytpl&quot; . | lower | quote }}
</code></pre>
<p>required函数允许你根据模板的需要声明特定的值，如果值为空，则默认渲染的时候会报错。下面的这个示例被声明为 .Values.who 是必须的，为空的时候会打印出一段错误提示信息：</p>
<pre><code>value: {{required &quot;A valid .Values.who entry required!&quot; .Values.who }}
</code></pre>
<h1 id="引用字符串不要引用整数">引用字符串，不要引用整数</h1>
<p>当你使用字符串数据的时候，为了安全考虑应该总是使用字符串而不是直接暴露出来：露：</p>
<pre><code>name: {{ .Values.MyName | quote }}
</code></pre>
<p>当使用整数时，不要直接引用这些值，在很多情况下，可能会导致 Kubernetes 内部的解析错误。</p>
<pre><code>port: {{ .Values.Port }}
</code></pre>
<h1 id="使用-include-功能">使用 include 功能</h1>
<p>Go 提供了一种使用内置template指令将一个模板包含在另外一个模板中的方法。但是，内置函数不能用于 Go 模板管道。为了能够包含模板，然后对该模板的输出执行操作，Helm 提供了特殊的include功能：</p>
<pre><code>{{ include &quot;toYaml&quot; $value | indent 2}}
</code></pre>
<p>上面包含一个名为toYaml的模板，然后将值$value传递给模板，最后将该模板的输出传递给indent函数。</p>
<p>由于 YAML 对于缩进级别和空格的重要性，所以这是包含代码片段的一种很好的方法，但是需要在相关的上下文中处理缩进。</p>
<h1 id="使用-tpl-函数">使用 tpl 函数</h1>
<p>tpl函数运行允许开发人员将字符串计算为模板内的模板，这对于将模板字符串作为值传递给 Chart 或者呈现外部配置文件很有用：{{ tpl TEMPLATE_STRING VALUES }}</p>
<p>例子：</p>
<pre><code># values
template: &quot;{{ .Values.name }}&quot;
name: &quot;Tom&quot;

# template
{{ tpl .Values.template . }}

# output
Tom
</code></pre>
<p>渲染外部配置文件：</p>
<pre><code># external configuration file conf/app.conf
firstName={{ .Values.firstName }}
lastName={{ .Values.lastName }}

# values
firstName: Peter
lastName: Parker

# template
{{ tpl (.Files.Get &quot;conf/app.conf&quot;) . }}

# output
firstName=Peter
lastName=Parker
</code></pre>
<h1 id="创建-imagepullsecret">创建 imagePullSecret</h1>
<p>imagePullSecret基本上是registry、用户名和密码的组合，我们在使用私有仓库的时候需要使用到，需要用base64对这些数据进行编码，我们可以编写一个模板来生成这个配置文件：</p>
<p>首先，假设我们在values.yaml中定义如下：</p>
<pre><code>imageCredentials:
  registry: quay.io
  username: someone
  password: sillyness
</code></pre>
<p>然后我们可以这样来定义模板：</p>
<pre><code>{{- define &quot;imagePullSecret&quot; }}
{{- printf &quot;{\&quot;auths\&quot;: {\&quot;%s\&quot;: {\&quot;auth\&quot;: \&quot;%s\&quot;}}}&quot; .Values.imageCredentials.registry (printf &quot;%s:%s&quot; .Values.imageCredentials.username .Values.imageCredentials.password | b64enc) | b64enc }}
{{- end }}
</code></pre>
<p>最后，我们在 Secret 模板中使用上面定义的模板来创建对象：</p>
<pre><code>apiVersion: v1
kind: Secret
metadata:
  name: myregistrykey
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: { { template &quot;imagePullSecret&quot; . } }
</code></pre>
<h1 id="configmap-或者-secret-更改时自动更新">ConfigMap 或者 Secret 更改时自动更新</h1>
<p>ConfigMap 或者 Secret 通常作为配置文件注入到容器中，如果后面使用helm upgrade来升级更新这些应用程序，则可能需要重新启动，但如果部署的资源清单数据没有改变则应用程序还会继续使用旧的配置，从而导致部署不一致。</p>
<p>sha256sum函数可用于确保在另一个文件更改时更新部署的 annotations 部分：</p>
<pre><code>kind: Deployment
spec:
  template:
    metadata:
      annotations:
        checksum/config: {{ include (print $.Template.BasePath &quot;/configmap.yaml&quot;) . | sha256sum }}
[...]
</code></pre>
<p>更多的信息我们可以查看 <code>helm upgrade --recreate-pods</code> 命令来了解这个问题的其他信息。</p>
<h1 id="告诉-tiller-不要删除资源">告诉 Tiller 不要删除资源</h1>
<p>有的时候在运行helm delete命令后有些资源不应该被删除。Chart 开发者可以在资源对象中添加一个 annotation 来保护资源不被删除：</p>
<pre><code>kind: Secret
metadata:
  annotations:
    &quot;helm.sh/resource-policy&quot;: keep
[...]
</code></pre>
<blockquote>
<p>注意引号是必须的</p>
</blockquote>
<p><code>&quot;helm.sh/resource-policy&quot;: keep</code> 这个 annotation 用来指示 Tiller 在删除一个 realease 的时候跳过当前这个资源。但是需要注意的是，这样这个资源就变成了孤儿，Helm 将不会再管理它了，如果在已经删除但是仍然还保留了部分资源的 realese 上面使用 <code>helm install --replace</code> 命令可能就会出现问题了。</p>
<h1 id="使用partials">使用Partials</h1>
<p>有时候可能你想要在 Chart 中创建一些可重复使用的片段，无论是一块还是模板的一部分，通常将它们保存在自己的文件中会更清晰。</p>
<p>在templates/目录下面，任何以下划线(_)开头的文件都不会被输出到 Kubernetes 资源清单文件中去，按照惯例，帮助模板一般放在_helpers.tpl文件中。</p>
<h1 id="有需要依赖的复杂-chart">有需要依赖的复杂 Chart</h1>
<p>官方的 Chart 仓库中有许多 Chart 都是用于创建更加高级的应用程序的“构建块”。但是 Chart 也可以用于创建大型应用程序。在这种情况下，单个 Chart 可能需要包含多个子 Chart，每个子 Chart 作为整体的一部分。</p>
<p>对于复杂的应用程序当前最佳的实践方式是创建一个顶级的 Chart，然后使用charts子目录嵌入每个组件。</p>
<p>下面是两个复杂的项目使用案例：</p>
<ul>
<li>**SAP 的 OpenStack Chart：**这个 Chart 包用于在 Kubernetes 上安装一套完整的OpenStack IaaS系统，所有的 Charts 包都在这个 Github 仓库中：openstack-helm</li>
<li>**Deis 的 Workflow：**这个 Chart 包使用一个 Chart 安装整个 Deis PaaS 系统，但是它与SAP Chart的不同之处在于，每个子 Chart 都是独立的，都在不同的 Git 仓库中进行托管的，查看requirements.yaml文件，可以了解该 Chart 是如何通过他的 CI/CD pipeline 构建的。仓库地址：(Workflow)[https://github.com/deis/workflow/tree/master/charts/workflow]</li>
</ul>
<p>这两个 Chart 都说明了使用 Helm 构建复杂环境是很成熟的技术。</p>
<h1 id="yaml-是-json-的超集">YAML 是 JSON 的超集</h1>
<p>根据 YAML 的规范，YAML 是 JSON 的超集，这意味着任何有效的 JSON 结构在 YAML 中都是有效的。</p>
<p>所以有时候可能我们去使用 JSON 的语法来表达数据结构更容易，而不是去处理 YAML 的空白。</p>
<p>当然作为最佳实践，模板应该遵循 YAML 的语法，除非 JSON 语法大大降低了格式化问题的风险。</p>
<h1 id="小心生成随机值">小心生成随机值</h1>
<p>Helm 中有一些功能运行你生成随机数据，加密密钥等，但需要注意的是，在升级过程中，会重新执行模板渲染，当模板运行生成的数据与上次不一致时，会触发该资源的更新。</p>
<h1 id="系统的升级版本">系统的升级版本</h1>
<p>在安装和升级版本时使用相同的命令：</p>
<pre><code>helm upgrade --install &lt;release name&gt; --values &lt;values file&gt; &lt;chart directory&gt;
</code></pre>
<hr>
<p>相关链接<br>
https://github.com/technosophos/k8s-helm/blob/master/docs/charts_tips_and_tricks.md<br>
https://github.com/sapcc/helm-charts<br>
https://github.com/deis/workflow/tree/master/charts/workflow<br>
https://godoc.org/text/template<br>
https://godoc.org/github.com/Masterminds/sprig</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[使用 kotlin + jdbcTemplate 写 dao 层]]></title>
        <id>https://phantomma.top/post/shi-yong-kotlin-jdbctemplate-xie-dao-ceng/</id>
        <link href="https://phantomma.top/post/shi-yong-kotlin-jdbctemplate-xie-dao-ceng/">
        </link>
        <updated>2019-01-10T12:55:05.000Z</updated>
        <content type="html"><![CDATA[<h3 id="类orm的方案">类orm的方案</h3>
<p>编写dal层代码，java领域比较火的两个阵营：JPA(Hibernate)和MyBatis。JPA是java制定的一种ORM规范；MyBatis较轻量，还需要开发人员理解sql。<br>
国内的情况，mybatis会更流行一些。能够胜出的点，大概是因为：可定制性更强，更易于进行性能优化。<br>
但是，mybatis用久了，需要不断修改xml，又引起了很多人的不爽。随后基于mybatis之上的工具，如mybatis-plus，tk.mybatis便被提出来。使用lambda的函数，替代动态xml模板。<br>
同时，采用类似做法的也有很多其他语言的框架，比如kotlin的Exposed、ktorm，golang的gorm。</p>
<p>简单介绍下这几个框架的用法：</p>
<ul>
<li>mybatis-plus和tk.mybatis比较类似，使用lambda函数描述sql中的一些语法含义，替代了mybatis的动态xml模板层<br>
<img src="https://phantomma.top/post-images/1701435481669.png" alt="" loading="lazy"></li>
<li>Exposed和ktorm，kotlin写的，和上面两个比较像。一个比较明显区别，没有基于mybatis<br>
<img src="https://phantomma.top/post-images/1701435488764.png" alt="" loading="lazy"></li>
</ul>
<h3 id="spring-jdbc">spring jdbc</h3>
<p>除了上面类orm的框架，还有spring提供的jdbcTemplate，对JDBC进行简单的封装，提供了数据层操作最基本的一些能力，诸如：动态数据源、返回结果到java对象的映射等。<br>
spring jdbc的用法，没有动态xml，需要在java代码里手写sql。sql一长，换行拼接的方式，又非常不具有可读性。所以，被大规模正式地使用的情况，还是比较少。<br>
所有，在github也有类似spring-data-mybatis-mini的小工具，基于spring jdbc + mybatis的xml动态模板，实现动态sql的能力。</p>
<p><strong>那么，在上面两种方式之外，有没有一种中间路径，比orm更轻，比spring jdbc更易用的第三个选择？</strong></p>
<h3 id="第三种方案kotlin-spring-jdbc">第三种方案：kotlin + spring jdbc</h3>
<p>先上代码：</p>
<pre><code>@Repository
abstract class BaseDAO {

    @Resource
    private lateinit var sequenceService: SequenceService

    fun createEntity(tableName: String, entityDO: EntityDO, jdbcInsert: SimpleJdbcInsert): Long {
        if (entityDO.id == null) {
            entityDO.id = sequenceService.nextValue(tableName)
        }
        entityDO.gmtCreate = Date()
        entityDO.gmtModified = Date()
        val affectRow: Number = jdbcInsert.execute(BeanPropertySqlParameterSource(entityDO))
        if (affectRow != 1) {
            throw RuntimeException(&quot;create jdbcInsert.execute fail&quot;)
        }

        return entityDO.id
    }
}

@Repository
class TenantStaffDAO : BaseDAO() {

    private val tableName: String = &quot;tenant_staff&quot;

    @Resource
    private lateinit var jdbcTemplate: JdbcTemplate

    private lateinit var jdbcInsert: SimpleJdbcInsert

    private lateinit var jdbcUpdate: SimpleJdbcUpdate

    @PostConstruct
    fun init() {
        jdbcInsert = SimpleJdbcInsert(jdbcTemplate).withTableName(tableName)
        jdbcUpdate = SimpleJdbcUpdate(jdbcTemplate).withTableName(tableName).ukColumns(&quot;tenant_id&quot;, &quot;id&quot;)
    }

    fun create(staffDO: TenantStaffDO): Long {
        return super.createEntity(tableName, staffDO, jdbcInsert)
    }

    fun get(tenantId: Long, staffId: Long): TenantStaffDO? {
        val results = jdbcTemplate.query(&quot;select * from tenant_staff where tenant_id = ? and id = ?&quot;,
                arrayOf(tenantId, staffId), BeanPropertyRowMapper(TenantStaffDO::class.java))
        return DataAccessUtils.singleResult(results)
    }

    fun getByUid(tenantId: Long, uid: Long): TenantStaffDO? {
        val results = jdbcTemplate.query(&quot;select * from tenant_staff where tenant_id = ? and uid = ?&quot;,
                arrayOf(tenantId, uid), BeanPropertyRowMapper(TenantStaffDO::class.java))
        return DataAccessUtils.singleResult(results)
    }

    fun batchGet(tenantId: Long, staffIdList: List&lt;Long&gt;): List&lt;TenantStaffDO&gt;? {
        val sql = &quot;&quot;&quot;
             select * from tenant_staff 
             where tenant_id = ? 
             and id in ${staffIdList.joinToString(prefix = &quot;(&quot;, separator = &quot;, &quot;, postfix = &quot;)&quot;) { &quot;?&quot; }}
            &quot;&quot;&quot;
        val args = arrayListOf&lt;Any&gt;()
        args.add(tenantId)
        args.addAll(staffIdList)
        return jdbcTemplate.query(sql, args.toArray(), BeanPropertyRowMapper(TenantStaffDO::class.java))
    }

    fun listStaffs(tenantId: Long, size: Int, offset: Long): List&lt;TenantStaffDO&gt;? {
        val sql = &quot;select * from tenant_staff where tenant_id = ? order by id desc limit ? offset ?&quot;
        return jdbcTemplate.query(sql, arrayOf(tenantId, size, offset), BeanPropertyRowMapper(TenantStaffDO::class.java))
    }

    fun listTenants(uid: Long): List&lt;TenantStaffDO&gt;? {
        val sql = &quot;select * from tenant_staff where uid = ?&quot;
        return jdbcTemplate.query(sql, arrayOf(uid), BeanPropertyRowMapper(TenantStaffDO::class.java))
    }

    fun singleUpdate(staffDO: TenantStaffDO): Boolean {
        staffDO.gmtModified = Date()
        return jdbcUpdate.singleUpdate(staffDO)
    }

    fun delete(tenantId: Long, id: Long): Int {
        return jdbcTemplate.update(&quot;delete from tenant_staff where tenant_id = ? and id = ?&quot;, tenantId, id)
    }

    fun count(tenantId: Long): Int {
        return jdbcTemplate.queryForObject(&quot;select count(*) from tenant_staff where tenant_id = ?&quot;,
                arrayOf(tenantId), Int::class.java)
    }

}
</code></pre>
<h4 id="解决了dal层最基本需求">解决了dal层最基本需求</h4>
<ul>
<li>数据库结果到java对象映射</li>
<li>支持动态sql能力，比如in查询的列表，动态拼接</li>
<li>使用SimpleJdbcInsert进行DO对象的写入，可以做到不用关心表的字段</li>
<li>另外，spring-jdbc没有提供<a href="http://gitlab.alibaba-inc.com/hanxuan.mh/simple-jdbc-update">SimpleJdbcUpdate</a>的实现，简单实现了一个，可以达到一样的效果：传入DO对象进行update</li>
</ul>
<h4 id="几个好处">几个好处</h4>
<ul>
<li>和java混合编程，基本可以无缝衔接</li>
<li>直接编写程序员最熟悉的sql，而不是去理解一个xml或者lambda的转换层
<ul>
<li>不需要了解xml动态模板语法</li>
<li>特殊字符无需转义</li>
</ul>
</li>
<li>底层基于spring jdbc，简单纯粹</li>
<li>借助kotlin的字符串模板功能，基本可以做到mybatis的xml模板同样的效果。支持if、for等</li>
</ul>
<h4 id="存在的问题">存在的问题</h4>
<ul>
<li>java编写的DO类，如果使用了lombok，kotlin dao层代码在引用DO对象的属性时，会报编译错误。目前lombok和kotlin还无法在一起完美使用。解决办法是，在DO类里将kotlin代码使用的对象，手动生成setter、getter</li>
</ul>
<hr>
<p>另外，附上文中提到的几个框架的介绍链接</p>
<ul>
<li><a href="https://www.atatech.org/articles/176973?spm=ata.13269325.0.0.712d49faNqVZYN">Mybatis-plus学习与实践——从繁琐的CRUD中解放出来</a></li>
<li><a href="https://www.atatech.org/articles/137371?spm=ata.13269325.0.0.712d49faNqVZYN">对mybatis和mybatis plus进行扩展，更好的支持dao编码和测试</a></li>
<li><a href="https://github.com/VonChange/spring-data-mybatis-mini">spring-data-mybatis-mini：spring jdbc + mybatis 实现动态sql能力</a></li>
<li><a href="https://www.atatech.org/articles/168216?spm=ata.13269325.0.0.690449faq0aiZg">你还在用 MyBatis 吗，Ktorm 了解一下？——专注于 Kotlin 的 ORM 框架</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpringCloud On K8s Demo]]></title>
        <id>https://phantomma.top/post/springcloud-on-k8s-demo/</id>
        <link href="https://phantomma.top/post/springcloud-on-k8s-demo/">
        </link>
        <updated>2018-12-12T12:59:27.000Z</updated>
        <content type="html"><![CDATA[<p>主要参考spring-cloud-kubernetes，它是springcloud官方推出的开源项目，用于将Spring Cloud和Spring Boot应用运行在kubernetes环境。并结合fabric8 maven插件，做到CI，自动部署到k8s环境。</p>
<h3 id="部署结构">部署结构</h3>
<figure data-type="image" tabindex="1"><img src="https://phantomma.top/post-images/1701435774998.png" alt="" loading="lazy"></figure>
<h3 id="demo工程结构">Demo工程结构</h3>
<pre><code class="language-bash">~/projects/SpringCloudOnK8sDemo &gt; tree -L 1
.
├── backend-hello
├── backend-login
├── eureka
├── gateway-eureka
└── gateway-k8s
</code></pre>
<ul>
<li><a href="http://eureak.c066372bb24a34c35a2050758bc67bb9e.cn-hangzhou.alicontainer.com">eureka服务注册中心</a><br>
<img src="https://phantomma.top/post-images/1701435795080.png" alt="" loading="lazy"></li>
</ul>
<blockquote>
<p>一个gateway，两个后端服务</p>
</blockquote>
<h3 id="服务发现机制">服务发现机制</h3>
<p>基于k8s可以有两种服务机制：</p>
<ol>
<li>
<p>SpringCloud eureka<br>
<img src="https://phantomma.top/post-images/1701435808134.png" alt="" loading="lazy"></p>
</li>
<li>
<p>k8s api server<br>
<img src="https://phantomma.top/post-images/1701435818620.png" alt="" loading="lazy"></p>
</li>
</ol>
<blockquote>
<p>他们分别提供了生态内的相关能力，都有类似如：灰度发布、容量分配、熔断、降级等基础能力。但考虑专有云是天基底座，可能没有k8s，避免依赖k8s，可以脱离其运行，内置SpringCloud是一种更架构独立、减小绑定的选择。</p>
</blockquote>
<h3 id="服务路由">服务路由</h3>
<p>在gateway中，通过uri中route前缀判断是需要做服务路由的请求<br>
<img src="https://phantomma.top/post-images/1701435833792.png" alt="" loading="lazy"></p>
<ul>
<li>例如这几个示例：
<ul>
<li>访问gateway：<a href="http://gateway-eureka.c066372bb24a34c35a2050758bc67bb9e.cn-hangzhou.alicontainer.com/hostName">http://gateway-eureka.c066372bb24a34c35a2050758bc67bb9e.cn-hangzhou.alicontainer.com/hostName</a></li>
<li>通过服务路由访问hello服务：<a href="http://gateway-eureka.c066372bb24a34c35a2050758bc67bb9e.cn-hangzhou.alicontainer.com/route/hello/hostName">http://gateway-eureka.c066372bb24a34c35a2050758bc67bb9e.cn-hangzhou.alicontainer.com/route/hello/hostName</a></li>
<li>通过服务路由访问login服务：<a href="http://gateway-eureka.c066372bb24a34c35a2050758bc67bb9e.cn-hangzhou.alicontainer.com/route/hello/hostName">http://gateway-eureka.c066372bb24a34c35a2050758bc67bb9e.cn-hangzhou.alicontainer.com/route/login/hostName</a></li>
<li>服务间调用hello调login：<a href="http://gateway-eureka.c066372bb24a34c35a2050758bc67bb9e.cn-hangzhou.alicontainer.com/route/hello/list">http://gateway-eureka.c066372bb24a34c35a2050758bc67bb9e.cn-hangzhou.alicontainer.com/route/hello/list</a></li>
</ul>
</li>
</ul>
<h3 id="负载均衡">负载均衡</h3>
<p>Ribbon简单来说 是一个基于 HTTP 和 TCP 客户端的负载均衡器，它可以在客户端配置 ribbonServerList（服务端列表），然后轮询请求以实现均衡负载。<br>
Feign是一个使用起来更加方便的 HTTP 客户端，使用起来就像是调用自身工程的方法，而感觉不到是调用远程方法，达到类rpc调用的效果。</p>
<blockquote>
<p>Ribbon和Feign可以封装组合使用（Feign封装了Ribbon）。</p>
</blockquote>
<ul>
<li>
<p>使用Ribbon客户端调用http接口，具有客户端负载均衡的能力<br>
<img src="https://phantomma.top/post-images/1701435859571.png" alt="" loading="lazy"></p>
</li>
<li>
<p>使用Feign做到类rpc调用<br>
<img src="https://phantomma.top/post-images/1701435872404.png" alt="" loading="lazy"><br>
<img src="https://phantomma.top/post-images/1701435880127.png" alt="" loading="lazy"></p>
</li>
</ul>
<h3 id="基于http做到类rpc调用的异常机制">基于http做到类rpc调用的异常机制</h3>
<figure data-type="image" tabindex="2"><img src="https://phantomma.top/post-images/1701435895362.png" alt="" loading="lazy"></figure>
<h3 id="隔离">隔离</h3>
<ul>
<li>hystrix熔断器，负载服务故障的隔离。</li>
<li>RestTemplate --&gt; Ribbon --&gt; hystrix</li>
</ul>
<h3 id="traceid的透传">traceId的透传</h3>
<ul>
<li>结合EagleEye.getTraceId()获得traceId</li>
<li>对于EagleEye，traceId是放ThreadLocal。基于SpringCloud http的方式，可以http header。在http header中透传<br>
<img src="https://phantomma.top/post-images/1701435924871.png" alt="" loading="lazy"></li>
</ul>
<h3 id="全链路跟踪轻量鹰眼">全链路跟踪（轻量鹰眼）</h3>
<p><img src="https://phantomma.top/post-images/1701435935143.png" alt="" loading="lazy"><br>
使用鹰眼生成全局唯一traceId，以及traceId机制。每台日志对请求打点，然后结合ELK，做到日志全链路查询<br>
<img src="https://phantomma.top/post-images/1701435944046.png" alt="" loading="lazy"></p>
<h3 id="灰度">灰度</h3>
<h4 id="灰度发布">灰度发布</h4>
<p>依赖k8s的内置支持</p>
<h4 id="流量灰度">流量灰度</h4>
<ul>
<li>AB测试</li>
<li>对用户打标，内测发布</li>
<li>beta发布</li>
</ul>
<h3 id="开发方式">开发方式</h3>
<h4 id="使用fabric8-maven插件完成-打包-上传-部署">使用fabric8 maven插件完成 打包、上传、部署</h4>
<pre><code class="language-bash">mvn clean package fabric8:build -Pdev	// 指定环境变量，打包docker镜像
docker tag xxx registry.cn-hangzhou.aliyuncs.com/cmp-poc/xxx	// 重命名docker镜像
docker push registry.cn-hangzhou.aliyuncs.com/cmp-poc/xxx		// push到私有仓库
mvn fabric8:deploy	// 使用kubectl create depoly &amp; service
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kotlin 快速浏览]]></title>
        <id>https://phantomma.top/post/kotlin-kuai-su-liu-lan/</id>
        <link href="https://phantomma.top/post/kotlin-kuai-su-liu-lan/">
        </link>
        <updated>2018-12-01T12:45:58.000Z</updated>
        <content type="html"><![CDATA[<h3 id="kotlin是啥">Kotlin是啥</h3>
<p>Kotlin是一门JVM上的，与Java完美互操作，简洁、易读、安全、通用为设计目标，同时支持面向对象和函数式两种编程范式，高质量的现代静态类型语言。或者换一句话，是集成了Java（面向对象，强大生态），C#（美妙的扩展方法），Ruby（魔法般的Code Block），Scala（函数式编程，克制的运算符重载，易用版Trait）等众多语言优点的高性能的美妙语言。</p>
<h3 id="kotlin简历">Kotlin「简历」</h3>
<ul>
<li>来自于著名的IntelliJ IDEA的软件开发公司 JetBrains (位于东欧捷克)</li>
<li>起源来自 JetBrains 的圣彼得堡团队，名称取自圣彼得堡附近的一个小岛 (Kotlin Island)</li>
<li>一种基于 JVM 的静态类型编程语言</li>
</ul>
<h3 id="增长情况">增长情况</h3>
<figure data-type="image" tabindex="1"><img src="https://phantomma.top/post-images/1701435017011.png" alt="" loading="lazy"></figure>
<h3 id="有啥亮点">有啥亮点</h3>
<h4 id="安身立命之本-互操作性和java生态">安身立命之本--互操作性和Java生态</h4>
<ul>
<li>无缝引入到现有java项目</li>
<li>java -&gt; kotlin ✔️</li>
<li>kotlin -&gt; java ✔️</li>
</ul>
<h4 id="安卓阵营的拯救者">安卓阵营的拯救者</h4>
<ul>
<li>谷歌和Oracle的Java侵权案</li>
<li>安卓Dalvik虚拟机智能使用jdk7以下api</li>
<li>kotlin被谷歌钦定官方推荐语言</li>
</ul>
<h4 id="编写更安全-易读的代码">编写更安全、易读的代码</h4>
<pre><code>var a: String = “abc”; // 定义个一个非null的字符串变量a
a = null; // 编译直接失败

var b: String? = “abc”; // 定义一个可为null的字符串变量b
b = null; // 编译通过

val l = b.length; // 编译失败，因为b可能为null
l = b?.length ?: -1 // 如b为null，就返回-1
l = b?.length; // 如b为null，就返回null
l = b!!.length; // 如b为null，就会直接抛NPE错误
b?.let { println(b) } // 如b为null，就不执行let后面的代码块

val nullableList: List&lt;Int?&gt; = listOf(1, 2, null, 4)
val intList: List&lt;Int&gt; = nullableList.filterNotNull() // 过滤出列表中所有不为null的数据，组成新的队列intList

// 可以通过lateinit var(不可为val)，定义一个无需在申明时初始化的non-nullable的参数，这个参数不允许被赋值为空，并且在调用时如果没有初始化会抛异常
lateinit var lateInitValue : String

// 通过by lazy { ... } 表达式，让所定义的参数在第一次访问(get)的时候执行{...}这段代码块，并赋值
val lazyValue: String by lazy {
  doAnything()
  &quot;build lazy value&quot;
}

// 可以直接在赋值中使用表达式，甚至内嵌执行语句
val max = if (a &gt; b) a else b	// 三元表达式
val max = if (a &gt; b) {
    print(&quot;Choose a&quot;)
    a
} else {
    print(&quot;Choose b&quot;)
    b
}

// 支持when的表达式
println(when (language) {
    &quot;EN&quot; -&gt; &quot;Hello!&quot;
    &quot;FR&quot; -&gt; &quot;Salut!&quot;
    else -&gt; &quot;Sorry, I can't greet you in $language yet&quot;
})

// 支持in，表达在一定的范围内作为条件
when (x) {
    in 1..10 -&gt; print(&quot;x is in the range&quot;)
    in validNumberArray -&gt; print(&quot;x is valid&quot;)
    else -&gt; print(&quot;none of the above&quot;)
}
</code></pre>
<h4 id="数据类">数据类</h4>
<p>一行代码搞定 pojo, 自动生成 get/set/toString, 类似 lombok 插件</p>
<pre><code>data class User(val name: String, val age: Int)
</code></pre>
<h4 id="更好用的函数式编程">更好用的函数式编程</h4>
<ul>
<li>原生支持的不可变对象</li>
<li>更好用的lambda表达式，且性能更高（提升30%）</li>
<li>比jdk8 stream api更好用的集合操作</li>
</ul>
<pre><code>val numbers = arrayListOf(-42, 17, 13, -9, 12) //创建一个List，并给定值
val nonNegative = numbers.filter { it &gt;= 0 } //从numbers中过滤出&gt;=0的队列

listOf(1, 2, 3, 4) // 列出 1, 2, 3, 4
.map { it * 10 } // 所有值乘以10 10, 20, 30, 40
.filter { it &gt; 20 } // 过滤出&gt;20的值 30, 40
.forEach { print(it) } // 打印出每个值 30, 40
</code></pre>
<h4 id="非受检异常">非受检异常</h4>
<p>unchecked exception，可以自己决定在调用栈的某一层 catch 和处理，不必到处 try-catch</p>
<h4 id="字符串模板">字符串模板</h4>
<pre><code>val s = &quot;abc&quot;
val str = &quot;$s.length is ${s.length}&quot; // 结果为 &quot;abc.length is 3&quot;

val x = 4
val y = 7
print(&quot;sum of $x and $y is ${x + y}&quot;)  // sum of 4 and 7 is 11
</code></pre>
<p>类似一个更智能、更易读的Java版本的String.format()</p>
<h4 id="类扩展">类扩展</h4>
<pre><code>fun MutableList&lt;Int&gt;.swap(index1: Int, index2: Int) { 
    val tmp = this[index1] // 'this' 对应该列表 
    this[index1] = this[index2]
    this[index2] = tmp
    }
}
//使用
val l = mutableListOf(1, 2, 3)
l.swap(0, 2) // 'swap()' 内部的 'this' 得到 'l' 的值
</code></pre>
<h4 id="免费的午餐-性能无损">免费的午餐--性能无损</h4>
<p>Kotlin增加特性的同时，没有降低性能，部分benchmark中相比java还有微弱的性能提升，马儿跑得更快还吃的更少。<br /><br>
<a href="https://blog.dreamtobe.cn/kotlin-performance">Kotlin运行时性能</a></p>
<h4 id="dsl去敌人的地盘吃鸡">DSL——去敌人的地盘吃鸡</h4>
<p>Kotlin因为其灵活的语法（比如可以重定义运算符），适合用来写DSL，比如：用Kotlin写HTML模板，anko 安卓绑定控件类库。</p>
<h3 id="kotlin协程">kotlin协程</h3>
<figure data-type="image" tabindex="2"><img src="https://phantomma.top/post-images/1701435043080.png" alt="" loading="lazy"></figure>
<ul>
<li>kotlin 1.3发布了稳定版的协程</li>
<li>让原本需要异步+回调的才能获得高性能的场景，可以通过使用看似同步的方式来编写代码</li>
<li>java平台上，在loom项目推出之前，最容易体验到协程遍历的途径</li>
</ul>
<p>Coroutine的四大金刚操作：launch, runBlocking, async和await。</p>
<ul>
<li>launch。launch最简单，只负责启动，启动完就不管了，既不等待结束，也不关心结果。</li>
<li>runBlocking。runBlocking在运行代码块之后会阻塞当前coroutine，直到代码块运行完成，然后获取结果。</li>
<li>async和await。launch和runBlocking都相对简单，如果我要启动coroutine，然后立马返回，但是又想关心结果怎么办呢，用async。async会返回一个Deferred，T是结果类型，然后你可以做别的，需要的时候，用Deferred上的await()函数来等待结果。注意await()本身也是suspend函数，你需要把它放在coroutine里面（launch，runBlocking，async调用的代码块）里面，或者将代码所在的函数标为suspend。</li>
</ul>
<p>四大金刚操作的比较：<br>
<img src="https://phantomma.top/post-images/1701435244219.png" alt="" loading="lazy"></p>
<h3 id="回调-vs-响应式编程-vs-协程">回调 vs 响应式编程 vs 协程</h3>
<ul>
<li>node.js</li>
<li>reactive</li>
<li>coroutine</li>
</ul>
<h3 id="如何在项目中使用kotlin">如何在项目中使用kotlin</h3>
<ul>
<li>idea（或eclipse）较新的版本+kotlin插件</li>
<li>添加kotlin依赖</li>
</ul>
<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.jetbrains.kotlin&lt;/groupId&gt;
    &lt;artifactId&gt;kotlin-stdlib-jdk8&lt;/artifactId&gt;
    &lt;version&gt;${kotlin.version}&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<ul>
<li>混合编程<br>
<img src="https://phantomma.top/post-images/1701435076535.png" alt="" loading="lazy"></li>
</ul>
<h3 id="现阶段的不足">现阶段的不足</h3>
<ul>
<li>ide代码提示还会有些卡顿</li>
<li>与lombok不兼容（看不到lombok增强后的方法）</li>
</ul>
<h3 id="相关资料">相关资料</h3>
<ul>
<li><a href="https://www.kotlincn.net/docs/reference/basic-syntax.html">基本语法</a></li>
<li><a href="https://www.kotlincn.net/docs/reference/idioms.html">习惯用法</a></li>
<li><a href="https://www.kotlincn.net/docs/reference/java-interop.html">与java互操作</a></li>
<li><a href="https://blog.csdn.net/lj402159806/article/details/82689122">kotlin与java语法对比</a></li>
<li><a href="https://pusher.com/state-of-kotlin#journey">kotlin增长报告</a></li>
</ul>
<h2 id="代码review">代码review</h2>
<ul>
<li>正反向关系表同步，逆向表查询索引，根据id回主表查询。（新加坡vpc有精卫环境）</li>
<li>lombok标记Data类，另外可选kotlin的data对象</li>
<li>使用pandora boot。定制了一个消费多个topic的MultiMetaqListener</li>
<li>jdk8新特性，stream</li>
<li>kotlin混合编程</li>
<li><a href="https://yuque.antfin-inc.com/nm6kn8/proj_z/binl02">改良版geo hash简单介绍</a></li>
<li>lwp、hsf接口测试方案</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[个人效率提升工具（Mac）]]></title>
        <id>https://phantomma.top/post/ge-ren-xiao-lu-ti-sheng-gong-ju-mac/</id>
        <link href="https://phantomma.top/post/ge-ren-xiao-lu-ti-sheng-gong-ju-mac/">
        </link>
        <updated>2018-11-01T14:00:21.000Z</updated>
        <content type="html"><![CDATA[<h2 id="mac个人软件">Mac个人软件</h2>
<h3 id="raycast">Raycast</h3>
<ul>
<li>对于没有使用过Alfred的同学，可以考虑直接使用Raycast</li>
<li>rust编写，效率更高</li>
<li>类Alfred的搜索功能，除了搜索（文件、app、日历、代办登），还集成了很多其它小功能，如：剪贴板历史、计算器。。</li>
</ul>
<h3 id="xnip">Xnip</h3>
<ul>
<li>截图并贴图</li>
<li>免费版基本够用，轻量不占资源</li>
</ul>
<h3 id="the-archiver">The archiver</h3>
<ul>
<li>轻量干净的解压缩软件</li>
</ul>
<h3 id="updf">UPDF</h3>
<ul>
<li>现代强大的pdf阅读软件</li>
</ul>
<h3 id="iterm">iTerm</h3>
<ul>
<li>全彩色终端</li>
<li>掌握快速唤起、隐藏终端</li>
<li>split新的窗口，以及在之间切换</li>
<li>终端增强：OhMyZsh + PowerLevel10K</li>
<li>安装zsh插件<pre><code>plugins=(
    git
    zsh-completions 
    zsh-autosuggestions 
    zsh-syntax-highlighting
)
</code></pre>
</li>
</ul>
<h3 id="maccy">Maccy</h3>
<ul>
<li>非常轻量的剪贴板历史工具。只保存纯文本</li>
<li>有了Raycast，就没有必要了</li>
</ul>
<h3 id="runcat">RunCat</h3>
<ul>
<li>mac菜单栏工具，一只奔跑的cat，显示一些系统运行状态</li>
</ul>
<h3 id="stats">Stats</h3>
<ul>
<li>免费软件，功能和界面基本做到了付费软件 iStatMenu一致的水平</li>
<li>比上面的runcat功能强大，不止cpu，内存、网络、磁盘都可以监控显示</li>
</ul>
<h3 id="bob翻译">Bob翻译</h3>
<ul>
<li>划词翻译</li>
<li>可设置第三方翻译服务</li>
<li>OCR识别</li>
</ul>
<h3 id="motrix">motrix</h3>
<ul>
<li>下载工具</li>
<li>支持多线程，BT下载</li>
</ul>
<h3 id="xmind">XMind</h3>
<ul>
<li>本地思维导图，免费版基本够用</li>
</ul>
<h3 id="alttab">AltTab</h3>
<ul>
<li>cmd + Tab 窗口切换之外的一个补充工具</li>
<li>可以在同一app下的多个窗口之间切换</li>
</ul>
<h3 id="gridea">Gridea</h3>
<ul>
<li>静态博客app。自带一个客户端，可以直接在app内编辑blog，提供了一个markdown编辑器，还算好用</li>
<li>支持上传图片到本地git仓库，渲染时自动替换为相对路径</li>
<li>自动github同步功能，写完一键同步至github page</li>
<li>不过最近停止更新了。软件包还是intel的打包</li>
</ul>
<h3 id="xnip-2">Xnip</h3>
<ul>
<li>试用下来，满足我当前需求的最佳截图软件。比较，iShot、Paste等，更原生，且轻量</li>
<li>具备钉图功能，图标标注也不错</li>
<li>重要的是，appstore上免费</li>
</ul>
<h3 id="clashx">ClashX</h3>
<ul>
<li>科学上网</li>
<li>结合fastlink梯子。https://v02.fl-aff.com/auth/register?code=5X1E</li>
<li><a href="https://macwk.cn/app/529.html">体验下来，最好的版本</a></li>
</ul>
<h3 id="shadowsocketsx-ng">ShadowSocketsX-NG</h3>
<ul>
<li>同上</li>
<li>比叫轻量，界面更像mac软件</li>
</ul>
<h3 id="hiddenbar">HiddenBar</h3>
<ul>
<li>macbook 刘海屏，拯救状态栏被刘海隐藏的图标</li>
</ul>
<h3 id="bartender">Bartender</h3>
<ul>
<li>同上，功能更强大一些。可以把隐藏图标单独显示成一栏</li>
</ul>
<h3 id="cheatsheet">CheatSheet</h3>
<ul>
<li>快捷键作弊表</li>
<li>长按cmd键，显示当前应用的快捷键列表</li>
</ul>
<h3 id="updf-2">UPDF</h3>
<ul>
<li>国产pdf软件，功能是真的强大</li>
<li>编辑功能，ocr识别等</li>
</ul>
<h3 id="notion">Notion</h3>
<ul>
<li>支持markdown语法，同时编辑时可见即所得</li>
<li>支持图片的大小调整，比较适合码农写技术文章</li>
<li>方便在文档里编辑表格。这对原生markdown在表格方便的弱鸡功能，做了很大的增强</li>
</ul>
<h3 id="omnigraffle">OmniGraffle</h3>
<ul>
<li>很适合画软件架构图</li>
<li>提供丰富的模版和内置图形</li>
</ul>
<h3 id="shortcutdetective">ShortCutDetective</h3>
<ul>
<li>快捷键冲突检测工具</li>
</ul>
<h3 id="silicon">Silicon</h3>
<ul>
<li>扫描电脑上的app，显示是否针对apple m芯片单独编译的</li>
<li><a href="https://www.macrumors.com/how-to/tell-apps-optimized-for-m1-apple-silicon-macs/">How to Tell Which Mac Apps Are Optimized for Apple Silicon</a></li>
</ul>
<h2 id="brew-安装的工具">brew 安装的工具</h2>
<ul>
<li>首先是安装 homebrew</li>
</ul>
<h3 id="通过brew-安装的工具">通过brew 安装的工具</h3>
<pre><code>brew install wget
brew install jq
brew install git
brew install tree
brew install jenv
brew install maven
</code></pre>
<h2 id="开发工具">开发工具</h2>
<h3 id="jdk">JDK</h3>
<ul>
<li>推荐Oracle JDK</li>
</ul>
<h3 id="jenv">jenv</h3>
<p>管理多个jdk版本的命令行工具</p>
<h3 id="idea">IDEA</h3>
<ul>
<li>java IDE</li>
</ul>
<h3 id="vs-code">VS Code</h3>
<ul>
<li>轻量IDE，可装各种插件，支持各种语言</li>
</ul>
<h3 id="orbstack">OrbStack</h3>
<ul>
<li>可以创建vm</li>
<li>可以创建docker</li>
<li>自带一个k8s集群</li>
<li>虚拟化方案针对mac平台专门优化，不想DockerDesktop底层以来一个linux的vm</li>
</ul>
<h3 id="lens">Lens</h3>
<ul>
<li>功能强大的k8s ide</li>
<li>能够显示所有k8s内置资源类型，以及自定义CRDs</li>
<li>不过也有个小bug。使用长时间后，肯跟会堵塞网络，导致浏览器也无法上网。解决办法，重启应用 就好了</li>
</ul>
<h2 id="saas服务">SaaS服务</h2>
<h3 id="poe">Poe</h3>
<ul>
<li>quora提供的聚合各种AI机器人的网站，也支持ChatGPT机器人，速度比较块，也不想OpenAI对使用者IP region限制比较严格</li>
</ul>
<h3 id="ai论文阅读">AI论文阅读</h3>
<ul>
<li>Humata：对输入的论文理解表现是AI中比较出色的，缺点是用多了就得付费</li>
<li>ChatPDF：免费的PDF学习AI，理解力比Humata差点，不过完全免费</li>
</ul>
<hr>
<p>推荐一个mac软件网站：https://haxmac.cc/bartender-crack-mac/</p>
<p>。。。TODO</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[MySQL Binlog 源码学习]]></title>
        <id>https://phantomma.top/post/mysql-binlog-yuan-ma-xue-xi/</id>
        <link href="https://phantomma.top/post/mysql-binlog-yuan-ma-xue-xi/">
        </link>
        <updated>2017-01-04T15:25:21.000Z</updated>
        <content type="html"><![CDATA[<p>MySQL主从节点之间同步数据，备份点还原，会用到binary log，其负责记录数据更改的操作。因为Binlog在运用到数据页之前需要经过复杂的过程，没有redolog直接，所以性能比不上直接使用redo复制的方式（物理复制的优势），但是它也有不可或缺的作用。本文重点介绍MySQL Binlog的作用、记录的内容、组织结构、写入方式、主备复制等内容，基于MySQL 8.0的代码。因为网上对Binlog各个知识点的介绍都非常详细，但是知识点非常杂，所以给本人初学Binlog的时候带来很多困难，因此本文的目的是总结这些知识点。</p>
<blockquote>
<p>本文内容基于 MySQL Community 8.0.13 Version</p>
</blockquote>
<h2 id="为什么要有binlog">为什么要有Binlog</h2>
<p>MySQL上下分为SQL层和引擎层，不同存储引擎中的日志格式是不同的，由于要对多引擎支持，必须在SQL层设计逻辑日志以透明化不同存储引擎，而这个逻辑日志就是Binlog。当有数据修改请求时，primary会产生包含该修改操作的Binlog，并发送给replica，replica通过回放该Binlog以执行和primary同样的修改。此外还可用于备份点还原。<br>
在PolarDB中，虽然通过物理复制可以完成上面的功能，但是MySQL生态中用户需要Binlog导出数据库做审计、数据校验、数据清理等操作；以及用户混合使用多种数据库搭建业务平台也需要Binlog完成不同数据库之间数据传输；还有某些数据备份工具例如阿里云DTS、第三方的OGG（Oracle）、开源的canal/open-replicator、以及MySQL自带的mysqlbinlog等仍然依赖Binlog。所以PolarDB也支持Binlog，但是其通过Logic Redo的方式将Binlog写入redo中来提升性能。</p>
<h2 id="binlog的记录格式">Binlog的记录格式</h2>
<p>显然，server记录Binlog要尽量少，因为对数据库的修改只有在其Binlog落盘后才算成功，同时还要保证在主从上执行的同一语句的结果相同。所以Mysql提供了三种记录Binlog的格式：基于语句的（statement-based logging），基于行的（row-based logging）和混合的（mixed logging）。可以通过binlog-format指定。<br>
在介绍Binlog类型前先说下什么是非确定性的语句（ non-deterministic），即同一条语句在集群的不同server上执行的结果不同，举个例子：UUID()，如果在某个修改操作的SQL中使用了这个语句，那么在不同server上的效果是不同的。<br>
基于语句的方式会直接记录SQL语句，这种方式产生的Binlog少，占用磁盘空间和I/O也最少，此外主从复制的数据也少，审计数据库更改也更加方便，缺点是无法用于非确定性的语句（ non-deterministic）；基于行的方式记录了对表中某个行的修改，这种方式因为是直接复制整个行，所以可以避免上面的问题，此外需要拿的行锁也更少，缺点就是日志本身占用空间更大，采用二进制记录格式不易审计；混合的方式会根据操作类型（切换原则）切换使用这两种方式。此外DDL操作即使在row格式下也是记录SQL语句的。<br>
所以如果存储空间和I/O不是主要问题，最好使用基于行的记录格式，因为这样更加安全。<br>
最后放一个测试的demo，让大家更好的从具体SQL去理解这三种记录格式。本文接下来只介绍row-based logging。</p>
<h2 id="binlog文件里面是什么">Binlog文件里面是什么</h2>
<p>Binlog的版本为4，以未加密的Binlog为例，布局如图1所示，Binlog的开头分别由MAGIC HEADER、FORMAT_DESCRIPTION_EVENT和PREVIOUS_GTIDS_LOG_EVENT构成。后面就是一个个其他Binlog Event了。注意是否开启GTID都是这样的布局，区别是若未开启GTID（gtid_mode=OFF），则previous_gtids_log_event和gtid_log_event记录的gtid为空。<br>
<img src="https://phantomma.top/post-images/1704382097930.png" alt="" loading="lazy"><br>
图1 开启GTID且未加密的Binlog文件</p>
<h3 id="binlog-event结构">Binlog Event结构</h3>
<p>前面说了Binlog的格式，下面说下Binlog文件的内容，首先介绍各种Binlog Event，每个Event分为Event Header、Post-Header、Payload、Event Footer四部分，如图2所示。<br>
<img src="https://phantomma.top/post-images/1704382115747.png" alt="" loading="lazy"><br>
图2 单个Binlog Event结构</p>
<p>Event Header 内容类型一样，占用19bytes，对应类，内容如图3，图4所示：<br>
<img src="https://phantomma.top/post-images/1704382142174.png" alt="" loading="lazy"><br>
图3 Event Header 内存结构</p>
<p><img src="https://phantomma.top/post-images/1704382150786.png" alt="" loading="lazy"><br>
图4 Event Header 各个字段含义</p>
<p>Event Footer记录计算event checksum的算法信息，这个信息也记录在FDE：FORMAT_DESCRIPTION_EVENT中，同一Binlog文件中的该信息一样，对应Log_event_footer类。Post-Header、Payload分别是每个Event 类型的Header和内容实体，不同Event种类不同。下面介绍几个典型event_type。</p>
<h3 id="binlog-event类型">Binlog Event类型</h3>
<p>FORMAT_DESCRIPTION_EVENT<br>
该event写在Binlog文件开始4字节的位置，紧挨着magic。用于描述binlog的layout和解码Binlog Event。该类型中Post-Header、Payload指的是同样的内容。记录了Binlog Version、Mysql Server Version和Create Timestamp信息。</p>
<p>PREVIOUS_GTIDS_LOG_EVENT<br>
GTID在后面介绍，这里只用知道这个Event中涵盖了该Binlog之前所有Binlog文件中（包括已经被删除的）事务的GTID，也就是说记录了所有被执行的事务的GTID。为什么说是被执行了的？因为在Binlog rotate出新的文件前，旧文件的事务会被提交或者回滚，保留下来的一定被提交了。</p>
<p>GTID_LOG_EVENT<br>
GTID唯一的对应一个事务。因为Binlog是逻辑层日志，本身不幂等，所以为了防止一个事务被多次执行，每个事务都需要有一个全局的事务标识——Global Transaction IDentifier（GTID）。GTID由全局事务标识的UUID和递增的Group number组成。该Event中还记录了事务在不同server上的提交时间，更详细的可以查看MySQL GTID EVENT。</p>
<p>QUERY_EVENT<br>
基于statement格式的对数据修改操作都是以这种Binlog类型记录，此外，DDL也是以这种类型记录的。这里不详细展开了，详见官方文档。</p>
<p>WRITE_ROWS_EVENT、UPDATE_ROWS_EVENT、DELETE_ROWS_EVENT<br>
Row记录格式下，对表的修改会产生这些类型的Binlog。</p>
<p>XID_EVENT<br>
在XA事务commit时记录，标识事务的结尾，其中XID是事务号，由一个8位无符号整型表示。在recover时会根据它判断Binlog所记录的XA事务是否完整，注意XID和GTID所描述的不同，XID是对上层应用而言的事务号，关系到事务能否原子的执行；GTID为保证集群内的一致性，关系到事务能否在集群中的所有server上有且只有一次执行。</p>
<h3 id="小结">小结</h3>
<p>介绍完Event类型后，很容易理解在基于Row格式记录的Binlog中，Event往往以图5这两种形式组合排布，图5（左）中的QUERY_EVENT记录的内容是执行的具体SQL，图5右中的QUERY_EVENT记录的是BEGIN，标识事务的开启。在下一节中，我会结合Binlog文件内容介绍Binlog是如何高效，安全的写入磁盘的，以及如何与物理层日志之间保持一致性。<br>
<img src="https://phantomma.top/post-images/1704382234164.png" alt="" loading="lazy"><br>
<img src="https://phantomma.top/post-images/1704382219203.png" alt="" loading="lazy"><br>
图5 DDL（左）/DML（右） Binlog事务</p>
<h2 id="binlog是如何写入的">Binlog是如何写入的</h2>
<p>由于MySQL的SQL和引擎层的双日志体系，Binlog写入需要解决多个引擎之间事务执行的一致性问题。此外，由于从日志产生到落盘是数据库写入的关键路径，所以写入的效率也是需要关注的。下面我就从这两个方面来介绍Binlog的写入过程。</p>
<h3 id="分布式事务模型xa">分布式事务模型——XA</h3>
<p>XA源于<a href="https://pubs.opengroup.org/onlinepubs/009680699/toc.pdf">Distributed Transaction Processing: The XA Specification</a>，这篇文章定义了分布式事务处理模型，其中定义了事务管理器（充当协调者），负责为事务分配标识符，监视它们在不同参与者上执行的进度，并负责事务完成和故障恢复。 还定义了资源管理器，充当参与者，受协调者管理。此外还有应用程序，充当事务的发起者。</p>
<h4 id="mysql中的xa类型以及协调者选择">MySQL中的XA类型以及协调者选择</h4>
<p>在MySQL中，如果事务的参与者是各个实例节点，那么是外部XA，由上层程序担当协调者，上层程序可以通过XA start，XA prepre，XA end，和XA commit的命令管理事务的执行。如果事务的参与者只在单实例节点内部，那么称为内部XA，例如参与者是Binlog和innodb。对于内部XA的协调者，如果开启Binlog，则Binlog为协调者，显然选择Binlog作为协调者是最合适的，因为Binlog位于引擎层之上且还负责主备之间数据的同步。如果不开Binlog，且只有innodb一个成员，那就不需要XA了。但是如果没有Binlog且在引擎层有多个参与者，那么MySQL会使用TC_LOG_MMAP作为协调者。XA采用两阶段提交协议保证分布式事务的一致性。两阶段提交分为prepare和commit两个阶段，协议的内容参考 <a href="https://help.aliyun.com/document_detail/132896.html">分布式事务两阶段提交</a>，<br>
在Prepare阶段前，进入函数ha_commit_trans。这里有个参数all。’all为false’ 表示这是用户发出的显式提交，’all为true’表示是 DDL 发出的隐式提交。某些DDL在执行完成后会隐式提交，也就是无需用户调用commit等结束语句而自发提交，这就意味着一条DDL是一个单独的事务，用户无法回滚它，详见<a href="https://dev.mysql.com/doc/refman/8.0/en/implicit-commit.html">Statements That Cause an Implicit Commit</a>。如果打开了autocommit，DML也会自发提交，详见 <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-autocommit-commit-rollback.html">autocommit</a>。所以XA事务有很多种情况（内部、外部、是否开Binlog、是否为DDL等），接下来主要介绍开启row_based格式的Binlog，开启GTID，存储引擎只有innodb的内部XA执行过程。以DDL和DML语句为例，整个过程如图6所示。<br>
<img src="https://phantomma.top/post-images/1704382421023.png" alt="" loading="lazy"><br>
图6 由Binlog担任协调者的XA事务处理过程</p>
<h4 id="prepare阶段">Prepare阶段</h4>
<p>prepare阶段分为binlog的prepare和innodb的prepare。进入binlog和innodb prepae前会设置durability_property = HA_IGNORE_DURABILITY, 表示在innodb prepare和finish_commit()时，不刷redo log到磁盘。</p>
<ul>
<li>
<p>Binlog Prepare<br>
入口：binlog_prepare<br>
对于all为false的事务，会更新该事务的last_commited为此时most recently commited事务的sequence_number，sequence_number是Binlog提交的逻辑时间戳，可用于在slave节点上并行执行Binlog事务，生成和自增策略参考Binlog事务依赖策略。</p>
</li>
<li>
<p>Innodb Prepare<br>
入口：innobase_xa_prepare<br>
初始化Innodb事务，将事务的状态由TRX_STATE_ACTIVE设置为TRX_STATE_PREPARED，标志事务进入prepare阶段，在undo log page中写入TRX_UNDO_PREPARED状态，若有xid则会在undo中也记录xid信息。</p>
</li>
</ul>
<h4 id="commit阶段">Commit阶段</h4>
<p>Commit成功意味着在当前事务中所做的更改是永久性的，并且对其他session可见。Commit阶段分为Binlog的Commit和Innodb的Commit。</p>
<ul>
<li>
<p>Binlog Commit<br>
入口：binlog_commit<br>
在Commit之前，Binlog已经写入到局部Binlog（见4.2.2），Commit时只需在结尾写入Binlog事务结尾的标识，例如XID_EVENT，在recover的时候据此判断事务的完整性。因为每条DDL都会implicit提交，所以一个DDL事务只会记录一条QUERY EVENT，所以结尾不需要记录XID_EVENT就可判断DDL事务是否完整。<br>
随后开始提交Binlog，也就是将各个thd的Binlog事务写到Binlog文件中，Binlog文件中事务之间要彼此独立的顺序排列，不会交错，因为交错的事务难以被slave apply。然而一个一个写binlog并落盘显然效率极低，为了提高效率，MySQL采用Group Commit的方式。整体过程在网上有很多讲解，本文主要从代码层面讲解具体的几个关键函数，关于Group Commit的实现方法本文不介绍了。Group Commit分为三个阶段flush、sync、commit。在flush阶段将redo log持久化，将Binlog 写到文件系统的page cache中。在sync阶段将Binlog刷盘。下面具体介绍（序号对应图中的序号）：<br>
（6）ending_trans()函数判断是否对本次事务进行提交，有四种情况，用户发起的一条DDL语句会分别执行显式（explicit）和隐式（implicit）的提交，若为显式，则事务不提交；若为隐式才真正提交，对DDL而言，其会在执行时将autocommit置为0，所以autocommit对DDL不生效，因为不论autocommit是否开启，DDL都会由server自发做提交（implicit commit）。若为DML，则若autocommit为1，自动提交，autocommit为0，则由用户手动提交。若为Begin语句，不论autocommit是否开启，都不提交。<br>
（7）对于需要提交的事务，如果是DML，会在trx_cache结尾append一个Xid_log_event。随后进入ordered_commit。<br>
（8）进入flush阶段，change_stage将线程入队。然后由leader执行process_flush_stage_queue，这里先刷innobase层的日志，也就是刷redo(innobase_flush_logs)，如果innodb_flush_log_at_trx_commit为1，则这里将redo落盘。<br>
（9）assign_automatic_gtids_to_flush_group为每个thd生成GTID。<br>
（10）flush_thread_caches，首先将上一步生成的GTID写到全局Binlog中，然后将局部binlog刷到全局Binlog，此时数据还在IO_CACHE结构中。thd的binlog_cache_mngr管理两种局部Binlog event缓存：stmt_cache和trx_cache，前者记录非事务性Binlog，后者记录事务型Binlog。XA事务中只有trx_cache有数据。所有局部Binlog flush完后判断是否需要rotate，若需要，将在ordered_commit最后完成。<br>
（11）flush_cache_to_file将IO_CACHE中Binlog write到文件。<br>
（12）进入sync阶段，sync_period用于控制sync的周期，比如经过几次flush后做一次sync。sync_binlog_file将Binlog落盘。由配置参数sync_binlog控制。<br>
（13）如果sync_period为1，则sync_binlog_file完更新atomic_binlog_end_pos，这个参数标识binlog结尾。如果sync_period不为1，则flush完就更新atomic_binlog_end_pos。<br>
（14）进入commit阶段，该阶段主要执行finish_commit，如果opt_binlog_order_commits==false，那么事务就不按照之前的顺序，各自进行提交(finish_commit)，这种情况下不能保证innodb commit顺序和binlog写入顺序一致，这不会影响到数据一致性，在高并发场景下还能提升一定的吞吐量。但可能影响到物理备份的数据一致性，例如xtrabackup（而不是基于其上的innobackup脚本）依赖于事务页上记录的binlog的end位点（flush_thread_caches会更新），如果位点发生乱序，就会导致备份的数据不一致。<br>
（15）执行finish_commit， update_max_committed更新最大commit事务的序号。<br>
（16）分别执行Binlog和innodb的commit。Binlog Commit在前面已经完成了，所以这里什么也不做。实际只有Innodb的Commit。<br>
（17）dec_prep_xids: 清除 m_atomic_prep_xids，rotate Binlog时通过它判断当前Binlog是否有正在提交的事务。<br>
（18）将commit事务的GTID加入executed_gtids。<br>
（19）在第10步判断的，如果Binlog文件大小超过了max_binlog_size，则会rotate新的Binlog。</p>
</li>
<li>
<p>Innodb Commit<br>
入口：innobase_commit<br>
将undo头的状态修改为TRX_UNDO_CACHED或TRX_UNDO_TO_FREE或TRX_UNDO_TO_PURGE (undo相关知识参阅之前的月报)；并释放事务锁，清理读写事务链表、readview等一系列操作。每个事务在commit阶段也会去更新事务页的binlog位点。然后根据该session已执行的GTID去更新全局GTID SET。在8.0.17版本会将GTID持久化到undo日志中（原因）。</p>
</li>
</ul>
<h3 id="写入效率">写入效率</h3>
<p>本节介绍Binlog写入之前，先介绍IO_CACHE结构，该结构贯穿了任何与Binlog相关文件（index文件，purge_index_file, crash_safe_index_file等）的读写过程，随后介绍XA过程中局部的Binlog和全局的Binlog。最后介绍仍然存在的性能瓶颈和解决方案。</p>
<h4 id="io_cache">IO_CACHE</h4>
<p>文件系统虽然向上呈现一段连续的空间，但是其内部以页的形式管理，页的大小通常为4K，满足4K对齐的读写对文件系统的性能会有很大的提高。而IO_CACHE的作用就是充当一层缓存，将连续的数据写入进行4K对齐后写入文件系统。<br>
知道了IO_CACHE的作用后，来看看其Binlog是如何利用它的。Binlog文件初始化的过程如下：</p>
<pre><code>class IO_CACHE_ostream {
    bool IO_CACHE_ostream::open() {
        file = mysql_file_open(log_file_key, file_name, O_CREAT | O_WRONLY, MYF(MY_WME));
        init_io_cache(&amp;m_io_cache, file, cache_size, WRITE_CACHE, 0, 0, flags);
    }
    IO_CACHE m_io_cache;
}

init_io_cache
|
----&gt;init_io_cache_ext(){
    info-&gt;file = file;
    info-&gt;buffer = (uchar *)my_malloc(key_memory_IO_CACHE, buffer_block, flags);
    init_functions(info);
}
</code></pre>
<p>可以看出MySQL在打开Binlog文件后将文件描述符交给IO_CACHE结构管理，IO_CACHE初始化过程中，会申请一个缓冲，默认大小是8K，随后计算读写缓冲区的位点以便对齐写入，还定义了对IO_CACHE的读写函数。IO_CACHE详见 <a href="https://www.bookstack.cn/read/aliyun-rds-core/c6c31b03cde784a5.md">IO_CACHE源码解析</a>。如图7所示，IO_CACHE会对齐PageCache进行写入，满足对齐条件后就会刷到Page Cache中，之后sync到Binlog文件。<br>
<img src="https://phantomma.top/post-images/1704382596749.png" alt="" loading="lazy"><br>
图7 数据写IO_CACHE的过程</p>
<h4 id="局部binlog和全局binlog">局部Binlog和全局Binlog</h4>
<p>Binlog中的事务是顺序独立的，不能交错，原因是交错的Binlog事务无法被slave重放。但是多个客户端连接MySQL，并对其并发写入的场景经常出现。为了解决高并发过程中顺序写入的问题，MySQL为每个连接都配置了一个局部的Binlog文件，各个连接产生的Binlog会事先写到各个局部Binlog中，等到group commit时再将各个局部Binlog合并到全局Binlog文件中。</p>
<ul>
<li>
<p>局部Binlog<br>
局部Binlog通过Binlog_cache_storage结构管理，实际上也是对IO_CACHE结构的包裹，可以通过binlog_cache_size来控制它的大小，如果事务的binlog日志大小超出了binlog_cache_size的定义的大小，多出来的部分会存在临时文件中，但是事务总大小不能超过max_binlog_cache_size。上面我们说到IO_CACHE在初始化的时候会关联一个磁盘文件，这里也不例外，但是这里特殊在是临时文件，通过下面这个函数创建。</p>
<pre><code>bool real_open_cached_file(IO_CACHE *cache) {
if ((cache-&gt;file = mysql_file_create_temp(
        cache-&gt;file_key, name_buff, cache-&gt;dir, cache-&gt;prefix,
        (O_RDWR | O_TRUNC), MYF(MY_WME))) &gt;= 0) {
    error = 0;
    /*
    Remove an open tempfile so that it doesn't survive
    if we crash.
    */
    (void)my_delete(name_buff, MYF(MY_WME));
}
}
</code></pre>
<p>该文件以“ML”为前缀，如果创建成功，则会被立刻删除，但是由于文件的描述符并没有被释放，所以该文件依然能被读写，当程序crash后，该文件会被真正的删除。由于其中保留的数据未提交，所以重启后无需恢复，其实删除就是最好的恢复。</p>
</li>
<li>
<p>全局Binlog<br>
全局Binlog是当前打开的Binlog，在MySQL启动时构造在m_binlog_file变量中，管理Binlog写入流。其底层依然是通过IO_CACHE管理文件写入的。</p>
</li>
<li>
<p>group commit<br>
在SQL执行的过程中，Binlog会伴随着产生并写入到局部Binlog中，在xa事务提交时，局部Binlog中的事务会被顺序拷贝到全局Binlog中。关于Group Commit可以参考4.1.3和MySQL组提交，图8展示了Binlog Event写入局部Binlog，并在提交时由局部Binlog拷贝至全局Binlog的过程。</p>
<pre><code>/*将局部Binlog拷贝到全局Binlog*/
bool MYSQL_BIN_LOG::do_write_cache(Binlog_cache_storage *cache,
                                Binlog_event_writer *writer) {
    cache-&gt;copy_to(writer, &amp;error)
}
</code></pre>
<p><img src="https://phantomma.top/post-images/1704382655884.png" alt="" loading="lazy"><br>
图8 Binlog Event写入局部Binlog和全局Binlog</p>
</li>
</ul>
<h4 id="性能问题">性能问题</h4>
<p>开启Binlog后的性能一直被诟病，对于AWS Aurora在开启Binlog后，通常有50%到60%的性能损耗；对于PolarDB也差不多是这个数值；Oracle则不写入Binlog，而是通过物理redo日志去生成Binlog。即使Binlog会带来如此严重的性能问题，但它仍然在业务中不可或缺的。所以数据库厂商采取了一些方法去解决这些问题。本节将介绍PolarDB和Aurora是如何提升Binlog性能的。</p>
<ul>
<li>
<p>PolarDB<br>
考虑到在Group Commit的过程中，在flush阶段redo会被sync到磁盘，在sync阶段Binlog会被sync到磁盘，这两个过程是串行的，两次对云盘的写入会造成很大的性能损耗，所以PolarDB采用logic redo的方法，将Binlog数据记录到redo日志中，在sync阶段，将redo和Binlog一起刷盘。关于Logic的详细介绍，可以移步 <a href="https://www.bookstack.cn/read/aliyun-rds-core/6250fee9592d8cff.md">Logic redo</a>。</p>
</li>
<li>
<p>Aurora<br>
Aurora面临的问题一样，瓶颈依然在全局Binlog sync到磁盘的时候。但是它的切入点和PolarDB不同，它的做法是enhance Binlog：将全局Binlog的sync过程打散到SQL执行的过程中，将局部Binlog下推到存储节点，这样在SQL执行过程中，Binlog就向存储节点写入，等到最后提交时，只需要存储节点对这些局部Binlog进行合并即可。详见 <a href="https://zhuanlan.zhihu.com/p/590576660">AWS re:Invent2022 Aurora 发布了啥</a></p>
</li>
</ul>
<h2 id="binlog-recover">Binlog Recover</h2>
<p>说完了Binlog写入过程，很容易想到如果写入过程中程序崩溃了怎么办，所以下面将介绍Binlog的Recover过程。Recover是基于xa过程的，本质上是根据已经落盘的Binlog决定如何处理未提交的事务。因为rotate新的Binlog时会recover老的Binlog中所有事务，因此在Binlog启动时，只需对最新的Binlog文件执行Recover即可，对于某个事务而言，如果它记录的Binlog是完整的（关于完整的Binlog事务参考第三章小结部分），说明它可以提交，反之，如果缺失任意一条Event都是不完整的Binlog，不完整的Binlog会被删除，与之关联的事务（binlog事务，innodb的事务）都会回滚。</p>
<ul>
<li>
<p>XID<br>
在前面介绍Binlog事务和XA过程的时候，可知每个Binlog事务都有个对应XID，对于非DDL Binlog事务，XID会以XID_EVENT的类型在事务提交时写到事务结尾；对于DDL事务，XID包含在DDL所在的QUERY_EVENT里。这个XID其实是xa事务的id，唯一标识每个xa事务。在xa过程的Innodb prepare时，会设置事务的状态为prepare，并记录在undo page中。这样Binlog recover时候根据xid能去Innodb层找哪些事务是prepare状态的，对这些事务提交或回滚。</p>
</li>
<li>
<p>recover<br>
入口：int MYSQL_BIN_LOG::open_binlog<br>
打开index 文件中最后一个 binlog，若该文件没有正常关闭（LOG_EVENT_BINLOG_IN_USE_F 置位），则recover它。从头开始，挨个扫描每个Binlog Event，只要发现某个Binlog事务不完整，那么该Binlog和其后面的Binlog都会被truncate。前面完整的Binlog的事务依据它们的xid去innodb层提交，其他事务进行回滚。原因是事务的binlog已经完整落盘，所以redolog也落盘了，该事务是可提交的。至此Binlog的recover完成，但是为了体现Binlog是参与者，之后会调用空函数binlog_dummy_recover()，该函数为空，因而后续的也不会调用commit和rollback函数。实际只进行innodb的recover。<br>
innodb的recover函数为innobase_xa_recover()，函数的主要目的是找到innobase层所有prepare状态的事务，这些事务的XID与前面Binlog找到的XID进行比对，从而决定哪些需要回滚innobase_rollback_by_xid，哪些需要提交innobase_commit_by_xid。提交和回滚可参考innodb事务系统。<br>
最后，Binlog会truncate到保留最后一个完整的事务，清除LOG_EVENT_BINLOG_IN_USE_F，表示binlog文件正常关闭，并rotate出一个新的Binlog进行写入。<br>
recover讲完，primary上的Binlog基本讲完了，下面将介绍Binlog是如何完成数据同步的——Binlog复制。</p>
</li>
</ul>
<h2 id="binlog复制">Binlog复制</h2>
<p>首先需要建立连接，MySQL将对应的连接称为channel，slave节点通过change master指令可以与master建立一个channel并对其命名，change master指令可以指定复制开始文件和位点。随后由slave发起start slave开启复制，slave可以为所有channel都开启复制（start slave）,也可以只为特定的channel开启复制（start slave for channel ‘channel_1’），开启复制是通过建立复制Binlog的IO线程和对其回放的SQL线程，本文不讨论SQL线程。下面来看看连接建立与复制过程，如图9所示。<br>
<img src="https://phantomma.top/post-images/1704382764408.png" alt="" loading="lazy"><br>
图9 连接建立与复制过程</p>
<p>（1）这里根据设置的thread_mask会启动相应的线程：handle_slave_io或handle_slave_sql线程。这里介绍handle_slave_io线程。handle_slave_sql是slave回放binlog的线程，执行完线程启动后，在handle_slave_io线程初始化完并被加到thd_manager后，客户端就能收到该指令执行的响应了。连接master的操作在后续执行。<br>
（2）slave通过safe_connect-&gt;connect_to_master与master建立连接。<br>
（3）slave向master发送COM_REGISTER_SLAVE指令，在master端register_slave，检查slave的权限，将其serverid、host、user、passwd等信息放在slave_list结构中。<br>
（4）slave发起dump请求，command是COM_BINLOG_DUMP，若开启gtid和auto_position(GTID Auto-Positioning)，通过设置gtid_mode=ON和在change master时指定MASTER_AUTO_POSITION=1，则使用GTID复制，command是COM_BINLOG_DUMP_GTID，区别是后者会发送slave上的m_exclude_gtid（slave上已有的Binlog事务gtid集合），master只会复制不在该gtid集合中的Binlog事务。<br>
（5）read_event调用mysql_binlog_fetch读取从master发来的packet，阻塞等待。<br>
（6）master响应request_dump()发起的COM_BINLOG_DUMP(_GTID)请求，如图10所示。<br>
- 首先初始化Binlog sender，如果slave未指定复制起始位点（change master指令可指定位点，还有是位点会保存在master.info文件，由slave启动时读取。），则在sender init时初始化位点为index文件中第一个binlog文件的第一个event位置（pos=4）。<br>
- 然后打开Binlog文件send_binlog，在该函数中：1）函数get_binlog_end_pos会判断当前正在复制的Binlog文件是否和全局Binlog文件相同，如果不同说明该文件不是最后一个Binlog，复制完该文件后需要rotate到下一个继续复制。如果相同，则复制完后会等待该Binlog文件中新的写入（end_pos更新）。2）函数send_events()一次读一个完整的event并发送，如果开启GTID和auto_position，则不发送gtid包含在m_exclude_gtid中的事务。如果没有event发送，则会等待超时并发送heartbeat event，heatbeat还可用于告知slave：master复制位点；<br>
<img src="https://phantomma.top/post-images/1704382799005.png" alt="" loading="lazy"><br>
图10 master响应com_binlog_dump</p>
<p>（7）来自master的Binlog会被存储到slave的relay log中，通常slave的Binlog被称为relay log。后续SQL线程会解析并应用relay log。</p>
<h2 id="杂记binlog相关文件">杂记——Binlog相关文件</h2>
<h3 id="binlog文件">Binlog文件</h3>
<p><img src="https://phantomma.top/post-images/1704382819686.png" alt="" loading="lazy"><br>
图11 Binlog文件</p>
<p>Binlog文件命名由log_bin_log或log-bin指定，这里假定为binlog，后面的例子中也一样，如图11所示，写入方式为顺序追加写。通过mysqlbinlog可以查看文件内容。</p>
<h3 id="index文件">Index文件</h3>
<p>每行都是Binlog文件名。如图12所示。<br>
<img src="https://phantomma.top/post-images/1704382841986.png" alt="" loading="lazy"><br>
图12 Binlog Index文件内容</p>
<h3 id="crash_safe_index_file">crash_safe_index_file</h3>
<p>临时文件，内容为Binlog文件名。保证了修改index文件时，写入的Binlog文件名是原子的，图13是在index文件中写文件名的一个例子，可以很容易看出crash_safe_index_file的功能。该文件命名方式为：./binlog.index_crash_safe<br>
<img src="https://phantomma.top/post-images/1704382861777.png" alt="" loading="lazy"><br>
图13 在index文件中写文件名的过程</p>
<h3 id="purge_index_file">purge_index_file</h3>
<p>临时文件，内容为Binlog文件名。保证index内容和Binlog文件互相匹配。图14展示的是新建Binlog文件时需要在purge_index_file文件中写入一条新的文件名，在Binlog文件创建完成后将该Binlog文件名写入index文件，随后删除purge_index_file。由此可见该文件中记录的Binlog文件名都是在创建过程中并且还未来得及被记录在index文件的Binlog文件，所以每次打开index文件时，会检查并删除purge_index_file中记录的Binlog文件，避免index文件和Binlog文件不匹配。该文件的命名方式为：./binlog.<sub>rec</sub><br>
<img src="https://phantomma.top/post-images/1704382882905.png" alt="" loading="lazy"><br>
图14 新建Binlog文件过程</p>
<hr>
<ul>
<li><a href="https://www.bookstack.cn/read/aliyun-rds-core/30b78cf21d82e781.md">物理复制解读</a></li>
<li><a href="https://dbadiaries.com/mysql-replication-events-statement-versus-row-based-formats">MySQL Replication Events – Statement versus Row-Based Formats</a></li>
<li><a href="https://dev.mysql.com/doc/dev/mysql-server/latest/page_protocol_replication_binlog_event.html">Mysql Binlog Event</a></li>
<li><a href="http://mysql.taobao.org/monthly/2015/12/01/">InnoDB 事务子系统介绍</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[系统压力测试工具之 sysbench]]></title>
        <id>https://phantomma.top/post/xi-tong-ya-li-ce-shi-gong-ju-zhi-sysbench/</id>
        <link href="https://phantomma.top/post/xi-tong-ya-li-ce-shi-gong-ju-zhi-sysbench/">
        </link>
        <updated>2016-12-29T14:08:15.000Z</updated>
        <content type="html"><![CDATA[<h2 id="sysbench">sysbench</h2>
<p>是一个模块化的、跨平台、多线程、流行的开源基准测试工具，主要用于评估测试各种不同系统参数下的数据库负载情况</p>
<p>安装<br>
软件版本:  <code>sysbench-1.0.20-6.el7.x86_64</code></p>
<pre><code>curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.rpm.sh | sudo bash
yum -y install sysbench
</code></pre>
<h3 id="命令选项">命令选项</h3>
<p>sysbench --help</p>
<pre><code>--time  # 压测时间
--threads  # 压测线程数
--events  # 请求数, 0 无限制
--rate    # 请求速率, 0 无限制
--tables  # 压测表数量
--table_size # 压测的单表大小,单位行
prepare   # 准备测试数据
run        # 开始压测
cleanup  # 清除压测数据
report-interval # 每多少秒钟报告一次测试结果
</code></pre>
<h3 id="file-io-压测">File IO 压测</h3>
<pre><code># 查看 fileio 测试模块下的帮助信息
sysbench fileio help
# 准备测试数据, 生成多个测试文件
sysbench fileio --file-total-size=5G prepare
# 运行测试
sysbench fileio --file-total-size=5G --file-test-mode=rndrw --time=30 --events=0 run
# 清除测试数据
sysbench fileio --file-total-size=5G cleanup
</code></pre>
<p>测试结果如下:</p>
<pre><code>sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time

Extra file open flags: (none)
128 files, 40MiB each
5GiB total file size
Block size 16KiB
Number of IO requests: 0
Read/Write ratio for combined random IO test: 1.50
Periodic FSYNC enabled, calling fsync() each 100 requests.
Calling fsync() at the end of test, Enabled.
Using synchronous I/O mode
Doing random r/w test
Initializing worker threads...

Threads started!

File operations:    # 磁盘io操作
    reads/s:                      47.68
    writes/s:                     31.79
    fsyncs/s:                     104.27
Throughput: # 磁盘吞吐量
    read, MiB/s:                  0.75
    written, MiB/s:               0.50
General statistics: # 测试时间30s, 总请求数 5421
    total time:                          30.1950s
    total number of events:              5421
Latency (ms):   # 延迟
         min:                                    0.01
         avg:                                    5.53
         max:                                  181.94
         95th percentile:                       20.74
         sum:                                29988.08
Threads fairness:
    events (avg/stddev):           5421.0000/0.00
    execution time (avg/stddev):   29.9881/0.00
</code></pre>
<h3 id="mysql-压测">MySQL 压测</h3>
<p>读写测试: 3 个表、每个表 1000 行，测试时间 120s, 请求数及请求频率无限制，12 个线程 首次测试 准备测试数据</p>
<pre><code>sysbench --test=/usr/share/sysbench/oltp_read_write.lua \
--mysql-host=127.0.0.1 --mysql-port=3306 --mysql-user=root --mysql-password=root \
--mysql-db=db01 --db-driver=mysql --report-interval=30 \
--time=120 --threads=12 --events=0 --rate=0 --table_size=1000 --tables=3 \
prepare

# 输出如下
Creating table 'sbtest3'...
Creating table 'sbtest1'...
Creating table 'sbtest2'...
Inserting 1000 records into 'sbtest3'
Inserting 1000 records into 'sbtest2'
Inserting 1000 records into 'sbtest1'
Creating a secondary index on 'sbtest1'...
Creating a secondary index on 'sbtest2'...
Creating a secondary index on 'sbtest3'...
</code></pre>
<h3 id="开始压测">开始压测</h3>
<pre><code>sysbench --test=/usr/share/sysbench/oltp_read_write.lua \
--mysql-host=127.0.0.1 --mysql-port=3306 --mysql-user=root --mysql-password=root \
--mysql-db=db01 --db-driver=mysql --report-interval=30 \
--time=120 --threads=12 --events=0 --rate=0 --table_size=1000 --tables=3 \
run

# 输出如下
[ 30s ] thds: 12 tps: 15.73 qps: 326.69 (r/w/o: 230.50/63.99/32.20) lat (ms,95%): 1771.29 err/s: 0.33 reconn/s: 0.00
[ 60s ] thds: 12 tps: 17.90 qps: 366.32 (r/w/o: 257.61/72.40/36.30) lat (ms,95%): 1678.14 err/s: 0.50 reconn/s: 0.00
[ 90s ] thds: 12 tps: 17.13 qps: 345.72 (r/w/o: 242.66/68.60/34.47) lat (ms,95%): 1618.78 err/s: 0.20 reconn/s: 0.00
[ 120s ] thds: 12 tps: 17.37 qps: 356.20 (r/w/o: 250.13/70.83/35.23) lat (ms,95%): 1561.52 err/s: 0.50 reconn/s: 0.00
SQL statistics:
    queries performed:
        read:                            29428
        write:                           8292
        other:                           4158
        total:                           41878
    transactions:                        2056   (17.04 per sec.)
    queries:                             41878  (346.99 per sec.)
    ignored errors:                      46     (0.38 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          120.6877s
    total number of events:              2056

Latency (ms):
         min:                                  132.72
         avg:                                  702.06
         max:                                 3397.70
         95th percentile:                     1678.14
         sum:                              1443438.80

Threads fairness:
    events (avg/stddev):           171.3333/9.13
    execution time (avg/stddev):   120.2866/0.21
</code></pre>
<h3 id="清除压测数据">清除压测数据</h3>
<pre><code>sysbench --test=/usr/share/sysbench/oltp_read_write.lua \
--mysql-host=127.0.0.1 --mysql-port=3306 --mysql-user=root --mysql-password=root \
--mysql-db=db01 --db-driver=mysql --report-interval=30 \
--time=120 --threads=12 --events=0 --rate=0 --table_size=1000 --tables=3 \
cleanup
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[frp 内网穿透]]></title>
        <id>https://phantomma.top/post/frp-nei-wang-chuan-tou/</id>
        <link href="https://phantomma.top/post/frp-nei-wang-chuan-tou/">
        </link>
        <updated>2016-05-06T13:12:26.000Z</updated>
        <content type="html"><![CDATA[<figure data-type="image" tabindex="1"><img src="https://phantomma.top/post-images/1701436417023.png" alt="" loading="lazy"></figure>
<h1 id="config-server-side">config server side</h1>
<h2 id="login-into-vps">login into vps</h2>
<p>ssh root@155.254.193.168</p>
<pre><code class="language-bash">cat frps.ini 
[common]
bind_port = 7000
token = areyouok

dashboard_port = 7500
dashboard_user = maksim
dashboard_pwd = zxc123123
</code></pre>
<h2 id="config-auto-start-in-deamon-mod">config auto start in deamon mod</h2>
<p>ubuntu or centos both support the configruation like this</p>
<pre><code class="language-bash">cat /etc/systemd/system/frps.service
[Unit]
Description=frps daemon
After=syslog.target network.target
Wants=network.target

[Service]
Type=simple
ExecStart=/root/frp/frps -c /root/frp/frps.ini
Restart= always
RestartSec=1min

[Install]
WantedBy=multi-user.target
</code></pre>
<h1 id="config-client-side">config client side</h1>
<pre><code class="language-bash">[common]
server_addr = some_ip
server_port = 7000
token = some_secret_string

[ssh]
type = tcp
local_ip = 127.0.0.1 
local_port = 22
remote_port = 6000
</code></pre>
<pre><code class="language-bash">cat /etc/systemd/system/frpc.service
[Unit]
Description=frpc daemon
After=syslog.target network.target
Wants=network.target

[Service]
Type=simple
ExecStart=/home/maxbee/frp/frpc -c /home/maxbee/frp/frpc.ini
Restart= always
RestartSec=1min
ExecStop=/usr/bin/killall frpc

[Install]
WantedBy=multi-user.target

</code></pre>
<hr>
<ul>
<li><a href="https://xenojoshua.com/posts/2020/10/frp">使用frp内网穿透进行ssh登录</a></li>
<li><a href="https://gofrp.org/docs/examples/ssh/">通过 SSH 访问内网机器</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kafka 体系架构分解]]></title>
        <id>https://phantomma.top/post/kafka-ti-xi-jia-gou-fen-jie/</id>
        <link href="https://phantomma.top/post/kafka-ti-xi-jia-gou-fen-jie/">
        </link>
        <updated>2015-12-03T09:14:59.000Z</updated>
        <content type="html"><![CDATA[<h1 id="基本概念">基本概念</h1>
<h2 id="kafka-体系架构">Kafka 体系架构</h2>
<p>Kafka 体系架构包括若干 Producer、若干 Broker、若干 Consumer，以及一个 ZooKeeper 集群。<br>
<img src="https://phantomma.top/post-images/1701595124651.png" alt="" loading="lazy"></p>
<p>在 Kafka 中还有两个特别重要的概念—主题（Topic）与分区（Partition）。</p>
<p>Kafka 中的消息以主题为单位进行归类，生产者负责将消息发送到特定的主题（发送到 Kafka 集群中的每一条消息都要指定一个主题），而消费者负责订阅主题并进行消费。</p>
<p>主题是一个逻辑上的概念，它还可以细分为多个分区，一个分区只属于单个主题，很多时候也会把分区称为主题分区（Topic-Partition）。</p>
<p>Kafka 为分区引入了多副本（Replica）机制，通过增加副本数量可以提升容灾能力。同一分区的不同副本中保存的是相同的消息（在同一时刻，副本之间并非完全一样），副本之间是“一主多从”的关系，其中 leader 副本负责处理读写请求，follower 副本只负责与 leader 副本的消息同步。当 leader 副本出现故障时，从 follower 副本中重新选举新的 leader 副本对外提供服务。<br>
<img src="https://phantomma.top/post-images/1701595156729.png" alt="" loading="lazy"></p>
<p>如上图所示，Kafka 集群中有4个 broker，某个主题中有3个分区，且副本因子（即副本个数）也为3，如此每个分区便有1个 leader 副本和2个 follower 副本。</p>
<h2 id="数据同步">数据同步</h2>
<p>分区中的所有副本统称为 AR（Assigned Replicas）。所有与 leader 副本保持一定程度同步的副本（包括 leader 副本在内）组成ISR（In-Sync Replicas），ISR 集合是 AR 集合中的一个子集。</p>
<p>与 leader 副本同步滞后过多的副本（不包括 leader 副本）组成 OSR（Out-of-Sync Replicas），由此可见，AR=ISR+OSR。在正常情况下，所有的 follower 副本都应该与 leader 副本保持一定程度的同步，即 AR=ISR，OSR 集合为空。</p>
<p>Leader 副本负责维护和跟踪 ISR 集合中所有 follower 副本的滞后状态，当 follower 副本落后太多或失效时，leader 副本会把它从 ISR 集合中剔除。默认情况下，当 leader 副本发生故障时，只有在 ISR 集合中的副本才有资格被选举为新的 leader。</p>
<p>HW 是 High Watermark 的缩写，俗称高水位，它标识了一个特定的消息偏移量（offset），消费者只能拉取到这个 offset 之前的消息。<br>
LEO 是 Log End Offset 的缩写，它标识当前日志文件中下一条待写入消息的 offset。<br>
<img src="https://phantomma.top/post-images/1701595178455.png" alt="" loading="lazy"></p>
<p>如上图所示，第一条消息的 offset（LogStartOffset）为0，最后一条消息的 offset 为8，offset 为9的消息用虚线框表示，代表下一条待写入的消息。日志文件的 HW 为6，表示消费者只能拉取到 offset 在0至5之间的消息，而 offset 为6的消息对消费者而言是不可见的。</p>
<h2 id="kafka生产者客户端的整体结构">Kafka生产者客户端的整体结构</h2>
<figure data-type="image" tabindex="1"><img src="https://phantomma.top/post-images/1701595199487.png" alt="" loading="lazy"></figure>
<p>整个生产者客户端由两个线程协调运行，这两个线程分别为主线程和 Sender 线程（发送线程）。</p>
<p>在主线程中由 KafkaProducer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（RecordAccumulator，也称为消息收集器）中。Sender 线程负责从 RecordAccumulator 中获取消息并将其发送到 Kafka 中。</p>
<h3 id="recordaccumulator">RecordAccumulator</h3>
<p>RecordAccumulator 主要用来缓存消息以便 Sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性能。</p>
<p>主线程中发送过来的消息都会被追加到 RecordAccumulator 的某个双端队列（Deque）中，在 RecordAccumulator 的内部为每个分区都维护了一个双端队列。</p>
<p>消息写入缓存时，追加到双端队列的尾部；Sender 读取消息时，从双端队列的头部读取。</p>
<p>Sender 从 RecordAccumulator 中获取缓存的消息之后，会进一步将原本&lt;分区, Deque&lt; ProducerBatch&gt;&gt; 的保存形式转变成 &lt;Node, List&lt; ProducerBatch&gt; 的形式，其中 Node 表示 Kafka 集群的 broker 节点。</p>
<p>KafkaProducer 要将此消息追加到指定主题的某个分区所对应的 leader 副本之前，首先需要知道主题的分区数量，然后经过计算得出（或者直接指定）目标分区，之后 KafkaProducer 需要知道目标分区的 leader 副本所在的 broker 节点的地址、端口等信息才能建立连接，最终才能将消息发送到 Kafka。</p>
<p>所以这里需要一个转换，对于网络连接来说，生产者客户端是与具体的 broker 节点建立的连接，也就是向具体的 broker 节点发送消息，而并不关心消息属于哪一个分区。</p>
<h3 id="inflightrequests">InFlightRequests</h3>
<p>请求在从 Sender 线程发往 Kafka 之前还会保存到 InFlightRequests 中，InFlightRequests 保存对象的具体形式为 Map&lt;NodeId, Deque&gt;，它的主要作用是缓存了已经发出去但还没有收到响应的请求（NodeId 是一个 String 类型，表示节点的 id 编号）。</p>
<h2 id="拦截器">拦截器</h2>
<p>生产者拦截器既可以用来在消息发送前做一些准备工作，比如按照某个规则过滤不符合要求的消息、修改消息的内容等，也可以用来在发送回调逻辑前做一些定制化的需求，比如统计类工作。</p>
<p>生产者拦截器的使用也很方便，主要是自定义实现 org.apache.kafka.clients.producer. ProducerInterceptor 接口。ProducerInterceptor 接口中包含3个方法：</p>
<pre><code>public ProducerRecord&lt;K, V&gt; onSend(ProducerRecord&lt;K, V&gt; record);
public void onAcknowledgement(RecordMetadata metadata, Exception exception);
public void close();
</code></pre>
<p>KafkaProducer 在将消息序列化和计算分区之前会调用生产者拦截器的 onSend() 方法来对消息进行相应的定制化操作。一般来说最好不要修改消息 ProducerRecord 的 topic、key 和 partition 等信息。</p>
<p>KafkaProducer 会在消息被应答（Acknowledgement）之前或消息发送失败时调用生产者拦截器的 onAcknowledgement() 方法，优先于用户设定的 Callback 之前执行。这个方法运行在 Producer 的I/O线程中，所以这个方法中实现的代码逻辑越简单越好，否则会影响消息的发送速度。</p>
<p>close() 方法主要用于在关闭拦截器时执行一些资源的清理工作。</p>
<h2 id="序列化器">序列化器</h2>
<p>生产者需要用序列化器（Serializer）把对象转换成字节数组才能通过网络发送给 Kafka。而在对侧，消费者需要用反序列化器（Deserializer）把从 Kafka 中收到的字节数组转换成相应的对象。</p>
<p>生产者使用的序列化器和消费者使用的反序列化器是需要一一对应的，如果生产者使用了某种序列化器，比如 StringSerializer，而消费者使用了另一种序列化器，比如 IntegerSerializer，那么是无法解析出想要的数据的。</p>
<p>序列化器都需要实现org.apache.kafka.common.serialization.Serializer 接口，此接口有3个方法：</p>
<pre><code>public void configure(Map&lt;String, ?&gt; configs, boolean isKey)
public byte[] serialize(String topic, T data)
public void close()
</code></pre>
<p>configure() 方法用来配置当前类，serialize() 方法用来执行序列化操作。而 close() 方法用来关闭当前的序列化器。</p>
<p>如下：</p>
<pre><code>public class StringSerializer implements Serializer&lt;String&gt; {
    private String encoding = &quot;UTF8&quot;;

    @Override
    public void configure(Map&lt;String, ?&gt; configs, boolean isKey) {
        String propertyName = isKey ? &quot;key.serializer.encoding&quot; :
                &quot;value.serializer.encoding&quot;;
        Object encodingValue = configs.get(propertyName);
        if (encodingValue == null)
            encodingValue = configs.get(&quot;serializer.encoding&quot;);
        if (encodingValue != null &amp;&amp; encodingValue instanceof String)
            encoding = (String) encodingValue;
    }

    @Override
    public byte[] serialize(String topic, String data) {
        try {
            if (data == null)
                return null;
            else
                return data.getBytes(encoding);
        } catch (UnsupportedEncodingException e) {
            throw new SerializationException(&quot;Error when serializing &quot; +
                    &quot;string to byte[] due to unsupported encoding &quot; + encoding);
        }
    }

    @Override
    public void close() {
        // nothing to do
    }
}
</code></pre>
<p>configure() 方法，这个方法是在创建 KafkaProducer 实例的时候调用的，主要用来确定编码类型。</p>
<p>serialize用来编解码，如果 Kafka 客户端提供的几种序列化器都无法满足应用需求，则可以选择使用如 Avro、JSON、Thrift、ProtoBuf 和 Protostuff 等通用的序列化工具来实现，或者使用自定义类型的序列化器来实现。</p>
<h2 id="分区器">分区器</h2>
<p>消息经过序列化之后就需要确定它发往的分区，如果消息 ProducerRecord 中指定了 partition 字段，那么就不需要分区器的作用，因为 partition 代表的就是所要发往的分区号。</p>
<p>如果消息 ProducerRecord 中没有指定 partition 字段，那么就需要依赖分区器，根据 key 这个字段来计算 partition 的值。分区器的作用就是为消息分配分区。</p>
<p>Kafka 中提供的默认分区器是 org.apache.kafka.clients.producer.internals.DefaultPartitioner，它实现了 org.apache.kafka.clients.producer.Partitioner 接口，这个接口中定义了2个方法，具体如下所示。</p>
<pre><code>public int partition(String topic, Object key, byte[] keyBytes, 
                     Object value, byte[] valueBytes, Cluster cluster);
public void close();
</code></pre>
<p>其中 partition() 方法用来计算分区号，返回值为 int 类型。partition() 方法中的参数分别表示主题、键、序列化后的键、值、序列化后的值，以及集群的元数据信息，通过这些信息可以实现功能丰富的分区器。close() 方法在关闭分区器的时候用来回收一些资源。</p>
<p>在默认分区器 DefaultPartitioner 的实现中，close() 是空方法，而在 partition() 方法中定义了主要的分区分配逻辑。如果 key 不为 null，那么默认的分区器会对 key 进行哈希，最终根据得到的哈希值来计算分区号，拥有相同 key 的消息会被写入同一个分区。如果 key 为 null，那么消息将会以轮询的方式发往主题内的各个可用分区。</p>
<p>自定义的分区器，只需同 DefaultPartitioner 一样实现 Partitioner 接口即可。由于每个分区下的消息处理都是有顺序的，我们可以利用自定义分区器实现在某一系列的key都发送到一个分区中，从而实现有序消费。</p>
<h2 id="broker">Broker</h2>
<h3 id="broker处理请求流程">Broker处理请求流程</h3>
<figure data-type="image" tabindex="2"><img src="https://phantomma.top/post-images/1701595374561.png" alt="" loading="lazy"></figure>
<p>在Kafka的架构中，会有很多客户端向Broker端发送请求，Kafka 的 Broker 端有个 SocketServer 组件，用来和客户端建立连接，然后通过Acceptor线程来进行请求的分发，由于Acceptor不涉及具体的逻辑处理，非常得轻量级，因此有很高的吞吐量。</p>
<p>接着Acceptor 线程采用轮询的方式将入站请求公平地发到所有网络线程中，网络线程池默认大小是 3个，表示每台 Broker 启动时会创建 3 个网络线程，专门处理客户端发送的请求，可以通过Broker 端参数 num.network.threads来进行修改。</p>
<p>那么接下来处理网络线程处理流程如下：<br>
<img src="https://phantomma.top/post-images/1701595387666.png" alt="" loading="lazy"></p>
<p>当网络线程拿到请求后，会将请求放入到一个共享请求队列中。Broker 端还有个 IO 线程池，负责从该队列中取出请求，执行真正的处理。如果是 PRODUCE 生产请求，则将消息写入到底层的磁盘日志中；如果是 FETCH 请求，则从磁盘或页缓存中读取消息。</p>
<p>IO 线程池处中的线程是执行请求逻辑的线程，默认是8，表示每台 Broker 启动后自动创建 8 个 IO 线程处理请求，可以通过Broker 端参数 num.io.threads调整。</p>
<p>Purgatory组件是用来缓存延时请求（Delayed Request）的。比如设置了 acks=all 的 PRODUCE 请求，一旦设置了 acks=all，那么该请求就必须等待 ISR 中所有副本都接收了消息后才能返回，此时处理该请求的 IO 线程就必须等待其他 Broker 的写入结果。</p>
<h3 id="控制器">控制器</h3>
<p>在 Kafka 集群中会有一个或多个 broker，其中有一个 broker 会被选举为控制器（Kafka Controller），它负责管理整个集群中所有分区和副本的状态。</p>
<h4 id="控制器是如何被选出来的">控制器是如何被选出来的？</h4>
<p>Broker 在启动时，会尝试去 ZooKeeper 中创建 /controller 节点。Kafka 当前选举控制器的规则是：第一个成功创建 /controller 节点的 Broker 会被指定为控制器。</p>
<p>在ZooKeeper中的 /controller_epoch 节点中存放的是一个整型的 controller_epoch 值。controller_epoch 用于记录控制器发生变更的次数，即记录当前的控制器是第几代控制器，我们也可以称之为“控制器的纪元”。</p>
<p>controller_epoch 的初始值为1，即集群中第一个控制器的纪元为1，当控制器发生变更时，每选出一个新的控制器就将该字段值加1。Kafka 通过 controller_epoch 来保证控制器的唯一性，进而保证相关操作的一致性。</p>
<p>每个和控制器交互的请求都会携带 controller_epoch 这个字段，如果请求的 controller_epoch 值小于内存中的 controller_epoch 值，则认为这个请求是向已经过期的控制器所发送的请求，那么这个请求会被认定为无效的请求。</p>
<p>如果请求的 controller_epoch 值大于内存中的 controller_epoch 值，那么说明已经有新的控制器当选了。</p>
<h4 id="控制器是做什么的">控制器是做什么的？</h4>
<ul>
<li>主题管理（创建、删除、增加分区）</li>
<li>分区重分配</li>
<li>Preferred 领导者选举<br>
Preferred 领导者选举主要是 Kafka 为了避免部分 Broker 负载过重而提供的一种换 Leader 的方案。</li>
<li>集群成员管理（新增 Broker、Broker 主动关闭、Broker 宕机）<br>
控制器组件会利用 Watch 机制检查 ZooKeeper 的 /brokers/ids 节点下的子节点数量变更。目前，当有新 Broker 启动后，它会在 /brokers 下创建专属的 znode 节点。一旦创建完毕，ZooKeeper 会通过 Watch 机制将消息通知推送给控制器，这样，控制器就能自动地感知到这个变化，进而开启后续的新增 Broker 作业。</li>
<li>数据服务<br>
控制器上保存了最全的集群元数据信息。<br>
<img src="https://phantomma.top/post-images/1701595446454.jpg" alt="" loading="lazy"></li>
</ul>
<h4 id="控制器宕机了怎么办">控制器宕机了怎么办？</h4>
<p>当运行中的控制器突然宕机或意外终止时，Kafka 能够快速地感知到，并立即启用备用控制器来代替之前失败的控制器。这个过程就被称为 Failover，该过程是自动完成的，无需你手动干预。<br>
<img src="https://phantomma.top/post-images/1701595462010.jpg" alt="" loading="lazy"></p>
<h2 id="消费者">消费者</h2>
<h3 id="消费组">消费组</h3>
<p>在Kafka中，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者。每个消费者只能消费所分配到的分区中的消息。而每一个分区只能被一个消费组中的一个消费者所消费。<br>
<img src="https://phantomma.top/post-images/1701595482647.png" alt="" loading="lazy"></p>
<p>入上图所示，我们可以设置两个消费者组来实现广播消息的作用，消费组A和组B都可以接受到生产者发送过来的消息。</p>
<p>消费者与消费组这种模型可以让整体的消费能力具备横向伸缩性，我们可以增加（或减少）消费者的个数来提高（或降低）整体的消费能力。对于分区数固定的情况，一味地增加消费者并不会让消费能力一直得到提升，如果消费者过多，出现了消费者的个数大于分区个数的情况，就会有消费者分配不到任何分区。</p>
<p>如下：一共有8个消费者，7个分区，那么最后的消费者C7由于分配不到任何分区而无法消费任何消息。<br>
<img src="https://phantomma.top/post-images/1701595509429.png" alt="" loading="lazy"></p>
<h3 id="消费端分区分配策略">消费端分区分配策略</h3>
<p>Kafka 提供了消费者客户端参数 partition.assignment.strategy 来设置消费者与订阅主题之间的分区分配策略。</p>
<h4 id="rangeassignor分配策略">RangeAssignor分配策略</h4>
<p>默认情况下，采用 RangeAssignor 分配策略。</p>
<p>RangeAssignor 分配策略的原理是按照消费者总数和分区总数进行整除运算来获得一个跨度，然后将分区按照跨度进行平均分配，以保证分区尽可能均匀地分配给所有的消费者。对于每一个主题，RangeAssignor 策略会将消费组内所有订阅这个主题的消费者按照名称的字典序排序，然后为每个消费者划分固定的分区范围，如果不够平均分配，那么字典序靠前的消费者会被多分配一个分区。</p>
<p>假设消费组内有2个消费者 C0 和 C1，都订阅了主题 t0 和 t1，并且每个主题都有4个分区，那么订阅的所有分区可以标识为：t0p0、t0p1、t0p2、t0p3、t1p0、t1p1、t1p2、t1p3。最终的分配结果为：</p>
<pre><code>消费者C0：t0p0、t0p1、t1p0、t1p1
消费者C1：t0p2、t0p3、t1p2、t1p3
</code></pre>
<p>假设上面例子中2个主题都只有3个分区，那么订阅的所有分区可以标识为：t0p0、t0p1、t0p2、t1p0、t1p1、t1p2。最终的分配结果为：</p>
<pre><code>消费者C0：t0p0、t0p1、t1p0、t1p1
消费者C1：t0p2、t1p2
</code></pre>
<p>可以明显地看到这样的分配并不均匀。</p>
<h4 id="roundrobinassignor分配策略">RoundRobinAssignor分配策略</h4>
<p>RoundRobinAssignor 分配策略的原理是将消费组内所有消费者及消费者订阅的所有主题的分区按照字典序排序，然后通过轮询方式逐个将分区依次分配给每个消费者。</p>
<p>如果同一个消费组内所有的消费者的订阅信息都是相同的，那么 RoundRobinAssignor 分配策略的分区分配会是均匀的。</p>
<p>如果同一个消费组内的消费者订阅的信息是不相同的，那么在执行分区分配的时候就不是完全的轮询分配，有可能导致分区分配得不均匀。</p>
<p>假设消费组内有3个消费者（C0、C1 和 C2），t0、t0、t1、t2主题分别有1、2、3个分区，即整个消费组订阅了 t0p0、t1p0、t1p1、t2p0、t2p1、t2p2 这6个分区。</p>
<p>具体而言，消费者 C0 订阅的是主题 t0，消费者 C1 订阅的是主题 t0 和 t1，消费者 C2 订阅的是主题 t0、t1 和 t2，那么最终的分配结果为：</p>
<pre><code>消费者C0：t0p0
消费者C1：t1p0
消费者C2：t1p1、t2p0、t2p1、t2p2
</code></pre>
<p>可以看 到 RoundRobinAssignor 策略也不是十分完美，这样分配其实并不是最优解，因为完全可以将分区 t1p1 分配给消费者 C1。</p>
<h4 id="stickyassignor分配策略">StickyAssignor分配策略</h4>
<p>这种分配策略，它主要有两个目的：</p>
<p>分区的分配要尽可能均匀。<br>
分区的分配尽可能与上次分配的保持相同。<br>
假设消费组内有3个消费者（C0、C1 和 C2），它们都订阅了4个主题（t0、t1、t2、t3），并且每个主题有2个分区。也就是说，整个消费组订阅了 t0p0、t0p1、t1p0、t1p1、t2p0、t2p1、t3p0、t3p1 这8个分区。最终的分配结果如下：</p>
<pre><code>消费者C0：t0p0、t1p1、t3p0
消费者C1：t0p1、t2p0、t3p1
消费者C2：t1p0、t2p1
</code></pre>
<p>再假设此时消费者 C1 脱离了消费组，那么分配结果为：</p>
<pre><code>消费者C0：t0p0、t1p1、t3p0、t2p0
消费者C2：t1p0、t2p1、t0p1、t3p1
</code></pre>
<p>StickyAssignor 分配策略如同其名称中的“sticky”一样，让分配策略具备一定的“黏性”，尽可能地让前后两次分配相同，进而减少系统资源的损耗及其他异常情况的发生。</p>
<h3 id="再均衡rebalance">再均衡（Rebalance）</h3>
<p>再均衡是指分区的所属权从一个消费者转移到另一消费者的行为，它为消费组具备高可用性和伸缩性提供保障，使我们可以既方便又安全地删除消费组内的消费者或往消费组内添加消费者。</p>
<p>弊端：</p>
<ul>
<li>在再均衡发生期间，消费组内的消费者是无法读取消息的。</li>
<li>Rebalance 很慢。如果一个消费者组里面有几百个 Consumer 实例，Rebalance 一次要几个小时。</li>
<li>在进行再均衡的时候消，费者当前的状态也会丢失。比如消费者消费完某个分区中的一部分消息时还没有来得及提交消费位移就发生了再均衡操作，之后这个分区又被分配给了消费组内的另一个消费者，原来被消费完的那部分消息又被重新消费一遍，也就是发生了重复消费。</li>
</ul>
<p>Rebalance 发生的时机有三个：</p>
<ul>
<li>组成员数量发生变化</li>
<li>订阅主题数量发生变化</li>
<li>订阅主题的分区数发生变化<br>
后两类通常是业务的变动调整所导致的，我们一般不可控制，我们主要说说因为组成员数量变化而引发的 Rebalance 该如何避免。</li>
</ul>
<p>当 Consumer Group 完成 Rebalance 之后，每个 Consumer 实例都会定期地向 Coordinator 发送心跳请求，表明它还存活着。如果某个 Consumer 实例不能及时地发送这些心跳请求，Coordinator 就会认为该 Consumer 已经“死”了，从而将其从 Group 中移除，然后开启新一轮 Rebalance。</p>
<p>Consumer端可以设置 <code>session.timeout.ms</code>，默认是10s，表示如果 Coordinator 在 10 秒之内没有收到 Group 下某 Consumer 实例的心跳，它就会认为这个 Consumer 实例已经挂了。</p>
<p>Consumer端还可以设置 <code>heartbeat.interval.ms</code>，表示发送心跳请求的频率。</p>
<p>以及<code>max.poll.interval.ms</code> 参数，它限定了 Consumer 端应用程序两次调用 poll 方法的最大时间间隔。它的默认值是 5 分钟，表示你的 Consumer 程序如果在 5 分钟之内无法消费完 poll 方法返回的消息，那么 Consumer 会主动发起“离开组”的请求，Coordinator 也会开启新一轮 Rebalance。</p>
<p>所以知道了上面几个参数后，我们就可以避免以下两个问题：</p>
<ul>
<li>非必要 Rebalance 是因为未能及时发送心跳，导致 Consumer 被“踢出”Group 而引发的。<br>
所以我们在生产环境中可以这么设置：
<ul>
<li>设置 session.timeout.ms = 6s。</li>
<li>设置 heartbeat.interval.ms = 2s。</li>
</ul>
</li>
<li>必要 Rebalance 是 Consumer 消费时间过长导致的。如何消费任务时间达到8分钟，而max.poll.interval.ms设置为5分钟，那么也会发生Rebalance，所以如果有比较重的任务的话，可以适当调整这个参数。</li>
<li>Consumer 端的频繁的 Full GC导致的长时间停顿，从而引发了 Rebalance。</li>
</ul>
<h3 id="消费者组再平衡全流程">消费者组再平衡全流程</h3>
<p>重平衡过程是靠消费者端的心跳线程（Heartbeat Thread），通知到其他消费者实例的。</p>
<p>当协调者决定开启新一轮重平衡后，它会将“REBALANCE_IN_PROGRESS”封装进心跳请求的响应中，发还给消费者实例。当消费者实例发现心跳响应中包含了“REBALANCE_IN_PROGRESS”，就能立马知道重平衡又开始了，这就是重平衡的通知机制。</p>
<p>所以，实际上heartbeat.interval.ms不止是设置了心跳的间隔时间，还可以控制重平衡通知的频率。</p>
<h3 id="消费者组状态机">消费者组状态机</h3>
<p>重平衡一旦开启，Broker 端的协调者组件就要完成整个重平衡流程，Kafka 设计了一套消费者组状态机（State Machine）来实现。</p>
<p>Kafka 为消费者组定义了 5 种状态，它们分别是：Empty、Dead、PreparingRebalance、CompletingRebalance 和 Stable。<br>
状态机的各个状态流转：<br>
<img src="https://phantomma.top/post-images/1701595723280.jpg" alt="" loading="lazy"></p>
<p>当有新成员加入或已有成员退出时，消费者组的状态从 Stable 直接跳到 PreparingRebalance 状态，此时，所有现存成员就必须重新申请加入组。当所有成员都退出组后，消费者组状态变更为 Empty。Kafka 定期自动删除过期位移的条件就是，组要处于 Empty 状态。因此，如果你的消费者组停掉了很长时间（超过 7 天），那么 Kafka 很可能就把该组的位移数据删除了。</p>
<h3 id="组协调器groupcoordinator">组协调器（GroupCoordinator）</h3>
<p>GroupCoordinator 是 Kafka 服务端中用于管理消费组的组件。协调器最重要的职责就是负责执行消费者再均衡的操作。</p>
<h3 id="消费者端重平衡流程">消费者端重平衡流程</h3>
<p>在消费者端，重平衡分为两个步骤：分别是加入组和等待领导者消费者（Leader Consumer）分配方案。即JoinGroup 请求和 SyncGroup 请求。</p>
<ul>
<li>
<p>加入组<br>
当组内成员加入组时，它会向协调器发送 JoinGroup 请求。在该请求中，每个成员都要将自己订阅的主题上报，这样协调器就能收集到所有成员的订阅信息。</p>
</li>
<li>
<p>选择消费组领导者<br>
一旦收集了全部成员的 JoinGroup 请求后，协调者会从这些成员中选择一个担任这个消费者组的领导者。<br>
这里的领导者是具体的消费者实例，它既不是副本，也不是协调器。领导者消费者的任务是收集所有成员的订阅信息，然后根据这些信息，制定具体的分区消费分配方案。</p>
</li>
<li>
<p>选举分区分配策略<br>
这个分区分配的选举是根据消费组内的各个消费者投票来决定的。<br>
协调器会收集各个消费者支持的所有分配策略，组成候选集 candidates。每个消费者从候选集 candidates 中找出第一个自身支持的策略，为这个策略投上一票。计算候选集中各个策略的选票数，选票数最多的策略即为当前消费组的分配策略。<br>
如果有消费者并不支持选出的分配策略，那么就会报出异常 IllegalArgumentException：Member does not support protocol。<br>
<img src="https://phantomma.top/post-images/1701595783876.png" alt="" loading="lazy"><br>
<img src="https://phantomma.top/post-images/1701595796973.png" alt="" loading="lazy"></p>
</li>
<li>
<p>发送 SyncGroup 请求<br>
协调器会把消费者组订阅信息封装进 JoinGroup 请求的响应体中，然后发给领导者，由领导者统一做出分配方案，然后领导者发送 SyncGroup 请求给协调器。<br>
<img src="https://phantomma.top/post-images/1701595823639.png" alt="" loading="lazy"></p>
</li>
<li>
<p>响应SyncGroup<br>
组内所有的消费者都会发送一个 SyncGroup 请求，只不过不是领导者的请求内容为空，然后就会接收到一个SyncGroup响应，接受订阅信息。</p>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[vps 折腾小记]]></title>
        <id>https://phantomma.top/post/vps-zhe-teng-xiao-ji/</id>
        <link href="https://phantomma.top/post/vps-zhe-teng-xiao-ji/">
        </link>
        <updated>2015-02-01T13:08:15.000Z</updated>
        <content type="html"><![CDATA[<h1 id="购买vps">购买vps</h1>
<p>使用的是俄罗斯的vps厂商：justhost，它的定价是最有性价比的一档。最便宜的俄罗斯区域的，120多人民币一年。一开始选择的俄罗斯region，不过流媒体对俄罗斯ip是封禁的。最后还是换成美西😓，价格也变成了360¥一年。</p>
<p>美西LOS，ping的延迟在150ms</p>
<pre><code class="language-bash">PING 155.254.193.168 (155.254.193.168): 56 data bytes
64 bytes from 155.254.193.168: icmp_seq=0 ttl=52 time=154.847 ms
64 bytes from 155.254.193.168: icmp_seq=1 ttl=52 time=177.327 ms
64 bytes from 155.254.193.168: icmp_seq=2 ttl=52 time=153.530 ms
64 bytes from 155.254.193.168: icmp_seq=3 ttl=52 time=151.163 ms
</code></pre>
<p><a href="https://justhost.asia/">https://justhost.asia/</a></p>
<h1 id="申请域名">申请域名</h1>
<p>域名丛<a href="https://freenom.com/">https://freenom.com/</a> 申请的免费域名。<br>
可以将域名的解析服务设置到国内的dnspod。</p>
<h1 id="一键安装v2ray脚本">一键安装v2ray脚本</h1>
<p>域名搞定以后，可以在主机上一键安装服务端 <a href="https://github.com/wulabing/V2Ray_ws-tls_bash_onekey">https://github.com/wulabing/V2Ray_ws-tls_bash_onekey</a></p>
<blockquote>
<p>vmess://ewogICJ2IjogIjIiLAogICJwcyI6ICJ3dWxhYmluZ19jcm9zcy5waGFudG9tbWEudGsiLAogICJhZGQiOiAiY3Jvc3MucGhhbnRvbW1hLnRrIiwKICAicG9ydCI6ICI0NDMiLAogICJpZCI6ICI4NmNiYjU0Mi1iYzRkLTQwYTMtYmQzOS1hNWM2OTkzOGE1NTQiLAogICJhaWQiOiAiMCIsCiAgIm5ldCI6ICJ3cyIsCiAgInR5cGUiOiAibm9uZSIsCiAgImhvc3QiOiAiY3Jvc3MucGhhbnRvbW1hLnRrIiwKICAicGF0aCI6ICIvMWJjODgvIiwKICAidGxzIjogInRscyIKfQo=</p>
</blockquote>
<h1 id="加速tcp">加速tcp</h1>
<p>压缩加速tcp的连接<br>
<a href="https://github.com/tcp-nanqinlang/wiki/wiki/general">https://github.com/tcp-nanqinlang/wiki/wiki/general</a></p>
<h1 id="配置软路由科学上网插件">配置软路由科学上网插件</h1>
<ul class="contains-task-list">
<li class="task-list-item"><input class="task-list-item-checkbox" disabled="" type="checkbox" id="task-item-791139"><label class="task-list-item-label" for="task-item-791139"> bypass</label></li>
<li class="task-list-item"><input class="task-list-item-checkbox" checked="" disabled="" type="checkbox" id="task-item-475235"><label class="task-list-item-label" for="task-item-475235"> luci-app-ssr-plus （额外多了 DNS防污染服务）</label></li>
</ul>
<h1 id="other">other</h1>
<ul>
<li>查看流媒体网站对该vps的支持情况。<a href="https://www.vpsgo.com/vps-ip-regionrestrictioncheck.html">https://www.vpsgo.com/vps-ip-regionrestrictioncheck.html</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[hbase orm 中间层 hbasedao]]></title>
        <id>https://phantomma.top/post/hbase-orm-zhong-jian-ceng-hbasedao/</id>
        <link href="https://phantomma.top/post/hbase-orm-zhong-jian-ceng-hbasedao/">
        </link>
        <updated>2015-01-03T02:19:17.000Z</updated>
        <content type="html"><![CDATA[<h1 id="背景">背景</h1>
<p>hbase 是分布式的 kv(key value) 存储系统，hbase 提供的针对底层数据的操作也是基于 kv 维度的，使用起来更像是Map的操作方式。但是上层业务应用一般是采用的面向对象的设计，这就导致了使用 hbase 的底层 api 必须要写很多的代码来做KV原始数据到上层业务对象的转化。</p>
<p>使用关系型数据库如 MySQL，也会遇到关系型数据跟对象之间的适配问题，所以出现了很多成熟的做对象关系映射的产品，像 <code>hibernate</code> 和 <code>mybatis</code>。</p>
<p>hbase 当然也需要一个类似的东西，来做对象到 kv 数据的适配，这样上层应用可以继续专注于上层 面向对象的方式的开发，而不用直接操作kv数据，提高工作效率。</p>
<h1 id="hbase使用场景">hbase使用场景</h1>
<p>hbase 要解决的问题是海量数据的分布式存储，传统数据库如 mysql 解决这个问题也是有办法的，可以通过分库分表的方式做到。 所以，hbase 和 mysql 的竞争点是在需要 mysql 分库分表的情况。下面以一个很常见的场景举例，来看之前数据存储使用mysql的场景，迁移hbase上的过程。</p>
<blockquote>
<p>问题描述：用户的基本信息（包括：用户头像，上次登录时间），好友关系（正向 我关注的人，反向关注我的人）</p>
</blockquote>
<h2 id="mysql的方式">mysql的方式：</h2>
<h3 id="用户信息表">用户信息表</h3>
<table>
<thead>
<tr>
<th>列名</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>id</td>
<td>用户id</td>
</tr>
<tr>
<td>head_image</td>
<td>头像</td>
</tr>
<tr>
<td>last_visit_time</td>
<td>上次登录时间</td>
</tr>
</tbody>
</table>
<h3 id="用户关注表">用户关注表</h3>
<table>
<thead>
<tr>
<th>列名</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>userId</td>
<td>用户id</td>
</tr>
<tr>
<td>follow_user</td>
<td>关注的人</td>
</tr>
</tbody>
</table>
<h3 id="用户粉丝表">用户粉丝表</h3>
<table>
<thead>
<tr>
<th>列名</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>userId</td>
<td>用户id</td>
</tr>
<tr>
<td>be_followed_user</td>
<td>关注我的人</td>
</tr>
</tbody>
</table>
<p>之所以用户关系两张表来保存是因为，用户关系数据量比较大的情况下，采用分库分表存储的方案，又要提供正向、反向的查询，所以需要分别以关注者和被关注者为路由字段存储两份。</p>
<h2 id="hbase的方式">hbase的方式：</h2>
<table>
<thead>
<tr>
<th>rowKey</th>
<th>列簇</th>
<th>列</th>
</tr>
</thead>
<tbody>
<tr>
<td>userId</td>
<td>info_cf（单version）</td>
<td>head_image</td>
</tr>
<tr>
<td></td>
<td></td>
<td>last_visit_time</td>
</tr>
<tr>
<td></td>
<td>relation_cf (多个version)</td>
<td>follow_user</td>
</tr>
<tr>
<td></td>
<td></td>
<td>be_followed_user</td>
</tr>
</tbody>
</table>
<p>存入 hbase 的数据的逻辑结构会是： hbase_data.png</p>
<blockquote></blockquote>
<p>通过上面 mysql 表到 hbase 表的迁移过程，可以清楚地看到：mysql 中 和 userId 一对一的用户基本信息迁移到 hbase 可以用无 version 特性的列簇保存；一对多的关注关系迁移到 hbase，可以利用hbase 多 version 的列簇保存。下面介绍的hbasedao中间层就会主要解决mysql迁移到 hbase 的适配，包括单 version（一对一）和多 version（一对多）的情形。<br>
NOTE： hbase 多 version 的列簇，不同column之间没有任何对应关系，所以不要尝试在不同的 column 之间找行的对应关系。</p>
<h1 id="hbasedao介绍">hbasedao介绍</h1>
<p>hbasedao 是一个简单地解决kv数据到业务对象适配的中间层，类似关系型数据库orm中间层mybatis。</p>
<p>针对于 hbase 存储结构抽象出来的类结构：<br>
habasedao_realtion.png</p>
<h2 id="使用">使用</h2>
<p>封装业务对象成 DO 类，一个 rowKey 对应一个 DO 类的对象，通过指定DO类里的 column，中间层可以做到针对于指定 column 的查询。使用 hbasedao 之后，查询的方式如下：</p>
<pre><code>public UserHBaseDO get(String rowKey) throws HBaseDAOException {
    HBaseDO hBaseDO = new HBaseDO();
    hBaseDO.setTableName(UserHBaseDO.TABLE_NAME);
    hBaseDO.setRowKey(rowKey);
    hBaseDO.addColumnFamily(UserHBaseDO.CF_NAME_info_cf)
        .putColumn(UserHBaseDO.CL_NAME_head_image)
        .putColumn(UserHBaseDO.CL_NAME_last_visit_time);
    hBaseDO.addColumnFamily(UserHBaseDO.CF_NAME_relation_cf, 10)
        .putColumn(UserHBaseDO.CL_NAME_follow_user)
        .putColumn(UserHBaseDO.CL_NAME_be_followed_user);
    super.getHbaseDao().get(hBaseDO);
    UserHBaseDO userHBaseDO = null;
    try {
        userHBaseDO = new UserHBaseDO(hBaseDO);
    } catch (UnsupportedEncodingException e) {
        new HBaseDAOException(e);
    }
    return userHBaseDO;
}
</code></pre>
<h2 id="支持的-api">支持的 API</h2>
<ol>
<li>以对象的方式插入数据</li>
<li>批量插入数据</li>
<li>传入 rowKey，查询对象。可支持针对不同的列簇指定查询的 version 数量，支持指定列簇中数据的时间范围</li>
<li>传入多个 rowKey，查询对象的列表</li>
<li>删除指定的 rowKey 的一行记录</li>
<li>指定开始扫描的 rowKey，开始按行扫描数据，可以限制扫描的结果集大小</li>
</ol>
<pre><code>public interface HBaseDAO {
    public void put(HBaseDO hBaseDO) throws HBaseDAOException;
    public void putList(List&lt;HBaseDO&gt; hBaseDOList, String tableName) 
        throws HBaseDAOException;
    public void get(HBaseDO hBaseDO) throws HBaseDAOException;
    public void getList(List&lt;HBaseDO&gt; hBaseDOList, String tableName) 
        throws HBaseDAOException;
    public void delete(HBaseDO hBaseDO) throws HBaseDAOException;
    public List&lt;HBaseDO&gt; scan(String tableName, String startRow, String endRow, 
        int maxVersion, int maxResultSize, Map&lt;String, List&lt;String&gt;&gt; columnFamilyMap) 
        throws HBaseDAOException;
}
</code></pre>
<h1 id="使用举例step-by-step">使用举例（step by step）</h1>
<p>代码工程结构，sample 包里提供了详细的使用举例。可以看到，使用时只需要定义 hbase 表对应的 DO 类，在此类中编写对象和 hbase 里数据的对应关系，在 DAO 层以上就可以提供和 mybatis 类似的接口访问形式。</p>
<pre><code>├── pom.xml
├── src
│   ├── main
│   │   ├── java
│   │   │   └── com
│   │   │       └── taobao
│   │   │           └── hbasedao
│   │   │               ├── ColumnFamilyInfo.java
│   │   │               ├── HBaseCell.java
│   │   │               ├── HBaseClientDaoSupport.java
│   │   │               ├── HBaseColumnFamily.java
│   │   │               ├── HBaseConsole.java
│   │   │               ├── HBaseDAO.java
│   │   │               ├── HBaseDAOException.java
│   │   │               ├── HBaseDAOFactory.java
│   │   │               ├── HBaseDAOImpl.java
│   │   │               ├── HBaseDO.java
│   │   │               ├── HBaseVO.java
│   │   │               ├── MapAble.java
│   │   │               └── sample
│   │   │                   ├── dao
│   │   │                   │   └── UserHBaseDAO.java
│   │   │                   ├── dataobject
│   │   │                   │   └── UserHBaseDO.java
│   │   │                   └── vo
│   │   │                       └── FollowerVO.java
│   │   └── resources
│   └── test
│       ├── java
│       │   └── com
│       │       └── taobao
│       │           └── hbasedao
│       │               └── sample
│       │                   └── test
│       │                       └── UserHBaseDAOTest.java
│       └── resources
│           └── hbase-dao.xml
</code></pre>
<h2 id="步骤">步骤</h2>
<ol>
<li>引入依赖</li>
</ol>
<pre><code>&lt;dependency&gt;
  &lt;groupId&gt;com.taobao.hbasedao&lt;/groupId&gt;
  &lt;artifactId&gt;hbasedao&lt;/artifactId&gt;
  &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<ol start="2">
<li>定义hbae表对应的DO类，相当于使用 mybati s时，定义的 sqlmap，需要给出 hbase 表的 schema 信息，如：列簇名、列名、列簇的最大 version，列簇和列的绑定关系等。</li>
</ol>
<pre><code>public class UserHBaseDO implements MapAble&lt;UserHBaseDO&gt; {
    public static final String TABLE_NAME = &quot;hbasedao_user&quot;;
    public static final String CF_NAME_info_cf = &quot;info_cf&quot;;
    public static final int MAX_SIZE_CF_NAME_info_cf = 1;
    public static final String CF_NAME_relation_cf = &quot;relation_cf&quot;;
    public static final int MAX_SIZE_CF_NAME_relation_cf = 1000;
    public static final String CL_NAME_head_image = &quot;head_image&quot;;
    public static final String CL_NAME_last_visit_time = &quot;last_visit_time&quot;;
    public static final String CL_NAME_follow_user = &quot;follow_user&quot;;
    public static final String CL_NAME_be_followed_user = &quot;be_followed_user&quot;;
    private static Map&lt;String, List&lt;String&gt;&gt; columnFamilyMap;
    static {
        columnFamilyMap = new HashMap&lt;String, List&lt;String&gt;&gt;();
        columnFamilyMap.put(CF_NAME_info_cf,
                Arrays.asList(new String[] { CL_NAME_head_image, CL_NAME_last_visit_time }));
        columnFamilyMap.put(CF_NAME_relation_cf,
                Arrays.asList(new String[] { CL_NAME_follow_user, CL_NAME_be_followed_user }));
    }
    private String rowKey;
    private String headImage;
    private long lastVisitTime;
    private final List&lt;FollowerVO&gt; followVOList = new ArrayList&lt;FollowerVO&gt;();
    private final List&lt;FollowerVO&gt; beFollowedVOList = new ArrayList&lt;FollowerVO&gt;();
</code></pre>
<ol start="3">
<li>实现 MapAble 接口，MapAble 接口包括两个方法，一个是插入数据时将用户定义的 UserHBaseDO 转化为框架使用的 HBaseDO，一个是查询时将 HBaseDO 里包含的数据转为为用户定义 UserHBaseDO。</li>
</ol>
<pre><code>@Override
public HBaseDO converDOToHBaseDO() {
    HBaseDO hbaseDO = new HBaseDO();
    hbaseDO.setRowKey(this.rowKey);
    hbaseDO.setTableName(UserHBaseDO.TABLE_NAME);
    for (Map.Entry&lt;String, List&lt;String&gt;&gt; columnFamilyEntry : 
        columnFamilyMap.entrySet()) {
        String columnFamilyName = columnFamilyEntry.getKey();
        List&lt;String&gt; columnNameList = columnFamilyEntry.getValue();
        HBaseColumnFamily columnFamily = new HBaseColumnFamily();
        for (String columnName : columnNameList) {
            if (CF_NAME_info_cf.equals(columnFamilyName) &amp;&amp; 
                CL_NAME_head_image.equals(columnName)) {
                if (this.headImage != null) {
                    List&lt;HBaseCell&gt; cellDOList = new ArrayList&lt;HBaseCell&gt;();
                    HBaseCell cellDO = new HBaseCell();
                    cellDO.setValue(this.headImage);
                    cellDOList.add(cellDO);
                    columnFamily.putColumn(columnName, cellDOList);
                }
            }
            ...
        }
        hbaseDO.getColumnFamilyMap().put(columnFamilyName, columnFamily);
    }
    return hbaseDO;
}
@Override
public UserHBaseDO convertHBaseDOToDO(HBaseDO hBaseDO) throws UnsupportedEncodingException {
    this.rowKey = hBaseDO.getRowKey();
    List&lt;KeyValue&gt; results = hBaseDO.getResults();
    for (Map.Entry&lt;String, List&lt;String&gt;&gt; columnFamilyEntry : 
        columnFamilyMap.entrySet()) {
        String columnFamilyName = columnFamilyEntry.getKey();
        List&lt;String&gt; columnNameList = columnFamilyEntry.getValue();
        for (String columnName : columnNameList) {
            if (CF_NAME_info_cf.equals(columnFamilyName) &amp;&amp; 
                CL_NAME_head_image.equals(columnName)) {
                for (KeyValue kv : results) {
                    if (columnFamilyName.equals(new String(kv.getFamily()))
                            &amp;&amp; columnName.equals(new String(kv.getQualifier()))) {
                        this.headImage = new String(kv.getValue(), &quot;UTF-8&quot;);
                    }
                }
            }
            ...
        }
    }
    return this;
}
</code></pre>
<ol start="4">
<li>然后就是定义 DAO 类，主要的逻辑已经在 UserHBaseDO 写了，这里的实现可以足够简单，查询的代码在 hbasedao 介绍里已经给出，插入数据的代码如下：</li>
</ol>
<pre><code>public void insert(UserHBaseDO userHBaseDO) throws HBaseDAOException {
    super.getHbaseDao().put(userHBaseDO.converDOToHBaseDO());
}
</code></pre>
<ol start="5">
<li>编写测试用例</li>
</ol>
<pre><code>@Test
public void test_insert() throws HBaseDAOException {
    ApplicationContext context = new ClassPathXmlApplicationContext(&quot;hbase-dao.xml&quot;);
    UserHBaseDAO userHBaseDAO = context.getBean(&quot;userHBaseDAO&quot;, UserHBaseDAO.class);
    UserHBaseDO userHBaseDO = new UserHBaseDO();
    userHBaseDO.setRowKey(&quot;2222&quot;);
    userHBaseDO.setHeadImage(&quot;icon1.jpg&quot;);
    userHBaseDO.setLastVisitTime(4820023);
    int i = 0;
    FollowerVO followerVO1 = new FollowerVO(&quot;444_errik&quot;);
    followerVO1.setTimeStamp(System.currentTimeMillis() + i++);
    FollowerVO followerVO2 = new FollowerVO(&quot;555_wiie&quot;);
    followerVO2.setTimeStamp(System.currentTimeMillis() + i++);
    FollowerVO followerVO3 = new FollowerVO(&quot;666_gate&quot;);
    followerVO3.setTimeStamp(System.currentTimeMillis() + i++);
    userHBaseDO.getFollowVOList().add(followerVO1);
    userHBaseDO.getFollowVOList().add(followerVO2);
    userHBaseDO.getFollowVOList().add(followerVO3);
    FollowerVO followerVO4 = new FollowerVO(&quot;999_hena&quot;);
    userHBaseDO.getBeFollowedVOList().add(followerVO4);
    userHBaseDAO.insert(userHBaseDO);
}
@Test
public void test_get() throws HBaseDAOException {
    ApplicationContext context = new ClassPathXmlApplicationContext(&quot;hbase-dao.xml&quot;);
    UserHBaseDAO userHBaseDAO = context.getBean(&quot;userHBaseDAO&quot;, UserHBaseDAO.class);
    UserHBaseDO userHBaseDO = userHBaseDAO.get(&quot;2222&quot;);
    System.out.println(userHBaseDO);
}
</code></pre>
<p>完整地代码示例请参考 <code>com.taobao.hbasedao.sample</code> 代码工程在：hbasedao</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Canal 源码走读]]></title>
        <id>https://phantomma.top/post/canal-yuan-ma-zou-du/</id>
        <link href="https://phantomma.top/post/canal-yuan-ma-zou-du/">
        </link>
        <updated>2014-12-28T15:00:58.000Z</updated>
        <content type="html"><![CDATA[<p>canal 是什么？ 引用一下官方回答：</p>
<blockquote>
<p>阿里巴巴mysql数据库binlog的增量订阅&amp;消费组件</p>
</blockquote>
<p>** canal 能做什么？**<br>
基于日志增量订阅&amp;消费支持的业务：</p>
<ul>
<li>数据库镜像</li>
<li>数据库实时备份</li>
<li>多级索引 (卖家和买家各自分库索引)</li>
<li>search build</li>
<li>业务cache刷新</li>
<li>价格变化等重要业务消息</li>
</ul>
<p>比如 LZ 目前就使用 canal 实现数据实时复制，搜索引擎数据构建等功能。既然要使用，就好好的研究一下。</p>
<p>时间有限，一起来简单看看。</p>
<h2 id="软件架构">软件架构</h2>
<p>关于 canal 的工作原理，我就不展开了，有兴趣的可以看看官方文档，或者这个 (ppt)[https://docs.google.com/presentation/d/1MkszUPYRDkfVPz9IqOT1LLT5d9tuwde_WC8GZvjaDRg/edit?pli=1#slide=id.p16].</p>
<p>说白了， canal 就是伪装成 mysql 的 slave，dump binlog，解析 binlog，然后传递给应用程序，总体还是蛮简单的。</p>
<p>好，我们来看看 canal 的代码架构。<br>
<img src="https://phantomma.top/post-images/1703775848845.png" alt="" loading="lazy"></p>
<p>我们看到，canal server 内部由几个模块组成， 最外部的是 Server，该 Server 接收 Canal Client 请求，并返回 Client 数据。一个 Server 就是一个 JVM。每个 Server 内部由多个 CanalInstance，每个 CanalInstance 其实就是我们设置的 destination，通常是一个数据库。</p>
<p>每个 CanalInstance 内部由 5 个模块，分别是 parser 解析，sink 过滤，store 存储，metaManager 元数据管理，Alarm 报警。</p>
<p>这 5 个模块是干嘛的呢？</p>
<p>简单说一下：</p>
<p>当 Canal Server 启动后，会根据配置启动 N 个 CanalInstance， 每个 CanalInstance 都会使用 socket 连接 mysql，dump binlog，然后将数据交给 parser 解析，sink 过滤，store 存储，当 client 连接时，会从 zk 上读取该 client 的信息，而 metaManager 元数据管理就是管理 zk（当然有多种实现，比如存储在文件中） 信息的，如果发生错误了，就调用 Alarm 发送报警信息（你可以接入自己公司的监控系统），目前是打印日志。</p>
<h2 id="canal-启动流程">Canal 启动流程</h2>
<p>canal 代码量目前有 6 万多行，去除 2 个 ProtocolBuffer 生成类大概 1.7 万行，也还有 4.3 万行，代码还是不少的。</p>
<p>启动过程也比较绕。这里我简单画了一个流程图：<br>
<img src="https://phantomma.top/post-images/1703775866770.png" alt="" loading="lazy"></p>
<p>解释一下这个图：</p>
<p>canal 脚本从 CanalLauncher main 方法启动，然后调用 CanalController 的 start 方法，CanalController 调用 InstanceConfigMonitor 的 start 方法，最后调用 canal 关键组件 CanalServerWithEmbedded 的 start 方法。</p>
<p>在 Canal 内部， 有 CanalServerWithEmbedded 和 CanalServerWithNetty，前者是没有 Server 端口的，是一个无端口的代理。后者是基于 Netty 实现的服务器，在 channelRead 方法中，会调用 CanalServerWithEmbedded 的相关方法。</p>
<p>CanalServerWithEmbedded 是单例的， 内部会有多个 CanalInstance， 他有多个实现，独立版本中使用的是 CanalInstanceWithSpring 版本，基于 Spring 管理组件的生命周期。</p>
<p>每个 CanalInstance 内部有 5 个组件，也就是上面说的几个组件，他们会分别启动。</p>
<p>其中，比较关键的是 parser，sink，store。</p>
<p>CanalEventParser 启动后，会启动一个叫做 parseThread 线程，不停的循环。主要是：构造与 mysql 的连接，然后启动心跳线程，然后开始 dump binlog。</p>
<p>dump 出来的 binlog 通过 disruptor 无锁队列发布，内部由 3 个消费者按照顺序消费 binlog，处理完之后，交给了 sink 模块。</p>
<p>然后是 sink，这个比较简单，就不说了。sink 处理完之后，交给了 store 模块。</p>
<p>store 模式是一个类似 RingBuffer 的循环数组，存储着从 mysql dump 出来的数据，client 也是从这里获取数据的。该数组维护着 3 个指针，get，put， ack。</p>
<p>这里比较奇怪的是，为什么不使用责任链模式够组装组件？</p>
<h2 id="canal-数据流向">Canal 数据流向</h2>
<p>看了启动流程，再来看看 canal 内部运行的数据流向是什么样子的。我这里简单画了一个图。<br>
<img src="https://phantomma.top/post-images/1703775932712.png" alt="" loading="lazy"></p>
<p>独立版本的 Canal 使用 Netty 暴露端口，使用自己构造的 SessionHandler 处理 TCP 请求，SessionHandler 将请求交给 CanalServerWithEmbedded 来处理。</p>
<p>我们看 CanalServerWithEmbedded 的一些方法，例如 subscribe，get，ack 等，都是和 client 对应的方法，也就是说，CanalServerWithEmbedded 是和 client 打交道的一个类。</p>
<p>CanalServerWithEmbedded 内部管理所有的 CanalInstance，通过 Client 的信息，找到 Client 订阅的 CanalInstance，然后调用 CanalInstance 内部的 Store 模块，也就是那个 RingBuffer 的 get 方法，获取 RingBuffer 的数据。</p>
<p>从 Myslq 的角度看，MysqlConnection 从 Myslq dump 数据，交给 parser 解析，parser 解析完，交给 sink，sink 处理完，交给 store 保存，等待 client 前来获取。</p>
<p>看完了数据流向，如果对哪里有什么疑问，就可以看看哪个模块对应的代码是什么，直接看是看就好了。</p>
<h2 id="总结">总结</h2>
<p>花了点时间看了看 Canal 的代码，总体上还是非常好的，只是有些地方有点疑问，例如 parser，sink，store 为什么不使用过滤器模式。</p>
<p>Client 和 CanalServerWithEmbedded 为什么不使用 RPC 的方式交互，这样更简单明了。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[docker 技巧]]></title>
        <id>https://phantomma.top/post/docker-ji-qiao/</id>
        <link href="https://phantomma.top/post/docker-ji-qiao/">
        </link>
        <updated>2014-12-01T12:43:32.000Z</updated>
        <content type="html"><![CDATA[<h3 id="docker服务">docker服务</h3>
<p>重启docker服务  sudo service docker restart<br>
关闭docker服务  docker service docker stop<br>
开启docker服务  docker service docker start</p>
<h3 id="docker镜像">docker镜像</h3>
<p>查看镜像：docker images<br>
删除镜像：docker rmi  IMAGEID  <br>
强制删除镜像：docker rmi -f  IMAGEID  <br>
利用镜像创建容器：docker run --name centos -itd centos:latest <br>
删除全部image的:  docker rmi $(docker images -q)<br>
直接删除带none的镜像：docker rmi $(docker images | grep &quot;none&quot; | awk '{print $3}')</p>
<h3 id="docker容器">docker容器</h3>
<p>查看当前运行的容器：docker ps<br>
查询存在的容器：docker ps -a</p>
<blockquote>
<p>命令后面加上--no-trunc，大概是不省略的意思，可以显示列的完整信息</p>
</blockquote>
<p>删除容器：docker -rm  CONTAINERID <br>
强制删除容器：docker -rm -f  CONTAINERID <br>
不能够删除一个正在运行的容器，会报错。需要先停止容器。<br>
进入后台运行的容器：docker exec -it containname /bin/bash<br>
启动容器：docker start containername<br>
停止容器：docker stop containername<br>
停止所有的container，这样才能够删除其中的images： docker stop $(docker ps -a -q)<br>
如果想要删除所有container的话再加一个指令： docker rm $(docker ps -a -q)<br>
注：-a标志列出所有容器，-q标志只列出容器的ID，然后传递给rm命令<br>
重命名一个容器：docker rename old_name new_name<br>
要获取所有容器名称及其IP地址：docker inspect -f '{{.Name}} - {{.NetworkSettings.IPAddress }}' $(docker ps -aq)<br>
覆盖dockerfile里的entrypoint：docker run -it --entrypoint /bin/bash [docker_image]</p>
<h3 id="docker-hub仓库">docker hub仓库</h3>
<p>docker login 配置账号信息<br>
<strong>docker</strong> tag chatroomserver:v1 mh494078416/chatroomserver:v2<br>
<strong>docker</strong> push mh494078416/chatroomserver:v2</p>
<p><strong>无需sudo</strong></p>
<pre><code class="language-bash">sudo usermod -aG docker ${USER}
su - ${USER}
</code></pre>
<p><a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-22-04">How To Install and Use Docker on Ubuntu 22.04 | DigitalOcean</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[数据库的复制与分区]]></title>
        <id>https://phantomma.top/post/shu-ju-ku-de-shu-ju-fu-zhi-yu-fen-qu/</id>
        <link href="https://phantomma.top/post/shu-ju-ku-de-shu-ju-fu-zhi-yu-fen-qu/">
        </link>
        <updated>2014-12-01T09:06:05.000Z</updated>
        <content type="html"><![CDATA[<p>在分布式数据库中通过复制让数据库的分布式节点拥有 copy 备份，主要为了达到以下目的：</p>
<ol>
<li>扩展性，因为单机所承载的数据量是有限的；</li>
<li>容错、高可用，在分布式系统中，单机故障是常态，多做些冗余在遇到单机故障时其他机器就可以及时接管；</li>
<li>性能，如果出现用户跨地区访问的情况， 可以通过多地就近部署减少访问的时延；</li>
</ol>
<p>那么如何复制呢？复制算法在数据库里面有很多，单主（single leader）复制、多主（multi-leader）复制、无主（leaderless）复制。</p>
<p>但是对于大量的数据与并发，仅仅复制是不够的，所以引入了分区或者是叫分区。分区可以让每个分区都是自己的小型数据库，每条数据属于且仅属于一个分区，这样数据库就可以支持多个分区就可以并行操作，可以让数据库支持非常高的吞吐量和数据量。</p>
<h1 id="单主复制">单主复制</h1>
<h2 id="同步-or-异步半同步">同步 or 异步？半同步！</h2>
<p>同步 or 异步其实指是分布式节点的数据是否和主节点保持一致。同步复制就是主库需要等待从库的确认才能返回，而异步不等待该从库的响应就返回。</p>
<p>同步复制的优点是，从库能保证有与主库一致的最新数据副本。如果主库突然失效，我们可以确信这些数据仍然能在从库上找到。缺点是，如果同步从库没有响应（比如它已经崩溃，或者出现网络故障，或其它任何原因），主库就无法处理写入操作。主库必须阻止所有写入，并等待同步副本再次可用。</p>
<p>异步复制虽然不会有阻塞的情况，但是如果从副本承担读请求，副本同步的数据由于复制速度的差异可能会出现不同用户从不同副本上面访问到的数据不一致的情况。</p>
<p>为了避免这种情况，一个是可以让客户端只从主副本读取数据，这样，在正常情况下，所有客户端读到的数据一定是一致的；另一种就是 使用半同步（semi-synchronous ）的方式，例如设置一个从库和主库进行同步，其他库则是异步。如果该同步从库变得不可用或缓慢，则将一个异步从库改为同步运行。这保证你至少在两个节点上拥有最新的数据副本。</p>
<p>拿 kafka 举例，在 kafka 的 leader 中会维护 ISR(In-Sync Replication) 列表，follower 从 leader 同步数据有一些延迟（由参数 replica.lag.time.max.ms 设置超时阈值），超过阈值的 follower 将被剔除出 ISR，只有将消息成功复制到所有 ISR 后，这条消息才算被提交，这样不用等待所有节点确认既保证了性能又能保证数据的可用性。</p>
<h2 id="故障处理">故障处理</h2>
<p>高可用的目标是，即使个别节点失效，也能保持整个系统运行，并尽可能控制节点停机带来的影响。</p>
<h3 id="从节点失效快速恢复">从节点失效——快速恢复</h3>
<p>如果从库崩溃并重新启动，或者，如果主库和从库之间的网络暂时中断，则比较容易恢复：从库可以从日志中知道，在发生故障之前处理的最后一个事务，然后接着这个事务进行处理即可。</p>
<p>我们看一下 Redis 主从同步是如何恢复的。主节点会将那些对自己的状态产生修改性影响的指令记录在本地的内存 buffer 中，然后异步将 buffer 中的指令同步到从节点，从节点一边执行同步的指令流来达到和主节点一样的状态，一边向主节点反馈自己同步到哪里了 (偏移量)。</p>
<p>如果从节点在短时间内无法和主节点进行同步，那么当网络状况恢复时，Redis 的主节点中那些没有同步的指令在 buffer 中有可能已经被后续的指令覆盖掉了，那么就需要使用快照同步。<br>
<img src="https://phantomma.top/post-images/1701421691534.png" alt="" loading="lazy"></p>
<p>它首先需要在主库上进行一次 bgsave 将当前内存的数据全部快照到磁盘文件中，然后再将快照文件的内容全部传送到从节点。从节点将快照文件接受完毕后，立即执行一次全量加载，加载之前先要将当前内存的数据清空。加载完毕后通知主节点继续进行增量同步。</p>
<h3 id="主从切换">主从切换</h3>
<p>主库故障其中一个从库需要被提升为新的主库，需要重新配置客户端，以将它们的写操作发送给新的主库，其他从库需要开始拉取来自新主库的数据变更。</p>
<p>主库的故障切换通常由几步组成：1.确认主库失效；2.选择一个新的主库；3.配置启用新的主库。看似把大象塞进冰箱里的简单步骤实际上可能存在很多地方可能出错：</p>
<ol>
<li>如果使用异步复制，则新主库可能没有收到老主库宕机前最后的写入操作。在选出新主库后，如果老主库重新加入集群，新主库在此期间可能会收到冲突的写入，那这些写入该如何处理？最常见的解决方案是简单丢弃老主库未复制的写入，这很可能打破客户对于数据持久性的期望，如果数据库需要和其他外部存储相协调，那么丢弃写入内容是极其危险的操作；</li>
<li>可能会出现两个节点都以为自己是主库的情况，也就是脑裂。如果两个主库都可以接受写操作，却没有冲突解决机制，那么数据就可能丢失或损坏。一些系统采取了安全防范措施：当检测到两个主库节点同时存在时会关闭其中一个节点，但设计粗糙的机制可能最后会导致两个节点都被关闭；</li>
<li>超时时间应该如何配置？越长意味着恢复时间也越长，太短又可能会出现不必要的故障切换。临时的负载峰值可能导致节点超时，如果系统已经处于高负载或网络问题的困扰之中，那么不必要的故障切换可能会让情况变得更糟糕。</li>
</ol>
<p>我们看看Redis Sentinel 集群是怎么解决这些问题的。Redis Sentinel 负责持续监控主从节点的健康，当主节点挂掉时，自动选择一个最优的从节点切换为主节点。客户端来连接集群时，会首先连接 sentinel，通过 sentinel 来查询主节点的地址，然后再去连接主节点进行数据交互。当主节点发生故障时，客户端会重新向 sentinel 要地址，sentinel 会将最新的主节点地址告诉客户端。<br>
<img src="https://phantomma.top/post-images/1701421757121.png" alt="" loading="lazy"></p>
<p>因为 master 节点是 Redis Sentinel 通过投票选出来的，所以我们可以设置 sentinel 为3个节点以上，且为奇数，选举法定人数设置为<code>（n/2+1）</code>那么每次选出 master 都需要半数以上的节点同意才能通过，少数服从多数，不会出现两个票数一样的 leader同时被选上。</p>
<p>Redis 主从采用异步复制，当主节点挂掉时，从节点可能没有收到全部的同步消息，Sentinel 无法保证消息完全不丢失，但是也尽可能保证消息少丢失。通过这两个配置：</p>
<pre><code>min-slaves-to-write 1
min-slaves-max-lag 10
</code></pre>
<p><code>min-slaves-to-write</code> 表示主节点必须至少有一个从节点在进行正常复制，否则就停止对外写服务，丧失可用性；</p>
<p><code>min-slaves-max-lag</code> 表示多少秒没有收到从节点的反馈，那么此时master就不会接受任何请求。我们可以减小min-slaves-max-lag参数的值，这样就可以避免在发生故障时大量的数据丢失。</p>
<h3 id="复制日志该如何实现">复制日志该如何实现？</h3>
<p>最简单的想法可能是基于 SQL 语句的复制，将每个 <code>INSERT</code>、<code>UPDATE</code> 或 <code>DELETE</code> 语句都被转发给每个从库，就像直接从客户端收到一样。但这样也会有很多问题：</p>
<ol>
<li>如果语句中使用了诸如 <code>NOW() </code>或 <code>RAND()</code>这样的函数，该怎么处理？使用了用户定义的函数、触发器、存储过程又该怎么处理；</li>
<li>如果使用了自增id，或依赖于数据库中的现有数据（例如，<code>UPDATE ... WHERE &lt;某些条件&gt;</code>），每个副本上按照完全相同的顺序执行它们，否则可能会产生不同的效果。当有多个并发执行的事务时，这可能成为一个限制；</li>
</ol>
<p>基于语句的复制在 5.1 版本前的 MySQL 中被使用到。但现在在默认情况下，如果语句中存在任何不确定性，MySQL 会切换到基于行的复制。</p>
<p>基于预写日志（WAL）复制，WAL 是在进行任何数据更改（更新、删除等）之前，先将这些更改操作写入到日志中，所以通过这个日志从库可以构建一个与主库一模一样的数据结构拷贝。缺点是与存储引擎紧密耦合，如果数据库的版本的变更修改了日志格式，这会让复制没法进行。</p>
<p>所以 WAL 复制对运维来说是很难受的， 如果复制协议不允许版本不匹配，则此类升级需要停机。</p>
<p>还有一种就是基于行的逻辑日志复制，逻辑日志是关系数据库用来表示行操作写入记录的序列， MySQL 的二进制日志（binlog）使用了这种方法。由于逻辑日志与存储引擎的内部实现是解耦的，系统可以更容易地做到向后兼容，从而使主库和从库能够运行不同版本的数据库软件，或者甚至不同的存储引擎。</p>
<h2 id="写后读一致性问题">写后读一致性问题</h2>
<h3 id="reading-your-own-writes">Reading Your Own Writes</h3>
<p>许多应用让用户提交一些数据，然后查看他们提交的内容。可能是用户数据库中的记录，也可能是对讨论主题的评论，或其他类似的内容。提交新数据时，必须将其发送给主库，但是当用户查看数据时，可以通过从库进行读取。如果数据经常被查看，但只是偶尔写入，这是非常合适的。</p>
<p>但对于异步复制，如果用户在写入后马上就查看数据，则新数据可能尚未到达副本。对用户而言，看起来好像是刚提交的数据丢失了。<br>
<img src="https://phantomma.top/post-images/1701421930673.png" alt="" loading="lazy"></p>
<p>在这种情况下，我们需要写后读一致性（read-after-write consistency），也被称作 read-your-writes<br>
consistency 。这是一个保证，如果用户重新加载页面，他们总会看到他们自己提交的任何更新。</p>
<p>基于领导者的复制系统中实现写后读一致性：</p>
<ul>
<li>对于用户 可能修改过的内容，总是从主库读取；这就要求得有办法不通过实际的查询就可以知道用户是否修改了某些东西。</li>
<li>如果应用中的大部分内容都可能被用户编辑，在这种情况下可以使用其他标准来决定是否从主库读取。例如可以跟踪上次更新的时间，在上次更新后的一分钟内，从主库读。</li>
<li>客户端可以记住最近一次写入的时间戳，系统需要确保从库在处理该用户的读取请求时，该时间戳前的变更都已经传播到了本从库中。如果当前从库不够新，则可以从另一个从库读取，或者等待从库追赶上来。这里的时间戳可以是逻辑时间戳（表示写入顺序的东西，例如日志序列号）或实际的系统时钟</li>
<li>如果你的副本分布在多个数据中心（为了在地理上接近用户或者出于可用性目的），还会有额外的复杂性。任何需要由主库提供服务的请求都必须路由到包含该主库的数据中心。</li>
</ul>
<p>另一种复杂的情况发生在同一位用户从多个设备（例如桌面浏览器和移动 APP）请求服务的时候。这种情况下可能就需要提供跨设备的写后读一致性：如果用户在一个设备上输入了一些信息，然后在另一个设备上查看，则应该看到他们刚输入的信息。</p>
<p>在这种情况下，还有一些需要考虑的问题：</p>
<ul>
<li>记住用户上次更新时间戳的方法变得更加困难，因为一个设备上运行的程序不知道另一个设备上发生了什么。需要对这些元数据进行中心化的存储。</li>
<li>如果副本分布在不同的数据中心，很难保证来自不同设备的连接会路由到同一数据中心。如果你的方法需要读主库，可能首先需要把来自该用户所有设备的请求都路由到同一个数据中心。</li>
</ul>
<h3 id="monotonic-reads-单调读">Monotonic Reads 单调读</h3>
<p>如果用户从不同从库进行多次读取，就可能发生时光倒流（moving backward in time）的情况。例如用户 2345 两次进行相同的查询，首先查询了一个延迟很小的从库，然后是一个延迟较大的从库，第一个查询返回了最近由用户 1234 添加的评论，但是第二个查询不返回任何东西，因为滞后的从库还没有拉取到该写入内容。用户 2345 先看见用户 1234 的评论，然后又看到它消失，这就会让人觉得非常困惑了。<br>
<img src="https://phantomma.top/post-images/1701422008085.png" alt="" loading="lazy"></p>
<p>单调读（monotonic reads）这是一个比 强一致性（strong consistency） 更弱，但比 最终一致性（eventual consistency） 更强的保证。当读取数据时，你可能会看到一个旧值；单调读仅意味着如果一个用户顺序地进行多次读取，则他们不会看到时间回退，也就是说，如果已经读取到较新的数据，后续的读取不会得到更旧的数据。</p>
<p>实现单调读的一种方式是确保每个用户总是从同一个副本进行读取，例如可以基于用户 ID 的散列来选择副本。</p>
<h1 id="多主复制">多主复制</h1>
<p>假如你有一个数据库，副本分散在好几个不同的数据中心（可能会用来容忍单个数据中心的故障，或者为了在地理上更接近用户）。多主配置中可以在每个数据中心都有主库。<br>
<img src="https://phantomma.top/post-images/1701422037696.png" alt="" loading="lazy"></p>
<p>在运维多个数据中心多主是有很多优点的，如：在多主配置中，每个写操作都可以在本地数据中心进行就近处理，性能会好一些；每个数据中心可以独立于其他数据中心继续运行，其他数据中心出现故障不会导致全局瘫痪；</p>
<p>多主配置也有一个很大的缺点：两个不同的数据中心可能会同时修改相同的数据，写冲突是必须解决的。</p>
<h2 id="处理写入冲突">处理写入冲突</h2>
<h3 id="避免冲突">避免冲突</h3>
<p>处理冲突的最简单的策略就是避免它们，如果应用程序可以确保特定记录的所有写入都通过同一个主库，那么冲突就不会发生。例如，在一个用户可以编辑自己数据的应用程序中，可以确保来自特定用户的请求始终路由到同一数据中心，并使用该数据中心的主库进行读写。不同的用户可能有不同的 “主” 数据中心。</p>
<p>但是也有可能因为数据中心出现故障，你需要将流量重新路由到另一个数据中心，在这种情况下，冲突避免将失效，你必须处理不同主库同时写入的可能性。</p>
<h3 id="收敛至一致的状态">收敛至一致的状态</h3>
<p>在多主配置中，由于没有明确的写入顺序，如果每个副本只是按照它看到写入的顺序写入，那么数据库最终将处于不一致的状态。所以需要以一种方式在所有变更复制完成时收敛至一个相同的最终值。</p>
<p>例如可以为每个副本分配一个唯一的 ID，ID 编号更高的写入具有更高的优先级，但是这种方法也意味着数据丢失。</p>
<h3 id="用户自行处理">用户自行处理</h3>
<p>把这个操作直接交给用户，让用户自己在读取或写入前进行冲突解决，这种例子也是屡见不鲜，Github采用就是这种方式。</p>
<h1 id="无主复制">无主复制</h1>
<p>在一些无主复制的实现中，客户端直接将写入发送到几个副本中，而另一些情况下，由一个 协调者（coordinator） 节点代表客户端进行写入。</p>
<p>在无主配置中，不存在故障转移。假设客户端（用户 1234）并行发送写入到所有三个副本，并且两个可用副本接受写入，但是不可用副本错过了它。假设三个副本中的两个承认写入是足够的：在用户 1234 已经收到两个确定的响应之后，我们认为写入成功。<br>
<img src="https://phantomma.top/post-images/1701422093234.png" alt="" loading="lazy"></p>
<p>因为有可能会有节点没写入成功，所以当一个客户端从数据库中读取数据时，它不仅仅把它的请求发送到一个副本，而是并行地发送到多个副本。客户可能会从不同的副本获得不同的响应，然后用版本号确定哪个值是更新的。</p>
<p>那么对于一个不可用的节点重新联机之后，它如何赶上它错过的写入？一般来说两种方法：</p>
<ul>
<li>读修复（Read repair），当客户端并行读取多个节点时发现有的副本的值是旧的就将新值写回到该副本；</li>
<li>反熵过程（Anti-entropy process），用后台进程不断查找副本之间的数据差异，并将任何缺少的数据从一个副本复制到另一个副本。</li>
</ul>
<p>在上面的例子中，在三个副本中的两个上进行处理，写入就算成功了。那么多个副本的集群中需要多少个副本写入成功才算成功？</p>
<p>一般地说，如果有 n 个副本，每个写入必须由 w 个节点确认才能被认为是成功的，并且我们必须至少为每个读取查询 r 个节点。只要 <code>w + r &gt; n</code>，我们可以预期在读取时能获得最新的值，r 和 w 是有效读写所需的最低票数。</p>
<p>然而，法定人数（如迄今为止所描述的）并不像它们可能的那样具有容错性。网络中断可以很容易地将客户端从大量的数据库节点上切断。虽然这些节点是活着的，而其他客户端可能也能够连接到它们，但是从数据库节点切断的客户端来看，它们也可能已经死亡。在这种情况下，剩余的可用节点可能会少于 w 或 r，因此客户端不再能达到法定人数。</p>
<h2 id="检测并发写入">检测并发写入</h2>
<p>由于可变的网络延迟和部分节点的故障，事件可能以不同的顺序到达不同的节点。那么就会有数据不一致的情况，比如下图显示了两个客户机 A 和 B 同时写入三节点数据存储中的键 X：</p>
<ul>
<li>节点 1 接收来自 A 的写入，但由于暂时中断，未接收到来自 B 的写入。</li>
<li>节点 2 首先接收来自 A 的写入，然后接收来自 B 的写入。</li>
<li>节点 3 首先接收来自 B 的写入，然后从 A 写入。</li>
</ul>
<p>如果每个节点只要接收到来自客户端的写入请求就简单地覆写某个键值，那么节点就会永久地不一致。<br>
<img src="https://phantomma.top/post-images/1701422168306.png" alt="" loading="lazy"></p>
<p>有一种解决的办法就是 Last write wins (discarding concurrent writes)，所谓 Last write wins 就是只需要存储 “最近” 的值，并允许 “更旧” 的值被覆盖和抛弃。那么实现上其实可以为每个写入附加一个时间戳，然后挑选最大的时间戳作为 “最近的”，并丢弃具有较早时间戳的任何写入。</p>
<h1 id="分区">分区</h1>
<p>分区通常与复制结合使用，使得每个分区的副本存储在多个节点上，一个节点也可能存储多个分区。每个分区领导者（主库）被分配给一个节点，追随者（从库）被分配给其他节点。 每个节点可能是某些分区的主库，同时是其他分区的从库。</p>
<p>我用 TiDB 的存储 TiKV 来举例好了，TiKV 的数据会按 Region 进行存放，一个 Region 就是一个分区。当某个 Region 的大小超过一定限制（默认是 144MB），TiKV 会将它分裂为两个或者更多个 Region，以保证各个 Region 的大小是大致接近的，同样，当某个 Region 因为大量的删除请求导致 Region 的大小变得更小时，TiKV 会将比较小的两个相邻 Region 合并为一个。</p>
<p>将数据划分成 Region 后，TiKV 会尽量保证每个节点上服务的 Region 数量差不多，并以 Region 为单位做 Raft 的复制和成员管理。也就是如下图，你可以看到不同的 node 节点其实都有一份 Region 的复制。<br>
<img src="https://phantomma.top/post-images/1701422199969.png" alt="" loading="lazy"></p>
<h2 id="分区的划分">分区的划分</h2>
<p>假设你有大量数据并且想要分区，如何决定在哪些节点上存储哪些记录呢？分区目标是将数据和查询负载均匀分布在各个节点上。如果分区是不公平的，一些分区比其他分区有更多的数据或查询，我们称之为偏斜（skew）。不均衡会导致负载可能不同节点不一致，高负载高的分区被称为热点（hot spot），这就丧失了分区的平衡负载的特性，这是需要避免的。</p>
<p>其实最简单的方法就是将记录随机分配给节点。这将在所有节点上平均分配数据，但是它有一个很大的缺点：当你试图读取一个特定的值时，你无法知道它在哪个节点上，所以你必须并行地查询所有的节点，这显然是不科学的。</p>
<p>所以现在有两种比较典型的方案：</p>
<ul>
<li>Range：按照 Key 分 Range，某一段连续的 Key 都保存在一个存储节点上。</li>
<li>Hash：按照 Key 做 Hash，根据 Hash 值选择对应的存储节点。</li>
</ul>
<h3 id="根据键的-range-分区">根据键的 Range 分区</h3>
<p>那么不随机的话，一种分区的方法是为每个分区指定一块连续的键范围（从最小值到最大值）。但是键的范围不一定均匀分布，因为数据也很可能不均匀分布。为了均匀分配数据，分区边界需要依据数据调整。</p>
<p>分区边界可以由管理员手动选择，也可以由数据库自动选择，然后在每个分区中，我们可以按照一定的顺序保存键。</p>
<p>对于 TiDB 来说也是根据键的范围进行分区，每个分区被称作 Region，因为有序的，所以可以用 [StartKey，EndKey) 这样一个左闭右开区间来描述。然后它是根据 Region 的大小来进行边界的调整，默认一个 Region 在 96 MiB 的时候就会创建新的 Region。<br>
<img src="https://phantomma.top/post-images/1701422280760.png" alt="" loading="lazy"></p>
<p>根据范围来划分分区还有个问题就是特定的访问模式会导致热点， 如果主键是时间戳，给每天分配一个分区，那么当天的数据的分区可能会因写入而过载，所以需要使用除了时间戳以外的其他东西作为主键的第一个部分。</p>
<h3 id="根据键的-hash-分区">根据键的 Hash 分区</h3>
<p>hash 分区就是通过 hash 散列函数，无论何时给定一个新的字符串输入，它将返回一个 0 到 2^32 -1 之间的 “随机” 数。即使输入的字符串非常相似，它们的散列也会均匀分布在这个数字范围内。然后为每个分区分配一个散列范围，每个通过哈希散列落在分区范围内的键将被存储在该分区中。<br>
<img src="https://phantomma.top/post-images/1701422304360.png" alt="" loading="lazy"></p>
<p>但是这样也有缺点，失去了高效执行范围查询的能力，所以对于关系型的数据库，因为经常性的需要表扫描或者索引扫描，基本上都会使用范围的分片策略，像 nosql 数据库 redis cluster 就使用的是 Hash 分区。</p>
<h2 id="分区再平衡">分区再平衡</h2>
<h3 id="固定分区数">固定分区数</h3>
<p>上面说了根据键的 Hash 分区，那么怎么根据 hash 值存放入分区中呢？一个简单的想法是让分区数等于机器节点数，根据分区数取模，也就是 <code>hash(key) % 分区数</code>。那么如果想要增加新的分区该怎么办呢？怎么把一个分区的数据迁移到另一个分区？这种数据的迁移成本其实会很高。</p>
<p>除此之外还可以用另一种方式：创建比节点更多的分区，并为每个节点分配多个分区。例如，运行在 10 个节点的集群上的数据库可能会从一开始就被拆分为 1,000 个分区，因此大约有 100 个分区被分配给每个节点。如果一个节点被添加到集群中，新节点可以从当前每个节点中窃取一些一些分区，直到分区再次公平分配。</p>
<p>只有分区在节点之间的移动。分区的数量不会改变，键所指定的分区也不会改变。唯一改变的是分区所在的节点。</p>
<p>redis cluster 就是采用这种方式，Redis Cluster为整个集群定义了一共 16384 个 slot，slot 就是分区，每个节点负责一部分的 slot。然后 key 会根据 crc16 计算出得结果和 16384 取模进行 slot 定位，从而定位到具体节点。<br>
<img src="https://phantomma.top/post-images/1701422354442.png" alt="" loading="lazy"></p>
<h3 id="动态分区">动态分区</h3>
<p>对于根据键的 Range 分区的数据库，具有固定边界的固定数量的分区将非常不便，所以一般是当分区增长到超过配置的大小时进行自动分区，我上面也讲到了 TiDB 它是根据分区的大小来进行边界的调整，默认一个 分区在 96MB的时候就会创建新的分区。</p>
<p>并且除了分裂以外还能进行分区的合并，TiDB 会根据分区的大小和 key 的数量，默认小于 20MB 并且 key 数量小于 200000 会触发合并。</p>
<h2 id="路由">路由</h2>
<p>最后要说的就是路由了，客户端怎么知道我要查的数据在哪个分区。一般来说有三种方式：</p>
<ul>
<li>让 client 随便连接哪个节点都行，如果正好数据在这个节点上，那么直接查询即可；如果数据不在这个节点上，这个节点会将请求转发到数据所在的节点上，这种集群是去中心化的，Redis Cluster 就是这种架构，节点与节点之间通过 gossip 协议来交互信息；</li>
<li>client首先要连接集群的路由层，路由层里面知道该请求的数据在哪个分区的哪个节点上，TiDB 就是这种架构的，TiDB 集群里面的 PD 负责管理所有分区的路由信息，TiDB 要访问某个分区的数据时，需要先向 PD 查询此分区的状态和位置；</li>
<li>最后一种方式就是 client 自己保存了分区和节点的信息，这样客户端就直接查询到想要的数据返回了。<br>
<img src="https://phantomma.top/post-images/1701422404698.png" alt="" loading="lazy"></li>
</ul>
<h1 id="总结">总结</h1>
<p>本篇主要是总结了复制和分区的一些主要技术的细节点，讨论了各种实现方式的异同，以及可能出现的问题，然后结合目前主流数据库来讲解如何应用落地。</p>
<p>无论是复制还是分区，都是围绕可用性、性能、扩展性这几个方面展开的。对于复制来说主要有单主复制、多主复制、无主复制。</p>
<p>目前最流行的还是单主复制，很多分布式数据库都是单主复制+分区架构来提供大吞吐的支持，并且单主复制不需要担心冲突解决，实现起来更简单。但是单主复制也可能因为异步复制 leader 宕机而造成数据丢失，所以很多都是半同步（semi-synchronous ）的方式进行复制，例如 kafka 加入 ISR 防丢机制，ISR 一组可靠的备份集合，只有当 ISR 里机器都成功复制，才认为这条消息被成功提交。</p>
<p>再来就是讲了由复制延迟引起的奇怪行为，比如在主库写了数据，但是还没同步到从库，那么将查不出来数据；还有就是从延迟大的从库查的数据和延迟小的从库查的数据也不一样。</p>
<p>接下来就是分区，这个技术在很多分布式数据库都有应用，使用它主要是为了伸缩性，因为不同的分区可以放在不共享集群中的不同节点上所以，并发负载都可以分摊到不同的处理器上，当负载增加时可以添加新的分区，当负载降低时也可以合并分区。</p>
<p>分区就需要考虑到分区怎么划分，一般有按range划分，按 hash 划分。还需要考虑分区的再平衡问题，添加新的节点分区数怎么分配，才能让负载均摊。然后就是路由，客户端的请求过来应该路由到哪个分区，哪个节点。</p>
<hr>
<p>Reference</p>
<ul>
<li>《Designing Data-Intensive Application》</li>
</ul>
]]></content>
    </entry>
</feed>